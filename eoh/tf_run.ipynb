{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-8sz1f7zg7oy0ui\"\n",
    "api_key = \"rpa_EPOJED42G59S80Y6SKMCOI330EQU4JPPMKV2UD2W7j0uku\"\n",
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temasek Foundation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node dataset should ideally be concise and pure application info included ...\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.load_data import prep_tf_node \n",
    "\n",
    "tf_meta_prompt, test_cases, tf_metric_map = prep_tf_node(prompt_mode=True)\n",
    "\n",
    "# Specific Metric required for TF dataset (Worth refactor the code a bit)\n",
    "from methods.llm import get_groq_response\n",
    "from methods.evolnode import EvolNode \n",
    "node = EvolNode(tf_meta_prompt, None, None, get_response=get_groq_response, test_cases=test_cases[:3],\n",
    "                custom_metric_map=tf_metric_map) # setting manual test cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refactor: Parallelize test input inference \n",
    "self = node \n",
    "test_inputs = [c[0] for c in test_cases[:3]]\n",
    "self.code = 'def generate_prompt(project_description):\\n    decision = \\'Maybe\\'\\n    comment = \\'No reasonable decision can be made without further information.\\'\\n    prompt = (\\n        f\"Given the project description {{project_description= \\'{project_description}\\' }}, do evaluate the grant application, make a decision (Yes, No, Maybe) and a brief comment explaining why the project is likely to be accepted or rejected. Make sure the output is in JSON format in markdown like this {{\\'label\\': \\'str()\\', \\'comment\\': \\'str()\\'}} near the top of your response.\"\\n        )\\n    return prompt'\n",
    "code = self.code \n",
    "\n",
    "# 1. Parallelize inference \n",
    "\n",
    "output_per_code_per_test, errors_per_code_per_test = self.call_prompt_function_parallel(test_inputs, codes=None, max_tries=3)\n",
    "\n",
    "\n",
    "# for test_input in test_inputs:\n",
    "        \n",
    "#     if self.meta_prompt.mode == PromptMode.CODE:   \n",
    "#         _, error_msg_delta = self.call_code_function(test_input, code, file_path=None)\n",
    "#         if error_msg_delta == \"\":\n",
    "#             passed_tests += 1\n",
    "#         else:\n",
    "#             error_msg += error_msg_delta\n",
    "            \n",
    "#     elif self.meta_prompt.mode == PromptMode.PROMPT:\n",
    "#         _, error_msg_delta = self.call_prompt_function(test_input, code, self.get_response)\n",
    "#         if error_msg_delta == \"\":\n",
    "#             passed_tests += 1\n",
    "#         else:\n",
    "#             error_msg += error_msg_delta\n",
    "#     else:\n",
    "#         raise ValueError(f\"Unknown mode: {self.meta_prompt.mode}\")\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def generate_prompt(project_description):\n",
      "    decision = 'Maybe'\n",
      "    comment = 'No reasonable decision can be made without further information.'\n",
      "    prompt = (\n",
      "        f\"Given the project description {{project_description= '{project_description}' }}, do evaluate the grant application, make a decision (Yes, No, Maybe) and a brief comment explaining why the project is likely to be accepted or rejected. Make sure the output is in JSON format in markdown like this {{'label': 'str()', 'comment': 'str()'}} near the top of your response.\"\n",
      "        )\n",
      "    return prompt\n"
     ]
    }
   ],
   "source": [
    "print(code)\n",
    "# test_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Prompts:  9\n"
     ]
    }
   ],
   "source": [
    "import signal\n",
    "from typing import Dict, Any, List, Union\n",
    "from methods.meta_prompt import extract_json_from_text\n",
    "from methods.meta_execute import timeout_handler\n",
    "import types\n",
    "\n",
    "def _call_func_prompt_batch(input_datas: Union[list, dict], code: str, get_response: callable, max_tries: int = 3):\n",
    "    \"\"\" \n",
    "    Batch prompt forward propagation\n",
    "    - Returns a dictionary with indices as keys to maintain input-output correspondence\n",
    "    \"\"\"\n",
    "    if isinstance(input_datas, dict):\n",
    "        input_datas = [input_datas] \n",
    "        \n",
    "    input_datas = input_datas * max_tries\n",
    "    input_indices = [idx for idx, _ in enumerate(input_datas)] * max_tries\n",
    "    error_messages = [\"\" for _ in range(len(input_indices))]\n",
    "\n",
    "    valid_prompt_indices, valid_prompts = [], []\n",
    "    for idx, input_data in zip(input_indices, input_datas):\n",
    "        # Set up the timeout handler\n",
    "        signal.signal(signal.SIGALRM, timeout_handler)\n",
    "        signal.alarm(3)  # Set timeout to 3 seconds\n",
    "        try:\n",
    "            mod = types.ModuleType('dynamic_module')\n",
    "            exec(code, mod.__dict__)\n",
    "            func_name = \"generate_prompt\" \n",
    "            prompt_func = mod.__dict__[func_name]\n",
    "            prompt = prompt_func(project_description=input_data['project_description'])\n",
    "            if prompt and isinstance(prompt, str):\n",
    "                valid_prompt_indices.append(idx)\n",
    "                valid_prompts.append(prompt)\n",
    "        except Exception as e:\n",
    "            error_messages[idx] += str(e)\n",
    "            continue\n",
    "        finally:\n",
    "            signal.alarm(0)\n",
    "            \n",
    "    print(\"Valid Prompts: \", len(valid_prompts))\n",
    "    # print(valid_prompts[1])\n",
    "    \n",
    "    responses = get_response(valid_prompts)\n",
    "    \n",
    "    valid_response_indices, valid_output_dicts = [], []\n",
    "    for (idx, response) in zip(valid_prompt_indices, responses):\n",
    "        try:\n",
    "            output_dict = extract_json_from_text(response)\n",
    "            if output_dict:\n",
    "                valid_response_indices.append(idx)\n",
    "                valid_output_dicts.append(output_dict)\n",
    "        except Exception:\n",
    "            error_messages[idx] += \"Error extracting JSON from response\"\n",
    "            continue\n",
    "         \n",
    "    output_dicts = [{} for _ in range(len(input_indices) // max_tries)]\n",
    "    for idx, output_dict in zip(valid_response_indices, valid_output_dicts):\n",
    "        output_dicts[idx // max_tries] = output_dict # does not matter which one we use, anything output is fine\n",
    "        \n",
    "    return output_dicts, \"\\n\".join(error_messages)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# come up with test inputs for above function \n",
    "output_dicts, error_msgs = _call_func_prompt_batch(test_inputs, code, self.get_response, max_tries=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'Maybe',\n",
       "  'comment': 'The project has potential, but it requires further clarification and improvement in several areas.'},\n",
       " {'label': 'Yes',\n",
       "  'comment': 'The project demonstrates a clear vision for carbon-negative city infrastructure and proposes innovative solutions to address environmental concerns. The project has also secured necessary support and partners, indicating a high potential for success.'},\n",
       " {'label': 'Maybe',\n",
       "  'comment': \"The project's clear focus on sustainability and commercial viability is a positive, but its current ideation stage and lack of fully defined project scope of work reduce its overall feasibility. The project's potential impact may be significant, but a more detailed plan and proof of concept are needed before being fully considered for funding.\"},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {},\n",
       " {}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ† Best Code Performance Summary ðŸ†\n",
      "  âš¡ Structural fitness: 0.00\n",
      "  ðŸŽ¯ Functional fitness: 0.00\n",
      "  â­ Global fitness:     0.00\n",
      "  ðŸ”„ Compiled solutions:        1\n",
      "  â±ï¸ Time breakdown:\n",
      "     :: Query time: 3.23s\n",
      "     :: Evolution time: 1.25s\n",
      "     :: Evaluation time: 0.40s\n",
      "     :: Total time: 4.88s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ðŸ“Š Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "âŒ Error Messages:\n",
      "cannot access local variable 'text_rank' where it is not associated with a value\n",
      "cannot access local variable 'text_rank' where it is not associated with a value\n",
      "cannot access local variable 'text_rank' where it is not associated with a value\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "node.evolve(\"i1\", batch_size=1) # run evolution :: Need to parallize test input inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from datetime import datetime\\nimport http.client\\nimport json\\nimport os\\nfrom typing import Dict, Any\\n\\n\\ndef _search_google(query: str) ->Dict[str, Any]:\\n    \"\"\"\\n    Use Serper API to search Google for information\\n    \\n    Args:\\n        query (str): The search query\\n    \\n    Returns:\\n        Dict[str, Any]: Parsed JSON response from the API\\n    \"\"\"\\n    conn = http.client.HTTPSConnection(\\'google.serper.dev\\')\\n    payload = json.dumps({\\'q\\': query})\\n    headers = {\\'X-API-KEY\\': os.environ[\\'SERPER_API_KEY\\'], \\'Content-Type\\':\\n        \\'application/json\\'}\\n    try:\\n        conn.request(\\'POST\\', \\'/search\\', payload, headers)\\n        res = conn.getresponse()\\n        data = res.read()\\n        return json.loads(data.decode(\\'utf-8\\'))\\n    except Exception as e:\\n        print(f\\'Error occurred during API request: {str(e)}\\')\\n        return {}\\n    finally:\\n        conn.close()\\n\\n\\ndef search_google(query: str) ->str:\\n    \"\"\" \\n    Input query, return search result string from Google\\n    \"\"\"\\n    result = _search_google(query)\\n    result_dict = {k.replace(\\'organic\\', \\'Search Result\\'): v for k, v in\\n        result.items() if k in [\\'answerBox\\', \\'organic\\']}\\n    result_str = json.dumps(result_dict, indent=2)\\n    return result_str\\n\\n\\ndef get_celeb_age(name):\\n    current_year = datetime.now().year\\n    search_query = f\\'{name} birth date\\'\\n    search_result = search_google(search_query)\\n    prompt = f\"\"\"\\n    Based on the following information about {name}, please determine their current age:\\n\\n    Search results: {search_result}\\n\\n    Calculate the age by subtracting the birth year from the current year ({current_year}).\\n\\n    Please provide the result in the following JSON format:\\n    {{\\n        \"age\": int(calculated_age)\\n    }}\\n\\n    If the birth date is not found or unclear, return null for the age value.\\n    \"\"\"\\n    return prompt\\n\\n\\ndef generate_prompt(project_description):\\n    decision = \\'Maybe\\'\\n    comment = \\'No reasonable decision can be made without further information.\\'\\n    if project_description:\\n        key_insights = text_rank(project_description)\\n        researcher_age = get_celeb_age(name=key_insights[\\'researcher\\'])\\n        if researcher_age > 40:\\n            decision = \\'No\\'\\n            comment = (\\n                \\'The researcher is likely too old and inexperienced to lead this project.\\'\\n                )\\n        elif researcher_age < 30:\\n            decision = \\'Yes\\'\\n            comment = (\\n                \\'The researcher is young and likely to bring fresh ideas and perspectives to the project.\\'\\n                )\\n        else:\\n            decision = \\'Maybe\\'\\n            comment = (\\n                \\'The researcher has a moderate amount of experience and may be a good fit for this project.\\'\\n                )\\n    prompt = (\\n        f\"Given the project description {{project_description= \\'{project_description}\\' }}, do evaluate the grant application, make a decision (Yes, No, Maybe) and a brief comment explaining why the project is likely to be accepted or rejected. Make sure the output is in JSON format in markdown like this {{\\'label\\': \\'str()\\', \\'comment\\': \\'str()\\'}} near the top of your response.\"\\n        )\\n\\n    def text_rank(text):\\n        return {\\'researcher\\': \\'John Doe\\'}\\n    return prompt\\n\\n\\ndef text_rank(text):\\n    return {\\'researcher\\': \\'John Doe\\'}\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
