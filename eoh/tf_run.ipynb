{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-8sz1f7zg7oy0ui\"\n",
    "api_key = \"rpa_EPOJED42G59S80Y6SKMCOI330EQU4JPPMKV2UD2W7j0uku\"\n",
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temasek Foundation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Test cases (dataloading ...)\n",
    "import sys \n",
    "sys.path.append(\"../notebook/\")\n",
    "from optm.soft_prompt import load_tf_data\n",
    "\n",
    "train_data, test_data = load_tf_data(\"../data/processed_data_clean.json\")\n",
    "\n",
    "# reference metric fuction below ... we care about prediction of label, specifically \"no\"\n",
    "# we care about precision more than we care abou recall on 'No' prediction \n",
    "\n",
    "# Node dataset should ideally be concise and pure application info included ...\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "\n",
    "tf_meta_prompt = MetaPrompt(\n",
    "    task = \"Evaluate grant application, make a decision (Yes, No, Maybe) and a brief comment explanating your decision on why this project is likely to be accepted or rejected.\",\n",
    "    func_name = \"evaluate_grant\",\n",
    "    inputs = [\"project_description\"],\n",
    "    outputs = [\"label\", \"comment\"],\n",
    "    input_types = [\"str\"],\n",
    "    output_types = [\"str\", \"str\"],\n",
    "    mode = PromptMode.PROMPT\n",
    ")\n",
    "\n",
    "# Prepare test cases :: input dict & output dict\n",
    "test_cases = []\n",
    "for prompt, label, comment in zip(train_data[\"prompt\"], train_data[\"label\"], train_data[\"comment\"]):\n",
    "    test_cases.append(({\"project_description\": prompt}, {\"label\": label, \"comment\": comment}))\n",
    "    \n",
    "\n",
    "# Specific Metric required for TF dataset (Worth refactor the code a bit)\n",
    "from methods.evolnode import EvolNode \n",
    "node = EvolNode(tf_meta_prompt, None, None, get_response=get_endpoint_response, test_cases=test_cases,\n",
    "                custom_metric_map=None) # setting manual test cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = test_cases[0][1]\n",
    "pred_dict = test_cases[1][1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom metric works on output dictionary, both element level and batch level (indiv score & overall score)\n",
    "\n",
    "from typing import Union, Callable\n",
    "\n",
    "def tf_name_to_metric(output_name: str) -> Callable: \n",
    "    \"\"\" \n",
    "    Dataset specific metric pointer\n",
    "    \"\"\"\n",
    "    if output_name == \"label\":\n",
    "        return tf_label_metric\n",
    "    else:\n",
    "        raise ValueError(f\"No metric for output: {output_name}\")\n",
    "\n",
    "def tf_label_metric(target_outputs: Union[list, dict], pred_outputs: Union[list, dict],\n",
    "                    beta: float = 5):\n",
    "    \"\"\" \n",
    "    \"No\" prediction > \"Yes\" prediction \n",
    "    Recall > Precision\n",
    "    Weighted score (recall + precision) with beta : 1 as weights \n",
    "    \"\"\"\n",
    "    \n",
    "    if isinstance(target_outputs, dict):\n",
    "        target_outputs = [target_outputs]\n",
    "    if isinstance(pred_outputs, dict):\n",
    "        pred_outputs = [pred_outputs]\n",
    "        \n",
    "    assert all(\"label\" in target and \"comment\" in target for target in target_outputs), \"Target outputs must contain 'label' and 'comment'\"\n",
    "    assert all(\"label\" in pred and \"comment\" in pred for pred in pred_outputs), \"Predicted outputs must contain 'label' and 'comment'\"\n",
    "    \n",
    "    # Extract labels\n",
    "    true_labels = [t[\"label\"].lower().strip() for t in target_outputs]\n",
    "    pred_labels = [p[\"label\"].lower().strip() for p in pred_outputs]\n",
    "    \n",
    "    # Calculate metrics for \"no\" predictions\n",
    "    true_no = sum(1 for label in true_labels if label == \"no\")\n",
    "    pred_no = sum(1 for label in pred_labels if label == \"no\")\n",
    "    true_pos_no = sum(1 for t, p in zip(true_labels, pred_labels) if t == \"no\" and p == \"no\")\n",
    "    \n",
    "    # Calculate precision and recall for \"no\"\n",
    "    no_precision = true_pos_no / pred_no if pred_no > 0 else 0\n",
    "    no_recall = true_pos_no / true_no if true_no > 0 else 0\n",
    "    \n",
    "    # Calculate precision and recall for \"no\"\n",
    "    no_precision = true_pos_no / pred_no if pred_no > 0 else 0\n",
    "    no_recall = true_pos_no / true_no if true_no > 0 else 0\n",
    "    \n",
    "    # Simple weighted score using beta\n",
    "    beta = 5  # Keeping your beta parameter\n",
    "    weighted_score = ((1 / (1 + beta)) * no_precision + (beta / (1 + beta)) * no_recall) if (no_precision + no_recall) > 0 else 0.0\n",
    "    \n",
    "    return weighted_score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_meta_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 16.70s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 24/24 [00:46<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 46.41s, 0 errors\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_require_llm_metric() missing 1 required positional argument: 'value'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m test_inputs \u001b[38;5;241m=\u001b[39m [c[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m test_cases]\n\u001b[1;32m     17\u001b[0m node \u001b[38;5;241m=\u001b[39m EvolNode(mp, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, get_response\u001b[38;5;241m=\u001b[39mget_endpoint_response, test_cases\u001b[38;5;241m=\u001b[39mtest_cases) \u001b[38;5;66;03m# setting manual test cases\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_summary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Scale up batch size\u001b[39;00m\n\u001b[1;32m     22\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDilireba\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m     23\u001b[0m output_dict \u001b[38;5;241m=\u001b[39m node(input_dict, max_attempts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m) \u001b[38;5;66;03m# Batch Inference with vLLM\u001b[39;00m\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:492\u001b[0m, in \u001b[0;36mEvolNode.evolve\u001b[0;34m(self, method, parents, replace, feedback, batch_size, fitness_threshold, num_runs, max_tries, print_summary, query_node)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreasonings \u001b[38;5;241m=\u001b[39m reasonings\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodes \u001b[38;5;241m=\u001b[39m codes\n\u001b[0;32m--> 492\u001b[0m fitness_per_code, errors_per_code, global_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metric_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_metric_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    494\u001b[0m evaluation_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m evolve_end_time\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:683\u001b[0m, in \u001b[0;36mEvolNode._evaluate_fitness\u001b[0;34m(self, test_cases, codes, max_tries, num_runs, custom_metric_map)\u001b[0m\n\u001b[1;32m    681\u001b[0m test_inputs \u001b[38;5;241m=\u001b[39m [case[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m test_cases]\n\u001b[1;32m    682\u001b[0m target_outputs \u001b[38;5;241m=\u001b[39m [case[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m case \u001b[38;5;129;01min\u001b[39;00m test_cases]\n\u001b[0;32m--> 683\u001b[0m score_per_code_per_test, evaluate_errors_per_code_per_test \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_alignment_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_per_code_per_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                                                                                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_response\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metric_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_metric_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    685\u001b[0m errors_per_code \u001b[38;5;241m=\u001b[39m combine_errors(evaluate_errors_per_code_per_test, errors_per_code_per_test)\n\u001b[1;32m    687\u001b[0m fitness_per_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msummarize_fitness(codes, score_per_code_per_test, output_per_code_per_test, test_inputs, max_tries)\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:273\u001b[0m, in \u001b[0;36mcheck_alignment_parallel\u001b[0;34m(output_per_code_per_test, test_inputs, target_outputs, get_response, batch_size, custom_metric_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m errors_per_code_per_test \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Get scores from metric-based alignment check\u001b[39;00m\n\u001b[0;32m--> 273\u001b[0m metric_scores, errors_per_code_per_test \u001b[38;5;241m=\u001b[39m \u001b[43m_check_alignment_with_metric_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_per_code_per_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors_per_code_per_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;66;03m# Get scores from LLM-based alignment check \u001b[39;00m\n\u001b[1;32m    276\u001b[0m llm_scores, errors_per_code_per_test \u001b[38;5;241m=\u001b[39m _check_alignment_with_llm_parallel(output_per_code_per_test, errors_per_code_per_test, test_inputs, target_outputs,\n\u001b[1;32m    277\u001b[0m                                                                           get_response, batch_size)\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:160\u001b[0m, in \u001b[0;36m_check_alignment_with_metric_parallel\u001b[0;34m(output_per_code_per_test, errors_per_code_per_test, test_inputs, target_outputs, custom_metric_map)\u001b[0m\n\u001b[1;32m    157\u001b[0m trimmed_target \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pred_output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 160\u001b[0m     trimmed_pred \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpred_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrequire_llm_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metric_map\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    161\u001b[0m     trimmed_target \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m target_output\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m require_llm_metric(k, v, custom_metric_map)}            \n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trimmed_pred) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trimmed_target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:160\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m trimmed_target \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pred_output, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(target_output, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 160\u001b[0m     trimmed_pred \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m pred_output\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mrequire_llm_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metric_map\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m    161\u001b[0m     trimmed_target \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m target_output\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m require_llm_metric(k, v, custom_metric_map)}            \n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trimmed_pred) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trimmed_target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:81\u001b[0m, in \u001b[0;36mrequire_llm_metric\u001b[0;34m(name, value, custom_metric_map)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_require_llm_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: _require_llm_metric() missing 1 required positional argument: 'value'"
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "# Code + Compilor Task\n",
    "# mp = MetaPrompt(\"Search for age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.CODE)\n",
    "# Prompt + LLM Task\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "test_inputs = [c[0] for c in test_cases]\n",
    "\n",
    "node = EvolNode(mp, None, None, get_response=get_endpoint_response, test_cases=test_cases) # setting manual test cases\n",
    "\n",
    "node.evolve(\"i1\", replace=True, batch_size=20, num_runs=2, print_summary=True) # Scale up batch size\n",
    "\n",
    "\n",
    "input_dict = {\"name\": \"Dilireba\"}\n",
    "output_dict = node(input_dict, max_attempts=6) # Batch Inference with vLLM\n",
    "\n",
    "# node.get_response = get_groq_response # fast sequential inference\n",
    "# output_dict = node(input_dict, max_attempts=6, batch_inference=False)\n",
    " \n",
    "print(\"Output dict: \", output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
