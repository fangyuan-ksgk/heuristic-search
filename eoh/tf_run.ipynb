{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-8sz1f7zg7oy0ui\"\n",
    "api_key = \"rpa_EPOJED42G59S80Y6SKMCOI330EQU4JPPMKV2UD2W7j0uku\"\n",
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temasek Foundation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Test cases (dataloading ...)\n",
    "import sys \n",
    "sys.path.append(\"../notebook/\")\n",
    "from optm.soft_prompt import load_tf_data\n",
    "\n",
    "train_data, test_data = load_tf_data(\"../data/processed_data_clean.json\")\n",
    "\n",
    "# reference metric fuction below ... we care about prediction of label, specifically \"no\"\n",
    "# we care about precision more than we care abou recall on 'No' prediction \n",
    "\n",
    "# Node dataset should ideally be concise and pure application info included ...\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "\n",
    "tf_meta_prompt = MetaPrompt(\n",
    "    task = \"Evaluate grant application, make a decision (Yes, No, Maybe) and a brief comment explanating your decision on why this project is likely to be accepted or rejected.\",\n",
    "    func_name = \"evaluate_grant\",\n",
    "    inputs = [\"project_description\"],\n",
    "    outputs = [\"label\", \"comment\"],\n",
    "    input_types = [\"str\"],\n",
    "    output_types = [\"str\", \"str\"],\n",
    "    mode = PromptMode.PROMPT\n",
    ")\n",
    "\n",
    "# Prepare test cases :: input dict & output dict\n",
    "test_cases = []\n",
    "for prompt, label, comment in zip(train_data[\"prompt\"], train_data[\"label\"], train_data[\"comment\"]):\n",
    "    test_cases.append(({\"project_description\": prompt}, {\"label\": label, \"comment\": comment}))\n",
    "test_cases = test_cases[:3]\n",
    "\n",
    "    \n",
    "# Custom metric for TF datase\n",
    "\n",
    "from typing import Union, Callable\n",
    "\n",
    "\n",
    "def tf_label_metric(target_labels: Union[str, list], pred_labels: Union[str, list],\n",
    "                    beta: float = 5):\n",
    "    \"\"\" \n",
    "    \"No\" prediction > \"Yes\" prediction \n",
    "    Recall > Precision\n",
    "    Weighted score (recall + precision) with beta : 1 as weights \n",
    "    \"\"\"\n",
    "    if isinstance(target_labels, str):\n",
    "        target_labels = [target_labels]\n",
    "    if isinstance(pred_labels, str): \n",
    "        pred_labels = [pred_labels]\n",
    "        \n",
    "    # Convert labels to lowercase for case-insensitive comparison\n",
    "    target_labels = [label.lower() for label in target_labels]\n",
    "    pred_labels = [label.lower() for label in pred_labels]\n",
    "    \n",
    "    # For single label comparison, we can simplify:\n",
    "    # true positive: target is \"no\" and prediction is \"no\"\n",
    "    true_pos = sum(1 for t, p in zip(target_labels, pred_labels) if t == \"no\" and p == \"no\")\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    # If target is \"no\", recall denominator is 1, else 0\n",
    "    # If pred is \"no\", precision denominator is 1, else 0\n",
    "    no_targets = sum(1 for t in target_labels if t == \"no\")\n",
    "    no_preds = sum(1 for p in pred_labels if p == \"no\")\n",
    "    \n",
    "    no_precision = true_pos / no_preds if no_preds > 0 else 0\n",
    "    no_recall = true_pos / no_targets if no_targets > 0 else 0\n",
    "    \n",
    "    # Simple weighted score using beta\n",
    "    weighted_score = ((1 / (1 + beta)) * no_precision + (beta / (1 + beta)) * no_recall) if (no_precision + no_recall) > 0 else 0.0\n",
    "    \n",
    "    err_msg = []\n",
    "    for t, p in zip(target_labels, pred_labels):\n",
    "        if t != p: \n",
    "            err_msg.append(f\"Target is '{t}', but prediction is '{p}'\") \n",
    "        \n",
    "    return weighted_score, \"\\n\".join(err_msg) if err_msg else \"\"\n",
    "\n",
    "\n",
    "\n",
    "tf_metric_map = {\"label\": tf_label_metric}\n",
    "    \n",
    "\n",
    "# Specific Metric required for TF dataset (Worth refactor the code a bit)\n",
    "from methods.llm import get_groq_response\n",
    "from methods.evolnode import EvolNode \n",
    "node = EvolNode(tf_meta_prompt, None, None, get_response=get_groq_response, test_cases=test_cases,\n",
    "                custom_metric_map=tf_metric_map) # setting manual test cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Metric not defined for type: <class 'str'>, use LLM-based alignment check instead !",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:489\u001b[0m, in \u001b[0;36mEvolNode.evolve\u001b[0;34m(self, method, parents, replace, feedback, batch_size, fitness_threshold, num_runs, max_tries, print_summary, query_node)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreasonings \u001b[38;5;241m=\u001b[39m reasonings\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodes \u001b[38;5;241m=\u001b[39m codes\n\u001b[0;32m--> 489\u001b[0m fitness_per_code, errors_per_code, global_summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate_fitness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tries\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_runs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metric_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_metric_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    490\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    491\u001b[0m evaluation_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m evolve_end_time\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:680\u001b[0m, in \u001b[0;36mEvolNode._evaluate_fitness\u001b[0;34m(self, test_cases, codes, max_tries, num_runs, custom_metric_map)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m outputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m test_index \u001b[38;5;129;01min\u001b[39;00m output_per_code_per_test[code_index]:\n\u001b[0;32m--> 680\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_per_code_per_test[code_index][test_index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors_per_code_per_test[code_index][test_index]:\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mErrors: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrors_per_code_per_test[code_index][test_index]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:270\u001b[0m, in \u001b[0;36mcheck_alignment_parallel\u001b[0;34m(output_per_code_per_test, test_inputs, target_outputs, get_response, batch_size, custom_metric_map)\u001b[0m\n\u001b[1;32m    267\u001b[0m errors_per_code_per_test \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: defaultdict(\u001b[38;5;28mlist\u001b[39m))\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Get scores from metric-based alignment check\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m metric_scores, errors_per_code_per_test \u001b[38;5;241m=\u001b[39m \u001b[43m_check_alignment_with_metric_parallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_per_code_per_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors_per_code_per_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metric_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;66;03m# Get scores from LLM-based alignment check \u001b[39;00m\n\u001b[1;32m    273\u001b[0m llm_scores, errors_per_code_per_test \u001b[38;5;241m=\u001b[39m _check_alignment_with_llm_parallel(output_per_code_per_test, errors_per_code_per_test, test_inputs, target_outputs,\n\u001b[1;32m    274\u001b[0m                                                                           get_response, batch_size)\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:163\u001b[0m, in \u001b[0;36m_check_alignment_with_metric_parallel\u001b[0;34m(output_per_code_per_test, errors_per_code_per_test, test_inputs, target_outputs, custom_metric_map)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trimmed_pred) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trimmed_target) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 163\u001b[0m is_aligned, error_msg \u001b[38;5;241m=\u001b[39m \u001b[43m_check_alignment_with_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_metric_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m scores_per_code_per_test[code_index][test_index]\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(is_aligned))\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_aligned:\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:99\u001b[0m, in \u001b[0;36m_check_alignment_with_metric\u001b[0;34m(pred_output, target_output, custom_metric_map)\u001b[0m\n\u001b[1;32m     97\u001b[0m     metric \u001b[38;5;241m=\u001b[39m custom_metric_map[key]\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 99\u001b[0m     metric \u001b[38;5;241m=\u001b[39m \u001b[43mtype_to_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m is_aligned, error_msg_delta \u001b[38;5;241m=\u001b[39m metric(pred_value, target_value)\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_aligned:\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/evolnode.py:66\u001b[0m, in \u001b[0;36mtype_to_metric\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m float_metric\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric not defined for type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, use LLM-based alignment check instead !\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Metric not defined for type: <class 'str'>, use LLM-based alignment check instead !"
     ]
    }
   ],
   "source": [
    "node.evolve(\"i1\", batch_size=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf_meta_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 20/20 [00:10<00:00,  1.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 10.15s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 29/29 [00:25<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 25.63s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 0.00s, 0 errors\n",
      "🏆 Best Code Performance Summary 🏆\n",
      "  ⚡ Structural fitness: 0.33\n",
      "  🎯 Functional fitness: 1.00\n",
      "  ⭐ Global fitness:     0.67\n",
      "  🔄 Compiled solutions:        17\n",
      "  ⏱️ Time breakdown:\n",
      "     :: Query time: 33.43s\n",
      "     :: Evolution time: 10.19s\n",
      "     :: Evaluation time: 55.46s\n",
      "     :: Total time: 99.09s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "name 're' is not defined\n",
      "name 're' is not defined\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 1: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 2: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 3: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 4: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 5: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 6: Fitness: 8.3%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'results': [{'name': 'Dilireba', 'age': 32, ' occupation': 'Actress'}, {'name': 'Other Person', 'age': 25, ' occupation': 'Engineer'}]}, Error message: Key age not found in prediction output\n",
      "\n",
      "\n",
      "'NoneType' object has no attribute 'group'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 7: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- No JSON structure found in the provided text.\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 8: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "string indices must be integers, not 'str'\n",
      "string indices must be integers, not 'str'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 9: Fitness: 8.3%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'age': None}, Error message: Value None can't be converted into integer\n",
      "Value mismatch for key age: None != 32\n",
      "\n",
      "\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 10: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 11: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 12: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- No JSON structure found in the provided text.\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 13: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 14: Fitness: 8.3%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'age': 30}, Error message: \n",
      "Value mismatch for key age: 30 != 32\n",
      "\n",
      "\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 15: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 16: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- No JSON structure found in the provided text.\n",
      "Failed to parse LLM response -- No JSON structure found in the provided text.\n",
      "================================================================================\n",
      "\n",
      "Error occurred during API request: Function execution timed out (> 3 seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 6/6 [00:10<00:00,  1.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 10.45s, 0 errors\n",
      "Output dict:  {'age': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "# Code + Compilor Task\n",
    "# mp = MetaPrompt(\"Search for age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.CODE)\n",
    "# Prompt + LLM Task\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "test_inputs = [c[0] for c in test_cases]\n",
    "\n",
    "node = EvolNode(mp, None, None, get_response=get_endpoint_response, test_cases=test_cases) # setting manual test cases\n",
    "\n",
    "node.evolve(\"i1\", replace=True, batch_size=20, num_runs=2, print_summary=True) # Scale up batch size\n",
    "\n",
    "\n",
    "input_dict = {\"name\": \"Dilireba\"}\n",
    "output_dict = node(input_dict, max_attempts=6) # Batch Inference with vLLM\n",
    "\n",
    "# node.get_response = get_groq_response # fast sequential inference\n",
    "# output_dict = node(input_dict, max_attempts=6, batch_inference=False)\n",
    " \n",
    "print(\"Output dict: \", output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
