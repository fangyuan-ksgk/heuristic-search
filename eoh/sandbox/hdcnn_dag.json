{"node16": {"name": "HDCNNClassifier", "type": "class", "file": "hdcnn.py", "importance": 2, "file_path": "hdcnn/hdcnn.py", "edges": ["node18"], "opacity": 0.8, "level": 2, "summary": "This code defines a simple neural network classifier called HDCNNClassifier. It consists of two fully connected layers with a ReLU activation and dropout in between. The network takes high-dimensional input vectors and outputs class probabilities for multiple classes.", "minimal_code": "import torch.nn as nn\n\nclass HDCNNClassifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super().__init__()\n        self.fc1 = nn.Linear(input_dim, 512)\n        self.fc2 = nn.Linear(512, num_classes)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.6)\n\n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.dropout(x)\n        return self.fc2(x)", "code": "class HDCNNClassifier(nn.Module):\n    def __init__(self, dim, num_classes):\n        super(HDCNNClassifier, self).__init__()\n        self.fc1 = nn.Linear(dim, 512)\n        self.activation = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.6)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.activation(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out"}, "node18": {"name": "HDCNNClassifier::forward", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This code defines a forward pass for a simple neural network layer. It applies a fully connected layer, followed by an activation function, dropout for regularization, and another fully connected layer. This structure is typical for a single hidden layer in a neural network.", "minimal_code": "import torch.nn as nn\n\nclass SimpleLayer(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SimpleLayer, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.activation = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.activation(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out", "code": "def forward(self, x):\n    out = self.fc1(x)\n    out = self.activation(out)\n    out = self.dropout(out)\n    out = self.fc2(out)\n    return out"}, "node6": {"name": "HDComputing", "type": "class", "file": "hdcnn.py", "importance": 5, "file_path": "hdcnn/hdcnn.py", "edges": ["node9", "node8", "node11", "node10"], "opacity": 1.0, "level": 2, "summary": "This code defines a class `HDComputing` that implements core operations for hyperdimensional computing. It includes methods for generating random hypervectors, superposing (combining) multiple vectors, binding (associating) two vectors, and permuting (shifting) a vector. These operations are fundamental in hyperdimensional computing for creating and manipulating high-dimensional representations of data.", "minimal_code": "import numpy as np\n\nclass HDComputing:\n    def __init__(self, dim, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n    def superpose(self, hvs):\n        return np.sign(np.sum(hvs, axis=0))\n\n    def bind(self, hv1, hv2):\n        return hv1 * hv2\n\n    def permute(self, hv, shifts=1):\n        return np.roll(hv, shifts)", "code": "class HDComputing:\n    def __init__(self, dim, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n    def superpose(self, hvs):\n        sum_hv = np.sum(hvs, axis=0)\n        return np.sign(sum_hv)\n\n    def bind(self, hv1, hv2):\n        return hv1 * hv2\n\n    def permute(self, hv, shifts=1):\n        return np.roll(hv, shifts)"}, "node9": {"name": "HDComputing::superpose", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "The `superpose` function combines multiple high-dimensional vectors (HVs) into a single representative HV. It does this by summing the vectors element-wise and then applying the sign function to binarize the result. This process is a key operation in hyperdimensional computing, used to aggregate multiple vectors into a single meaningful representation.", "minimal_code": "import numpy as np\n\ndef superpose(hvs):\n    return np.sign(np.sum(hvs, axis=0))", "code": "def superpose(self, hvs):\n    sum_hv = np.sum(hvs, axis=0)\n    return np.sign(sum_hv)"}, "node14": {"name": "AGNewsDataset::__len__", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This code defines a `__len__` method for a class, likely representing a data structure. The method returns the length of the `data` attribute of the class instance. This is a common implementation for making objects support the `len()` function in Python, allowing users to get the size or number of elements in the object.", "minimal_code": "class DataStructure:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n# Usage example\nds = DataStructure([1, 2, 3, 4, 5])\nprint(len(ds))  # Output: 5", "code": "def __len__(self):\n    return len(self.data)"}, "node4": {"name": "build_vocab", "type": "function", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This function builds a vocabulary from a dataset of text items. It tokenizes each text, removes non-alphabetic tokens and stop words, counts token frequencies, and creates a vocabulary dictionary with the most common tokens up to a specified maximum size. Special tokens are added at the beginning of the vocabulary.", "minimal_code": "from collections import Counter\nfrom nltk import word_tokenize\n\ndef build_vocab(dataset, max_vocab_size, stop_words):\n    counter = Counter()\n    for item in dataset:\n        tokens = word_tokenize(item['text'].lower())\n        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n        counter.update(tokens)\n    \n    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]']\n    vocab_tokens = special_tokens + [t for t, _ in counter.most_common(max_vocab_size)]\n    return {token: idx for idx, token in enumerate(vocab_tokens)}", "code": "def build_vocab(dataset, max_vocab_size, stop_words):\n    counter = Counter()\n    for item in dataset:\n        tokens = word_tokenize(item['text'].lower())\n        tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n        counter.update(tokens)\n    vocab_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + [token for token, _ in counter.most_common(max_vocab_size)]\n    return {token: idx for idx, token in enumerate(vocab_tokens)}"}, "node12": {"name": "AGNewsDataset", "type": "class", "file": "hdcnn.py", "importance": 4, "file_path": "hdcnn/hdcnn.py", "edges": ["node15", "node3", "node14"], "opacity": 0.9333333333333333, "level": 2, "summary": "This code defines a custom PyTorch Dataset class called AGNewsDataset for processing and encoding text data. It preprocesses text by tokenizing, removing stop words and punctuation, truncating to a maximum length, and adding special tokens. The processed tokens are then encoded into high-dimensional vectors using hyperdimensional computing techniques. The class provides methods to get the dataset length and retrieve encoded items with their labels.", "minimal_code": "from torch.utils.data import Dataset\nfrom nltk.tokenize import word_tokenize\n\nclass AGNewsDataset(Dataset):\n    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n        self.data = data\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.max_seq_len = max_seq_len\n        self.stop_words = stop_words\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        text, label = item['text'], item['label']\n        tokens = word_tokenize(text.lower())\n        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\ndef encode_sequence(tokens, token_hvs, hd):\n    # Placeholder for encode_sequence function\n    # This should be implemented based on the HDComputing methods\n    pass", "code": "class AGNewsDataset(Dataset):\n    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n        self.data = data\n        self.vocab = vocab\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.max_seq_len = max_seq_len\n        self.stop_words = stop_words\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        text = item['text']\n        label = item['label']\n        # Tokenize using NLTK\n        tokens = word_tokenize(text.lower())\n        # Remove stop words and punctuation\n        tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n        tokens = tokens[:self.max_seq_len]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"}, "node1": {"name": "hdcnn.py", "type": "file", "file": "hdcnn.py", "importance": 15, "file_path": "hdcnn/hdcnn.py", "edges": ["node16", "node18", "node6", "node9", "node14", "node4", "node12", "node15", "node2", "node11", "node10", "node3", "node8", "node5"], "opacity": 1.0, "level": 1, "summary": "This code implements a text classification system for the AG News dataset using Hyperdimensional Computing (HDC) and a neural network. It processes text data into high-dimensional vectors, trains a simple neural network classifier, and compares its performance with a Naive Bayes baseline. The system includes data preprocessing, vocabulary building, HDC encoding, model training with early stopping, and evaluation.", "minimal_code": "", "code": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom datasets import load_dataset\nfrom collections import Counter\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# Download NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\nrandom.seed(42)\n\n# Define the Hyperdimensional Computing class\nclass HDComputing:\n    def __init__(self, dim, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n    def superpose(self, hvs):\n        sum_hv = np.sum(hvs, axis=0)\n        return np.sign(sum_hv)\n\n    def bind(self, hv1, hv2):\n        return hv1 * hv2\n\n    def permute(self, hv, shifts=1):\n        return np.roll(hv, shifts)\n\n# Custom Dataset Class for AG News\nclass AGNewsDataset(Dataset):\n    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n        self.data = data\n        self.vocab = vocab\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.max_seq_len = max_seq_len\n        self.stop_words = stop_words\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        text = item['text']\n        label = item['label']\n        # Tokenize using NLTK\n        tokens = word_tokenize(text.lower())\n        # Remove stop words and punctuation\n        tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n        tokens = tokens[:self.max_seq_len]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# Function to create token hypervectors\ndef create_token_hvs(vocab, dim, hd):\n    return {token: hd.random_hv() for token in vocab}\n\n# Function to encode sequences into hypervectors\ndef encode_sequence(tokens, token_hvs, hd):\n    sequence_hv = np.zeros(hd.dim)\n    for i, token in enumerate(tokens):\n        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n        permuted_token_hv = hd.permute(token_hv, shifts=i)\n        sequence_hv += permuted_token_hv\n    return np.sign(sequence_hv)\n\n# Define the HDC Neural Network model for multi-class classification\nclass HDCNNClassifier(nn.Module):\n    def __init__(self, dim, num_classes):\n        super(HDCNNClassifier, self).__init__()\n        self.fc1 = nn.Linear(dim, 512)\n        self.activation = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.6)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.activation(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out\n\n# Helper function to build vocabulary\ndef build_vocab(dataset, max_vocab_size, stop_words):\n    counter = Counter()\n    for item in dataset:\n        tokens = word_tokenize(item['text'].lower())\n        tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n        counter.update(tokens)\n    vocab_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + [token for token, _ in counter.most_common(max_vocab_size)]\n    return {token: idx for idx, token in enumerate(vocab_tokens)}\n\n# Main function to train and evaluate the model\ndef main():\n    # Initialize parameters\n    dim = 5000\n    hd = HDComputing(dim, seed=42)\n    max_vocab_size = 5000\n    max_seq_len = 50\n    batch_size = 128\n    num_epochs = 5\n    learning_rate = 0.001\n    num_classes = 4\n    stop_words = set(stopwords.words('english'))\n\n    # Load AG News dataset\n    dataset = load_dataset('ag_news')\n    full_train_data = dataset['train']\n    full_test_data = dataset['test']\n\n    # Increase dataset size\n    train_size = len(full_train_data)\n    test_size = len(full_test_data)\n\n    # Use stratified sampling to maintain class distribution\n    train_data = full_train_data.shuffle(seed=42)\n    test_data = full_test_data.shuffle(seed=42)\n\n    # Build vocabulary only on the full training set\n    vocab = build_vocab(train_data, max_vocab_size, stop_words)\n\n    # Create token hypervectors\n    token_hvs = create_token_hvs(vocab, dim, hd)\n\n    # Create datasets\n    train_dataset_full = AGNewsDataset(train_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n    test_dataset = AGNewsDataset(test_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n\n    # Split training data into training and validation sets\n    val_size = int(0.1 * len(train_dataset_full))\n    train_size = len(train_dataset_full) - val_size\n    train_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Implement simple baseline (Naive Bayes)\n    vectorizer = CountVectorizer(stop_words='english', max_features=max_vocab_size)\n    X_train = vectorizer.fit_transform(train_data['text'])\n    X_test = vectorizer.transform(test_data['text'])\n    nb_classifier = MultinomialNB()\n    nb_classifier.fit(X_train, train_data['label'])\n    nb_predictions = nb_classifier.predict(X_test)\n    nb_accuracy = accuracy_score(test_data['label'], nb_predictions)\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")\n\n    # Initialize the model, loss function, and optimizer with weight decay\n    model = HDCNNClassifier(dim, num_classes)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n    # Move model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Early stopping parameters\n    best_val_loss = float('inf')\n    patience = 2\n    trigger_times = 0\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            if (batch_idx + 1) % 50 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n\n        # Evaluate on validation set\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(val_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n\n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            trigger_times = 0\n            # Save the best model\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            trigger_times += 1\n            if trigger_times >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load the best model\n    model.load_state_dict(torch.load('best_model.pth'))\n\n    # Evaluate on test set\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.4f}')\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")\n\nif __name__ == '__main__':\n    main()\n"}, "node15": {"name": "AGNewsDataset::__getitem__", "type": "method", "file": "hdcnn.py", "importance": 2, "file_path": "hdcnn/hdcnn.py", "edges": ["node3"], "opacity": 0.8, "level": 3, "summary": "This code defines a custom dataset item retrieval method. It processes text data by tokenizing, removing stop words and punctuation, truncating to a maximum length, and adding special tokens. The processed tokens are then encoded into a high-dimensional vector using hyperdimensional computing. The method returns this encoded vector along with the corresponding label.", "minimal_code": "import torch\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nclass CustomDataset:\n    def __init__(self, data, token_hvs, hd, max_seq_len=512):\n        self.data = data\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.max_seq_len = max_seq_len\n        self.stop_words = set(stopwords.words('english'))\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        text, label = item['text'], item['label']\n        \n        tokens = word_tokenize(text.lower())\n        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\n        \n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\ndef encode_sequence(tokens, token_hvs, hd):\n    # Placeholder for encode_sequence implementation\n    return [0] * hd  # Returns a zero vector of length hd", "code": "def __getitem__(self, idx):\n    item = self.data[idx]\n    text = item['text']\n    label = item['label']\n    # Tokenize using NLTK\n    tokens = word_tokenize(text.lower())\n    # Remove stop words and punctuation\n    tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n    tokens = tokens[:self.max_seq_len]\n    tokens = ['[CLS]'] + tokens + ['[SEP]']\n    seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n    return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"}, "node2": {"name": "create_token_hvs", "type": "function", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This function creates a dictionary of high-dimensional vectors (HVs) for a given vocabulary. Each token in the vocabulary is assigned a random HV generated using a hyperdimensional computing (HD) object. The function uses a dictionary comprehension to efficiently create this mapping.", "minimal_code": "def create_token_hvs(vocab, dim, hd):\n    return {token: hd.random_hv() for token in vocab}", "code": "def create_token_hvs(vocab, dim, hd):\n    return {token: hd.random_hv() for token in vocab}"}, "node11": {"name": "HDComputing::permute", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "The `permute` function performs a circular shift on a given high-dimensional vector (hv). It uses NumPy's `roll` function to shift the elements of the vector by a specified number of positions (default is 1). This operation is typically used in hyperdimensional computing to create position-dependent representations of tokens in a sequence.", "minimal_code": "import numpy as np\n\ndef permute(hv, shifts=1):\n    return np.roll(hv, shifts)", "code": "def permute(self, hv, shifts=1):\n    return np.roll(hv, shifts)"}, "node10": {"name": "HDComputing::bind", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "The `bind` function performs element-wise multiplication of two hypervectors (HVs). This operation is commonly used in hyperdimensional computing to associate or bind information from two different HVs, creating a new HV that represents their combined information.", "minimal_code": "def bind(self, hv1, hv2):\n    return hv1 * hv2", "code": "def bind(self, hv1, hv2):\n    return hv1 * hv2"}, "node3": {"name": "encode_sequence", "type": "function", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 4, "summary": "This function encodes a sequence of tokens into a single high-dimensional vector (HV) using hyperdimensional computing techniques. It iterates through the tokens, retrieves their corresponding HVs, applies position-based permutations, and combines them using element-wise addition. The final result is binarized using the sign function.", "minimal_code": "import numpy as np\n\ndef encode_sequence(tokens, token_hvs, hd):\n    sequence_hv = np.zeros(hd.dim)\n    for i, token in enumerate(tokens):\n        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n        permuted_token_hv = hd.permute(token_hv, shifts=i)\n        sequence_hv += permuted_token_hv\n    return np.sign(sequence_hv)", "code": "def encode_sequence(tokens, token_hvs, hd):\n    sequence_hv = np.zeros(hd.dim)\n    for i, token in enumerate(tokens):\n        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n        permuted_token_hv = hd.permute(token_hv, shifts=i)\n        sequence_hv += permuted_token_hv\n    return np.sign(sequence_hv)"}, "node8": {"name": "HDComputing::random_hv", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "The `random_hv` function generates a random hypervector (HV) of a specified dimension. It uses the object's random state to choose between -1 and 1 for each element of the vector, effectively creating a binary hypervector of the given dimension.", "minimal_code": "import numpy as np\n\nclass HDComputing:\n    def __init__(self, dim, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n# Usage example\nhd = HDComputing(dim=10000, seed=42)\nrandom_vector = hd.random_hv()\nprint(random_vector)", "code": "def random_hv(self):\n    return self.random_state.choice([-1, 1], size=self.dim)"}, "node5": {"name": "main", "type": "function", "file": "hdcnn.py", "importance": 3, "file_path": "hdcnn/hdcnn.py", "edges": ["node2", "node4"], "opacity": 0.8666666666666667, "level": 2, "summary": "This code implements a text classification task using hyperdimensional computing (HD) and a neural network on the AG News dataset. It includes data preprocessing, vocabulary building, HD encoding, model training with early stopping, and evaluation. The program also implements a Naive Bayes classifier as a baseline for comparison.", "minimal_code": "", "code": "def main():\n    # Initialize parameters\n    dim = 5000\n    hd = HDComputing(dim, seed=42)\n    max_vocab_size = 5000\n    max_seq_len = 50\n    batch_size = 128\n    num_epochs = 5\n    learning_rate = 0.001\n    num_classes = 4\n    stop_words = set(stopwords.words('english'))\n\n    # Load AG News dataset\n    dataset = load_dataset('ag_news')\n    full_train_data = dataset['train']\n    full_test_data = dataset['test']\n\n    # Increase dataset size\n    train_size = len(full_train_data)\n    test_size = len(full_test_data)\n\n    # Use stratified sampling to maintain class distribution\n    train_data = full_train_data.shuffle(seed=42)\n    test_data = full_test_data.shuffle(seed=42)\n\n    # Build vocabulary only on the full training set\n    vocab = build_vocab(train_data, max_vocab_size, stop_words)\n\n    # Create token hypervectors\n    token_hvs = create_token_hvs(vocab, dim, hd)\n\n    # Create datasets\n    train_dataset_full = AGNewsDataset(train_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n    test_dataset = AGNewsDataset(test_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n\n    # Split training data into training and validation sets\n    val_size = int(0.1 * len(train_dataset_full))\n    train_size = len(train_dataset_full) - val_size\n    train_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Implement simple baseline (Naive Bayes)\n    vectorizer = CountVectorizer(stop_words='english', max_features=max_vocab_size)\n    X_train = vectorizer.fit_transform(train_data['text'])\n    X_test = vectorizer.transform(test_data['text'])\n    nb_classifier = MultinomialNB()\n    nb_classifier.fit(X_train, train_data['label'])\n    nb_predictions = nb_classifier.predict(X_test)\n    nb_accuracy = accuracy_score(test_data['label'], nb_predictions)\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")\n\n    # Initialize the model, loss function, and optimizer with weight decay\n    model = HDCNNClassifier(dim, num_classes)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n    # Move model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Early stopping parameters\n    best_val_loss = float('inf')\n    patience = 2\n    trigger_times = 0\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            if (batch_idx + 1) % 50 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n\n        # Evaluate on validation set\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(val_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n\n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            trigger_times = 0\n            # Save the best model\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            trigger_times += 1\n            if trigger_times >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load the best model\n    model.load_state_dict(torch.load('best_model.pth'))\n\n    # Evaluate on test set\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.4f}')\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")"}}