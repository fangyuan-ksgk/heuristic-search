{"node5": {"name": "main", "type": "function", "file": "hdcnn.py", "importance": 3, "file_path": "hdcnn/hdcnn.py", "edges": ["node2", "node4"], "opacity": 0.8666666666666667, "level": 2, "summary": "This code implements a text classification model using Hyperdimensional Computing (HDC) and a neural network for the AG News dataset. It includes data loading, preprocessing, vocabulary building, and encoding text into high-dimensional vectors. The main steps involve training a neural network model (HDCNNClassifier) with early stopping, evaluating on a test set, and comparing performance with a Naive Bayes baseline.", "minimal_code": "", "code": "def main():\n    # Initialize parameters\n    dim = 5000\n    hd = HDComputing(dim, seed=42)\n    max_vocab_size = 5000\n    max_seq_len = 50\n    batch_size = 128\n    num_epochs = 5\n    learning_rate = 0.001\n    num_classes = 4\n    stop_words = set(stopwords.words('english'))\n\n    # Load AG News dataset\n    dataset = load_dataset('ag_news')\n    full_train_data = dataset['train']\n    full_test_data = dataset['test']\n\n    # Increase dataset size\n    train_size = len(full_train_data)\n    test_size = len(full_test_data)\n\n    # Use stratified sampling to maintain class distribution\n    train_data = full_train_data.shuffle(seed=42)\n    test_data = full_test_data.shuffle(seed=42)\n\n    # Build vocabulary only on the full training set\n    vocab = build_vocab(train_data, max_vocab_size, stop_words)\n\n    # Create token hypervectors\n    token_hvs = create_token_hvs(vocab, dim, hd)\n\n    # Create datasets\n    train_dataset_full = AGNewsDataset(train_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n    test_dataset = AGNewsDataset(test_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n\n    # Split training data into training and validation sets\n    val_size = int(0.1 * len(train_dataset_full))\n    train_size = len(train_dataset_full) - val_size\n    train_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Implement simple baseline (Naive Bayes)\n    vectorizer = CountVectorizer(stop_words='english', max_features=max_vocab_size)\n    X_train = vectorizer.fit_transform(train_data['text'])\n    X_test = vectorizer.transform(test_data['text'])\n    nb_classifier = MultinomialNB()\n    nb_classifier.fit(X_train, train_data['label'])\n    nb_predictions = nb_classifier.predict(X_test)\n    nb_accuracy = accuracy_score(test_data['label'], nb_predictions)\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")\n\n    # Initialize the model, loss function, and optimizer with weight decay\n    model = HDCNNClassifier(dim, num_classes)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n    # Move model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Early stopping parameters\n    best_val_loss = float('inf')\n    patience = 2\n    trigger_times = 0\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            if (batch_idx + 1) % 50 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n\n        # Evaluate on validation set\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(val_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n\n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            trigger_times = 0\n            # Save the best model\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            trigger_times += 1\n            if trigger_times >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load the best model\n    model.load_state_dict(torch.load('best_model.pth'))\n\n    # Evaluate on test set\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.4f}')\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")"}, "node9": {"name": "HDComputing::superpose", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "The `superpose` method performs vector superposition, a key operation in Hyperdimensional Computing. It takes a list of high-dimensional vectors (HVs) as input, sums them element-wise, and then applies the sign function to the result. This creates a new HV that represents the collective information from all input vectors.", "minimal_code": "import numpy as np\n\ndef superpose(hvs):\n    return np.sign(np.sum(hvs, axis=0))", "code": "def superpose(self, hvs):\n    sum_hv = np.sum(hvs, axis=0)\n    return np.sign(sum_hv)"}, "node14": {"name": "AGNewsDataset::__len__", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This code defines a `__len__` method for a class. It returns the length of the `data` attribute of the class instance. This is typically used to make an object \"sized,\" allowing it to work with Python's built-in `len()` function.", "minimal_code": "class SomeClass:\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)", "code": "def __len__(self):\n    return len(self.data)"}, "node8": {"name": "HDComputing::random_hv", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This function, `random_hv`, generates a random high-dimensional vector (HV) of binary values (-1 or 1) with a specified dimension. It uses a random state object to ensure reproducibility and returns the generated vector.", "minimal_code": "import numpy as np\n\nclass HDComputing:\n    def __init__(self, dim=10000, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n# Usage example\nhd = HDComputing(dim=1000, seed=42)\nrandom_vector = hd.random_hv()\nprint(random_vector)", "code": "def random_hv(self):\n    return self.random_state.choice([-1, 1], size=self.dim)"}, "node3": {"name": "encode_sequence", "type": "function", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 4, "summary": "This function encodes a sequence of tokens into a single high-dimensional vector (HV) using Hyperdimensional Computing techniques. It does this by:", "minimal_code": "import numpy as np\n\ndef encode_sequence(tokens, token_hvs, hd):\n    sequence_hv = np.zeros(hd.dim)\n    for i, token in enumerate(tokens):\n        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n        permuted_token_hv = hd.permute(token_hv, shifts=i)\n        sequence_hv += permuted_token_hv\n    return np.sign(sequence_hv)", "code": "def encode_sequence(tokens, token_hvs, hd):\n    sequence_hv = np.zeros(hd.dim)\n    for i, token in enumerate(tokens):\n        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n        permuted_token_hv = hd.permute(token_hv, shifts=i)\n        sequence_hv += permuted_token_hv\n    return np.sign(sequence_hv)"}, "node12": {"name": "AGNewsDataset", "type": "class", "file": "hdcnn.py", "importance": 4, "file_path": "hdcnn/hdcnn.py", "edges": ["node14", "node15", "node3"], "opacity": 0.9333333333333333, "level": 2, "summary": "This code defines a custom PyTorch Dataset class called AGNewsDataset for processing text data from the AG News dataset. It tokenizes and preprocesses the text, removes stop words, applies length constraints, and encodes the text into high-dimensional vectors using hyperdimensional computing techniques. The class provides methods to get the dataset length and retrieve individual items as encoded vectors and labels.", "minimal_code": "from torch.utils.data import Dataset\nfrom nltk import word_tokenize\nimport torch\n\nclass AGNewsDataset(Dataset):\n    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n        self.data = data\n        self.vocab = vocab\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.max_seq_len = max_seq_len\n        self.stop_words = stop_words\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        text = item['text']\n        label = item['label']\n        \n        tokens = word_tokenize(text.lower())\n        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\n        \n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# Note: The encode_sequence function is not provided in this minimal implementation\n# as it depends on the specific hyperdimensional computing library being used.", "code": "class AGNewsDataset(Dataset):\n    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n        self.data = data\n        self.vocab = vocab\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.max_seq_len = max_seq_len\n        self.stop_words = stop_words\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        text = item['text']\n        label = item['label']\n        # Tokenize using NLTK\n        tokens = word_tokenize(text.lower())\n        # Remove stop words and punctuation\n        tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n        tokens = tokens[:self.max_seq_len]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"}, "node18": {"name": "HDCNNClassifier::forward", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This code defines a forward pass for a simple neural network layer. It applies a fully connected layer, an activation function, dropout for regularization, and another fully connected layer to the input. This structure is common in many neural network architectures, representing a basic building block of a multi-layer perceptron.", "minimal_code": "import torch\nimport torch.nn as nn\n\nclass SimpleLayer(nn.Module):\n    def __init__(self, input_size, output_size):\n        super(SimpleLayer, self).__init__()\n        self.fc1 = nn.Linear(input_size, 64)\n        self.activation = nn.ReLU()\n        self.dropout = nn.Dropout(0.5)\n        self.fc2 = nn.Linear(64, output_size)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.activation(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out", "code": "def forward(self, x):\n    out = self.fc1(x)\n    out = self.activation(out)\n    out = self.dropout(out)\n    out = self.fc2(out)\n    return out"}, "node4": {"name": "build_vocab", "type": "function", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This function builds a vocabulary from a dataset of text items. It tokenizes the text, removes non-alphabetic words and stop words, counts word frequencies, and creates a vocabulary dictionary with the most common words up to a specified maximum size. It also includes special tokens like '[PAD]', '[UNK]', '[CLS]', and '[SEP]'.", "minimal_code": "from collections import Counter\nfrom nltk.tokenize import word_tokenize\n\ndef build_vocab(dataset, max_vocab_size, stop_words):\n    counter = Counter()\n    for item in dataset:\n        tokens = word_tokenize(item['text'].lower())\n        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n        counter.update(tokens)\n    \n    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]']\n    vocab_tokens = special_tokens + [t for t, _ in counter.most_common(max_vocab_size)]\n    return {token: idx for idx, token in enumerate(vocab_tokens)}", "code": "def build_vocab(dataset, max_vocab_size, stop_words):\n    counter = Counter()\n    for item in dataset:\n        tokens = word_tokenize(item['text'].lower())\n        tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n        counter.update(tokens)\n    vocab_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + [token for token, _ in counter.most_common(max_vocab_size)]\n    return {token: idx for idx, token in enumerate(vocab_tokens)}"}, "node2": {"name": "create_token_hvs", "type": "function", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "This function, `create_token_hvs`, generates a dictionary where each token in a given vocabulary is associated with a random high-dimensional vector (HV). It uses a hyperdimensional computing library (represented by the `hd` parameter) to create these random vectors. The function takes three parameters: the vocabulary, the dimension of the vectors, and the hyperdimensional computing object.", "minimal_code": "def create_token_hvs(vocab, dim, hd):\n    return {token: hd.random_hv() for token in vocab}", "code": "def create_token_hvs(vocab, dim, hd):\n    return {token: hd.random_hv() for token in vocab}"}, "node10": {"name": "HDComputing::bind", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "The `bind` function performs element-wise multiplication of two high-dimensional vectors (HVs). This operation is a fundamental binding operation in Hyperdimensional Computing, used to combine or associate two vectors while preserving their information.", "minimal_code": "def bind(self, hv1, hv2):\n    return hv1 * hv2", "code": "def bind(self, hv1, hv2):\n    return hv1 * hv2"}, "node6": {"name": "HDComputing", "type": "class", "file": "hdcnn.py", "importance": 5, "file_path": "hdcnn/hdcnn.py", "edges": ["node9", "node10", "node8", "node11"], "opacity": 1.0, "level": 2, "summary": "This code defines a class `HDComputing` that implements core operations for Hyperdimensional Computing. It includes methods for generating random high-dimensional vectors, superposing multiple vectors, binding two vectors, and permuting a vector. These operations are fundamental in creating and manipulating high-dimensional representations of data in HDC systems.", "minimal_code": "import numpy as np\n\nclass HDComputing:\n    def __init__(self, dim, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n    def superpose(self, hvs):\n        return np.sign(np.sum(hvs, axis=0))\n\n    def bind(self, hv1, hv2):\n        return hv1 * hv2\n\n    def permute(self, hv, shifts=1):\n        return np.roll(hv, shifts)", "code": "class HDComputing:\n    def __init__(self, dim, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n    def superpose(self, hvs):\n        sum_hv = np.sum(hvs, axis=0)\n        return np.sign(sum_hv)\n\n    def bind(self, hv1, hv2):\n        return hv1 * hv2\n\n    def permute(self, hv, shifts=1):\n        return np.roll(hv, shifts)"}, "node11": {"name": "HDComputing::permute", "type": "method", "file": "hdcnn.py", "importance": 1, "file_path": "hdcnn/hdcnn.py", "edges": [], "opacity": 0.7333333333333334, "level": 3, "summary": "The `permute` function performs a circular shift on a given high-dimensional vector (HV). It rotates the elements of the vector by a specified number of positions, wrapping around at the ends. This operation is commonly used in Hyperdimensional Computing to create unique representations of sequential data.", "minimal_code": "import numpy as np\n\ndef permute(hv, shifts=1):\n    return np.roll(hv, shifts)", "code": "def permute(self, hv, shifts=1):\n    return np.roll(hv, shifts)"}, "node1": {"name": "hdcnn.py", "type": "file", "file": "hdcnn.py", "importance": 15, "file_path": "hdcnn/hdcnn.py", "edges": ["node5", "node9", "node14", "node8", "node3", "node12", "node18", "node4", "node2", "node10", "node6", "node11", "node16", "node15"], "opacity": 1.0, "level": 1, "summary": "This code implements a text classification model using Hyperdimensional Computing (HDC) and a neural network for the AG News dataset. It includes data loading, preprocessing, vocabulary building, and encoding text into high-dimensional vectors. The main steps involve training a neural network model (HDCNNClassifier) with early stopping, evaluating on a test set, and comparing performance with a Naive Bayes baseline.", "minimal_code": "", "code": "import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset, random_split\nfrom datasets import load_dataset\nfrom collections import Counter\nimport random\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\n# Download NLTK data\nnltk.download('punkt')\nnltk.download('stopwords')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\ntorch.manual_seed(42)\nrandom.seed(42)\n\n# Define the Hyperdimensional Computing class\nclass HDComputing:\n    def __init__(self, dim, seed=None):\n        self.dim = dim\n        self.random_state = np.random.RandomState(seed)\n\n    def random_hv(self):\n        return self.random_state.choice([-1, 1], size=self.dim)\n\n    def superpose(self, hvs):\n        sum_hv = np.sum(hvs, axis=0)\n        return np.sign(sum_hv)\n\n    def bind(self, hv1, hv2):\n        return hv1 * hv2\n\n    def permute(self, hv, shifts=1):\n        return np.roll(hv, shifts)\n\n# Custom Dataset Class for AG News\nclass AGNewsDataset(Dataset):\n    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n        self.data = data\n        self.vocab = vocab\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.max_seq_len = max_seq_len\n        self.stop_words = stop_words\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        text = item['text']\n        label = item['label']\n        # Tokenize using NLTK\n        tokens = word_tokenize(text.lower())\n        # Remove stop words and punctuation\n        tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n        tokens = tokens[:self.max_seq_len]\n        tokens = ['[CLS]'] + tokens + ['[SEP]']\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n\n# Function to create token hypervectors\ndef create_token_hvs(vocab, dim, hd):\n    return {token: hd.random_hv() for token in vocab}\n\n# Function to encode sequences into hypervectors\ndef encode_sequence(tokens, token_hvs, hd):\n    sequence_hv = np.zeros(hd.dim)\n    for i, token in enumerate(tokens):\n        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n        permuted_token_hv = hd.permute(token_hv, shifts=i)\n        sequence_hv += permuted_token_hv\n    return np.sign(sequence_hv)\n\n# Define the HDC Neural Network model for multi-class classification\nclass HDCNNClassifier(nn.Module):\n    def __init__(self, dim, num_classes):\n        super(HDCNNClassifier, self).__init__()\n        self.fc1 = nn.Linear(dim, 512)\n        self.activation = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.6)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.activation(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out\n\n# Helper function to build vocabulary\ndef build_vocab(dataset, max_vocab_size, stop_words):\n    counter = Counter()\n    for item in dataset:\n        tokens = word_tokenize(item['text'].lower())\n        tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n        counter.update(tokens)\n    vocab_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]'] + [token for token, _ in counter.most_common(max_vocab_size)]\n    return {token: idx for idx, token in enumerate(vocab_tokens)}\n\n# Main function to train and evaluate the model\ndef main():\n    # Initialize parameters\n    dim = 5000\n    hd = HDComputing(dim, seed=42)\n    max_vocab_size = 5000\n    max_seq_len = 50\n    batch_size = 128\n    num_epochs = 5\n    learning_rate = 0.001\n    num_classes = 4\n    stop_words = set(stopwords.words('english'))\n\n    # Load AG News dataset\n    dataset = load_dataset('ag_news')\n    full_train_data = dataset['train']\n    full_test_data = dataset['test']\n\n    # Increase dataset size\n    train_size = len(full_train_data)\n    test_size = len(full_test_data)\n\n    # Use stratified sampling to maintain class distribution\n    train_data = full_train_data.shuffle(seed=42)\n    test_data = full_test_data.shuffle(seed=42)\n\n    # Build vocabulary only on the full training set\n    vocab = build_vocab(train_data, max_vocab_size, stop_words)\n\n    # Create token hypervectors\n    token_hvs = create_token_hvs(vocab, dim, hd)\n\n    # Create datasets\n    train_dataset_full = AGNewsDataset(train_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n    test_dataset = AGNewsDataset(test_data, vocab, token_hvs, hd, max_seq_len, stop_words)\n\n    # Split training data into training and validation sets\n    val_size = int(0.1 * len(train_dataset_full))\n    train_size = len(train_dataset_full) - val_size\n    train_dataset, val_dataset = random_split(train_dataset_full, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n\n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n    # Implement simple baseline (Naive Bayes)\n    vectorizer = CountVectorizer(stop_words='english', max_features=max_vocab_size)\n    X_train = vectorizer.fit_transform(train_data['text'])\n    X_test = vectorizer.transform(test_data['text'])\n    nb_classifier = MultinomialNB()\n    nb_classifier.fit(X_train, train_data['label'])\n    nb_predictions = nb_classifier.predict(X_test)\n    nb_accuracy = accuracy_score(test_data['label'], nb_predictions)\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")\n\n    # Initialize the model, loss function, and optimizer with weight decay\n    model = HDCNNClassifier(dim, num_classes)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\n    # Move model to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Early stopping parameters\n    best_val_loss = float('inf')\n    patience = 2\n    trigger_times = 0\n\n    # Training loop\n    for epoch in range(num_epochs):\n        model.train()\n        total_loss = 0\n        for batch_idx, (inputs, labels) in enumerate(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n            if (batch_idx + 1) % 50 == 0:\n                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n\n        avg_loss = total_loss / len(train_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Training Loss: {avg_loss:.4f}')\n\n        # Evaluate on validation set\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for inputs, labels in val_loader:\n                inputs, labels = inputs.to(device), labels.to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        avg_val_loss = val_loss / len(val_loader)\n        print(f'Epoch [{epoch+1}/{num_epochs}], Validation Loss: {avg_val_loss:.4f}')\n\n        # Early stopping check\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            trigger_times = 0\n            # Save the best model\n            torch.save(model.state_dict(), 'best_model.pth')\n        else:\n            trigger_times += 1\n            if trigger_times >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    # Load the best model\n    model.load_state_dict(torch.load('best_model.pth'))\n\n    # Evaluate on test set\n    model.eval()\n    correct = 0\n    total = 0\n    test_loss = 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            test_loss += loss.item()\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n    accuracy = correct / total\n    avg_test_loss = test_loss / len(test_loader)\n    print(f'Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.4f}')\n    print(f\"Naive Bayes Baseline Accuracy: {nb_accuracy:.4f}\")\n\nif __name__ == '__main__':\n    main()\n"}, "node15": {"name": "AGNewsDataset::__getitem__", "type": "method", "file": "hdcnn.py", "importance": 2, "file_path": "hdcnn/hdcnn.py", "edges": ["node3"], "opacity": 0.8, "level": 3, "summary": "This code defines a custom dataset's __getitem__ method, which prepares text data for machine learning. It tokenizes the text, removes stop words and punctuation, truncates or pads the sequence, adds special tokens, and encodes the sequence into a high-dimensional vector using a hyperdimensional computing approach. It returns the encoded sequence and its label as PyTorch tensors.", "minimal_code": "import torch\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n\nclass CustomDataset:\n    def __init__(self, data, max_seq_len, token_hvs, hd):\n        self.data = data\n        self.max_seq_len = max_seq_len\n        self.token_hvs = token_hvs\n        self.hd = hd\n        self.stop_words = set(stopwords.words('english'))\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        tokens = word_tokenize(item['text'].lower())\n        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(item['label'], dtype=torch.long)\n\ndef encode_sequence(tokens, token_hvs, hd):\n    # Implement hyperdimensional encoding here\n    pass", "code": "def __getitem__(self, idx):\n    item = self.data[idx]\n    text = item['text']\n    label = item['label']\n    # Tokenize using NLTK\n    tokens = word_tokenize(text.lower())\n    # Remove stop words and punctuation\n    tokens = [token for token in tokens if token.isalpha() and token not in self.stop_words]\n    tokens = tokens[:self.max_seq_len]\n    tokens = ['[CLS]'] + tokens + ['[SEP]']\n    seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n    return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"}, "node16": {"name": "HDCNNClassifier", "type": "class", "file": "hdcnn.py", "importance": 2, "file_path": "hdcnn/hdcnn.py", "edges": ["node18"], "opacity": 0.8, "level": 2, "summary": "This code defines a simple neural network classifier called HDCNNClassifier using PyTorch. It's designed to work with high-dimensional input vectors (likely from Hyperdimensional Computing) and classify them into a specified number of classes. The network consists of two fully connected layers with dropout and ReLU activation in between, forming a basic feedforward neural network structure.", "minimal_code": "import torch.nn as nn\n\nclass HDCNNClassifier(nn.Module):\n    def __init__(self, input_dim, num_classes):\n        super(HDCNNClassifier, self).__init__()\n        self.model = nn.Sequential(\n            nn.Linear(input_dim, 512),\n            nn.ReLU(),\n            nn.Dropout(0.6),\n            nn.Linear(512, num_classes)\n        )\n\n    def forward(self, x):\n        return self.model(x)", "code": "class HDCNNClassifier(nn.Module):\n    def __init__(self, dim, num_classes):\n        super(HDCNNClassifier, self).__init__()\n        self.fc1 = nn.Linear(dim, 512)\n        self.activation = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.6)\n        self.fc2 = nn.Linear(512, num_classes)\n\n    def forward(self, x):\n        out = self.fc1(x)\n        out = self.activation(out)\n        out = self.dropout(out)\n        out = self.fc2(out)\n        return out"}}