{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper Analysis + Nirvana"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-q8i6hwzq52sb9o\"\n",
    "api_key = os.environ[\"RUNPOD_API_KEY\"]\n",
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (I). Given a paper's link, extract author & institution & citations\n",
    "\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.evolnode import EvolNode\n",
    "\n",
    "extract_paper_info = MetaPrompt(\n",
    "    \"Extract author, institution, citations, date from a paper's link.\",\n",
    "    \"extract_paper_info\",\n",
    "    [\"link\"],\n",
    "    [\"authors\", \"institution\", \"citations\", \"date\"],\n",
    "    [\"str\"],\n",
    "    [\"list\", \"str\", \"list\", \"str\"],\n",
    "    PromptMode.PROMPT\n",
    ")\n",
    "\n",
    "extract_paper_info_test_cases  = [\n",
    "    ({\"link\": \"https://arxiv.org/abs/2402.15391\"}, {\"authors\": [\"Jake Bruce\", \"Michael Dennis\", \"Ashley Edwards\", \"Jack Parker-Holder\", \"Yuge Shi\", \"Edward Hughes\", \"Matthew Lai\", \"Aditi Mavalankar\", \"Richie Steigerwald\", \"Chris Apps\", \"Yusuf Aytar\", \"Sarah Bechtle\", \"Feryal Behbahani\", \"Stephanie Chan\", \"Nicolas Heess\", \"Lucy Gonzalez\", \"Simon Osindero\", \"Sherjil Ozair\", \"Scott Reed\", \"Jingwei Zhang\", \"Konrad Zolna\", \"Jeff Clune\", \"Nando de Freitas\", \"Satinder Singh\", \"Tim Rocktäschel\"],\n",
    "                                                    \"institution\": \"Google DeepMind\",\n",
    "                                                    \"citations\": [\"The deepmind jax ecosystem\",\n",
    "                                                                \"Frozen in time: A joint video and image encoder for end-to-end retrieval\",\n",
    "                                                                \"Video pretraining (vpt): Learning to act by watching unlabeled online videos\",\n",
    "                                                                \"Neural game engine: Accurate learning of generalizable forward models from pixels\",\n",
    "                                                                \"Human-timescale adaptation in an open-ended task space\",\n",
    "                                                                \"Stable video diffusion: Scaling latent video diffusion models to large datasets\",\n",
    "                                                                \"Align your latents: High-resolution video synthesis with latent diffusion models\"],\n",
    "                                                    \"date\": \"2024-02-15\"}),\n",
    "    ({\"link\": \"https://arxiv.org/abs/2411.18553\"}, {\"authors\": [\"Darius Feher\", \"Benjamin Minixhofer\", \"Ivan Vulić\"],\n",
    "                                                    \"institution\": \"University of Cambridge\",\n",
    "                                                    \"citations\": [\"Do all languages cost the same? tokenization in the era of commercial language models\",\n",
    "                                                                \"CharacterBERT: Reconciling ELMo and BERT for word-level open-vocabulary representations from characters\",\n",
    "                                                                \"Can Large Language Models Be an Alternative to Human Evaluations?\",\n",
    "                                                                \"Canine: Pre-training an efficient tokenization-free encoder for language representation\",\n",
    "                                                                \"Unsupervised Cross-lingual Representation Learning at Scale\"],\n",
    "                                                    \"date\": \"2024-11-18\"})\n",
    "    ]\n",
    "\n",
    "\n",
    "node = EvolNode(extract_paper_info, None, None, get_response=get_endpoint_response, test_cases=extract_paper_info_test_cases)\n",
    "node.evolve(\"i1\", replace=True, batch_size=20, num_runs=2, print_summary=True)\n",
    "\n",
    "input_dict = {\"link\": \"https://arxiv.org/abs/2411.18553\"}\n",
    "output_dict = node(input_dict)\n",
    "print(\"Output dict: \", output_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 20/20 [00:26<00:00,  1.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 26.46s, 0 errors\n",
      "ERROR PARSING CODE\n",
      "ERROR PARSING CODE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concurrent Execution of 36 Node functions to generate prompts ...: 100%|██████████| 36/36 [00:02<00:00, 17.71it/s]\n",
      "Processing LLM queries: 100%|██████████| 24/24 [00:22<00:00,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 22.84s, 0 errors\n",
      "Output per code per test:  defaultdict(<function call_func_prompt_parallel.<locals>.<lambda> at 0x2865c8900>, {15: defaultdict(<class 'dict'>, {1: {'authors': ['Yuxiang Hou, Yuanyuan Shi, Zhihao Jiao, Xiao Sun, Jianwei Liang, Depeng Li'], 'institution': 'University of Science and Technology of China', 'citations': ['2'], 'date': '2022-11-07'}}), 5: defaultdict(<class 'dict'>, {1: {'authors': [{'name': \"{author['name']}\", 'affiliation': \"{author['affiliation']}\", 'email': \"{author['email']}\"}], 'institution': {'name': \"{institution['name']}\", 'address': \"{institution['address']}\"}, 'citations': [{'title': \"{citation['title']}\", 'year': \"{citation['year']}\", 'journal': \"{citation['journal']}\", 'volume': \"{citation['volume']}\", 'pages': \"{citation['pages']}\"}], 'date': '{date}'}, 0: {'authors': [{'name': \"{author['name']}\", 'affiliation': \"{author['affiliation']}\"}], 'institution': {'name': \"{institution['name']}\", 'location': \"{institution['location']}\"}, 'citations': [{'id': \"{citation['id']}\", 'title': \"{citation['title']}\", 'year': \"{citation['year']}\"}], 'date': {'year': \"{date['year']}\", 'month': \"{date['month']}\", 'day': \"{date['day']}\"}}})})\n",
      "Errors per code per test:  defaultdict(<function process_all_inputs.<locals>.<lambda> at 0x2bf4befc0>, {9: defaultdict(<class 'list'>, {1: ['f-string expression part cannot include a backslash (<string>, line 33)'], 0: ['f-string expression part cannot include a backslash (<string>, line 33)']}), 10: defaultdict(<class 'list'>, {1: ['f-string expression part cannot include a backslash (<string>, line 20)'], 0: ['f-string expression part cannot include a backslash (<string>, line 20)']}), 16: defaultdict(<class 'list'>, {1: ['f-string expression part cannot include a backslash (<string>, line 19)'], 0: ['f-string expression part cannot include a backslash (<string>, line 19)']}), 14: defaultdict(<class 'list'>, {0: ['f-string expression part cannot include a backslash (<string>, line 26)'], 1: ['f-string expression part cannot include a backslash (<string>, line 26)']}), 15: defaultdict(<class 'list'>, {0: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)'], 1: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)']}), 3: defaultdict(<class 'list'>, {0: [\"'NoneType' object has no attribute 'text'\"], 1: [\"'NoneType' object has no attribute 'text'\"]}), 12: defaultdict(<class 'list'>, {0: [\"'NoneType' object has no attribute 'text'\"], 1: [\"'NoneType' object has no attribute 'text'\"]}), 4: defaultdict(<class 'list'>, {1: [\"'NoneType' object has no attribute 'text'\"], 0: [\"'NoneType' object has no attribute 'text'\"]}), 11: defaultdict(<class 'list'>, {1: ['Expecting value: line 1 column 1 (char 0)'], 0: ['Expecting value: line 1 column 1 (char 0)']}), 6: defaultdict(<class 'list'>, {1: ['Expecting value: line 1 column 1 (char 0)'], 0: ['Expecting value: line 1 column 1 (char 0)']}), 13: defaultdict(<class 'list'>, {0: [\"'NoneType' object has no attribute 'text'\"], 1: [\"'NoneType' object has no attribute 'text'\"]}), 0: defaultdict(<class 'list'>, {0: ['Expecting value: line 1 column 1 (char 0)'], 1: ['Expecting value: line 1 column 1 (char 0)']}), 2: defaultdict(<class 'list'>, {0: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\nunterminated string literal (detected at line 47) (<unknown>, line 47)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)'], 1: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)']}), 17: defaultdict(<class 'list'>, {1: [\"'NoneType' object has no attribute 'get'\"], 0: [\"'NoneType' object has no attribute 'get'\"]}), 7: defaultdict(<class 'list'>, {1: ['list index out of range'], 0: ['list index out of range']}), 5: defaultdict(<class 'list'>, {1: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)'], 0: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)']}), 8: defaultdict(<class 'list'>, {0: ['list index out of range'], 1: ['list index out of range']}), 1: defaultdict(<class 'list'>, {0: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', \"Failed to parse LLM response -- JsonDecodeError : \\nExpecting property name enclosed in double quotes: line 2 column 3 (char 4)AstLiteralError : \\n'{' was never closed (<unknown>, line 1)\"], 1: ['Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)', 'Failed to parse LLM response -- JsonDecodeError : \\nExpecting value: line 1 column 1 (char 0)AstLiteralError : \\ninvalid syntax (<unknown>, line 1)']})})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reasonings, codes = node._evolve(\"i1\", batch_size=20)\n",
    "\n",
    "max_tries = 3\n",
    "num_runs = 2\n",
    "timeout = True\n",
    "self = node \n",
    "# codes = node.codes \n",
    "\n",
    "# fitness_per_code, errors_per_code, global_summary = self._evaluate_fitness(codes=codes, max_tries=max_tries, num_runs=num_runs, custom_metric_map=self.custom_metric_map, timeout=timeout)\n",
    "\n",
    "\n",
    "test_cases = node.test_cases\n",
    "test_inputs = [case[0] for case in test_cases]\n",
    "\n",
    "output_per_code_per_test, errors_per_code_per_test = node.call_prompt_function_parallel(test_inputs, codes, max_tries)\n",
    "\n",
    "print(\"Output per code per test: \", output_per_code_per_test)\n",
    "print(\"Errors per code per test: \", errors_per_code_per_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concurrent Execution of 32 Node functions to generate prompts ...:  81%|████████▏ | 26/32 [00:02<00:00, 12.56it/s]\n",
      "100%|██████████| 950k/950k [00:00<00:00, 12.9MiB/s]\n",
      "Concurrent Execution of 32 Node functions to generate prompts ...:  97%|█████████▋| 31/32 [00:02<00:00, 15.77it/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "100%|██████████| 42.3M/42.3M [00:00<00:00, 43.9MiB/s]\n",
      "Concurrent Execution of 32 Node functions to generate prompts ...: 100%|██████████| 32/32 [00:03<00:00, 10.25it/s]\n",
      "Processing LLM queries: 100%|██████████| 24/24 [00:24<00:00,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 24.70s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from methods.meta_execute import process_all_inputs\n",
    "from methods.evolnode import extract_json_from_text\n",
    "\n",
    "get_response = node.get_response\n",
    "input_dicts = test_inputs\n",
    "\n",
    "outputs_per_code_per_test = defaultdict(lambda: defaultdict(list))\n",
    "errors_per_code_per_test = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "prompts = []\n",
    "input_indices = []\n",
    "\n",
    "# Multi-thread execution (I love GPU parallelism better now ... this is just frustratingly slow)    \n",
    "input_indices, full_prompts, errors_per_code_per_test = process_all_inputs(codes, input_dicts, max_tries)\n",
    "        \n",
    "desc_str = f\"Executing prompt node with LLM in parallel with batch size {len(prompts)}\"\n",
    "responses = get_response(full_prompts, desc=desc_str)\n",
    "\n",
    "for (response, (input_index, code_index)) in zip(responses, input_indices):\n",
    "    try:\n",
    "        output_dict = extract_json_from_text(response)\n",
    "        outputs_per_code_per_test[code_index][input_index].append(output_dict)\n",
    "    \n",
    "    except Exception as e:\n",
    "        errors_per_code_per_test[code_index][input_index].append(f\"Failed to parse LLM response -- \" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To achieve the task, you\\'ll need to install the `requests` library to make HTTP requests to the arXiv API and the `json` library to manipulate JSON data. I\\'ll provide the code to perform the task.\\n\\n```python\\nimport requests\\nimport json\\n\\ndef get_paper_metadata(paper_link):\\n    \"\"\"\\n    Get paper metadata from arXiv API.\\n    \\n    Args:\\n    - paper_link (str): arXiv paper link.\\n    \\n    Returns:\\n    - metadata (dict): Paper metadata.\\n    \"\"\"\\n    api_url = f\"https://arxiv.org/api/query?id_list={paper_link}\"\\n    headers = {\\'accept\\': \\'application/xml\\'}\\n    response = requests.get(api_url, headers=headers)\\n\\n    if response.status_code == 200:\\n        metadata = {}\\n        for entry in response.from_xml().findall(\\'.//entry\\'):\\n            metadata[\\'title\\'] = entry.find(\\'title\\').text\\n            metadata[\\'authors\\'] = [author.text for author in entry.findall(\\'author\\')]\\n            metadata[\\'institution\\'] = entry.find(\\'author\\').attrib.get(\\'affiliation\\')\\n            metadata[\\'citations\\'] = entry.find(\\'citation_count\\').text\\n            metadata[\\'date\\'] = entry.find(\\'published\\').text\\n        return metadata\\n    else:\\n        return None\\n\\ndef extract_paper_info(metadata):\\n    \"\"\"\\n    Extract relevant information from paper metadata.\\n    \\n    Args:\\n    - metadata (dict): Paper metadata.\\n    \\n    Returns:\\n    - extracted_info (dict): Extracted paper information.\\n    \"\"\"\\n    extracted_info = {}\\n    extracted_info[\\'authors\\'] = metadata[\\'authors\\']\\n    extracted_info[\\'institution\\'] = metadata[\\'institution\\']\\n    extracted_info[\\'citations\\'] = [metadata[\\'citations\\']]\\n    extracted_info[\\'date\\'] = metadata[\\'date\\']\\n    return extracted_info\\n\\ndef main():\\n    paper_link = input(\"Enter the arXiv paper link: \")\\n    metadata = get_paper_metadata(paper_link)\\n    if metadata is not None:\\n        extracted_info = extract_paper_info(metadata)\\n        print(json.dumps(extracted_info, indent=4))\\n    else:\\n        print(\"Failed to retrieve metadata.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nTo execute the script in parallel with batch size 0 (which means we\\'ll process one paper at a time), you can use the following command:\\n\\n```bash\\npython script.py & disown\\n```\\n\\nHowever, this will not truly run the script in parallel with batch size 0, as it will still process the papers one at a time. If you want to run the script in parallel, you\\'ll need to use a more advanced tool or library that supports parallel processing, such as `multiprocessing` or `concurrent.futures`.\\n\\nHere is an example of how to modify the script to use `concurrent.futures` to run the tasks in parallel:\\n\\n```python\\nimport requests\\nimport json\\nfrom concurrent.futures import ThreadPoolExecutor\\n\\ndef get_paper_metadata(paper_link):\\n    # ...\\n\\ndef extract_paper_info(metadata):\\n    # ...\\n\\ndef main():\\n    paper_link = input(\"Enter the arXiv paper link: \")\\n    with ThreadPoolExecutor() as executor:\\n        future = executor.submit(get_paper_metadata, paper_link)\\n        metadata = future.result()\\n        if metadata is not None:\\n            extracted_info = extract_paper_info(metadata)\\n            print(json.dumps(extracted_info, indent=4))\\n        else:\\n            print(\"Failed to retrieve metadata.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nIn this example, we use a `ThreadPoolExecutor` to run the `get_paper_metadata` function in a separate thread. The `submit` method returns a `Future` object that represents the result of the task. We then call the `result` method on the `Future` object to get the result of the task.',\n",
       " 'To accomplish this task, we\\'ll need to follow these steps:\\n\\n1.  Install the necessary libraries: `requests` and `json`.\\n2.  Use the `requests` library to send an HTTP GET request to the arXiv API with the given link.\\n3.  Parse the JSON response to extract the paper\\'s metadata.\\n4.  Extract the authors, institution, citations, and date from the metadata.\\n5.  Format the extracted information into a JSON string with the specified structure.\\n\\nHere\\'s how you can do it:\\n\\n```python\\nimport requests\\nimport json\\n\\ndef extract_paper_info(paper_link):\\n    \"\"\"\\n    This function takes a paper link as input, extracts the paper\\'s metadata from the arXiv API,\\n    and returns the extracted information in JSON format.\\n    \"\"\"\\n\\n    # Construct the arXiv API URL\\n    arxiv_api_url = \"https://api.arxiv.org\"\\n\\n    # Send an HTTP GET request to the arXiv API\\n    response = requests.get(f\"{arxiv_api_url}/search/query?query={paper_link}&search_type=title&format=json\")\\n\\n    # Check if the request was successful\\n    if response.status_code != 200:\\n        return f\"Failed to retrieve paper metadata: {response.status_code}\"\\n\\n    # Parse the JSON response\\n    data = response.json()\\n\\n    # Extract the paper\\'s metadata\\n    paper_id = data[\"response\"][\"id_list\"][0]\\n    metadata_url = f\"{arxiv_api_url}/pdf/{paper_id}\"\\n\\n    # Send an HTTP GET request to the paper\\'s metadata URL\\n    metadata_response = requests.get(metadata_url)\\n\\n    # Check if the request was successful\\n    if metadata_response.status_code != 200:\\n        return f\"Failed to retrieve paper metadata: {metadata_response.status_code}\"\\n\\n    # Parse the JSON response\\n    metadata = metadata_response.json()\\n\\n    # Extract the authors, institution, citations, and date from the metadata\\n    authors = [author[\"name\"] for author in metadata[\"authors\"]]\\n    institution = metadata[\"aff\"]\\n    citations = [citation[\"id\"] for citation in metadata[\"references\"]]\\n    date = metadata[\"created\"]\\n\\n    # Format the extracted information into a JSON string\\n    output = {\\n        \"authors\": authors,\\n        \"institution\": institution,\\n        \"citations\": citations,\\n        \"date\": date\\n    }\\n\\n    # Return the output as a JSON string\\n    return json.dumps(output, indent=4)\\n\\n# Example usage:\\npaper_link = \"https://arxiv.org/abs/2106.09677\"\\nprint(\"{{\\'authors\\':\", extract_paper_info(paper_link), \"}}\")\\n```\\n\\nThis script will output the extracted information in JSON format with the specified structure. The `extract_paper_info` function takes a paper link as input, sends an HTTP GET request to the arXiv API, parses the JSON response, extracts the authors, institution, citations, and date from the metadata, and returns the extracted information as a JSON string.\\n\\nNote that the `requests` library is used to send HTTP requests to the arXiv API, and the `json` library is used to parse the JSON responses. The `indent=4` argument in the `json.dumps` function is used to format the output JSON string with indentation for readability.',\n",
       " '**Please note that arXiv API has usage limits (5 requests per minute and 100 requests per hour) and it\\'s intended for non-commercial use. This code is for educational purposes only.**\\n\\nFirstly, you need to install the required libraries. You can do this by running the following commands in your terminal:\\n\\n```bash\\npip install requests json\\n```\\n\\nHere\\'s a Python script that uses the arXiv API to download the paper\\'s metadata and extract the required information:\\n\\n```python\\nimport requests\\nimport json\\nfrom urllib.parse import urlparse\\n\\ndef get_paper_metadata(paper_id):\\n    # Construct the API URL\\n    url = f\"http://api.arxiv.org:8080/search/query?query=all:{paper_id}&max_results=1\"\\n    \\n    try:\\n        # Send a GET request to the API\\n        response = requests.get(url)\\n        \\n        # Check if the request was successful\\n        if response.status_code == 200:\\n            # Parse the JSON response\\n            data = json.loads(response.text)\\n            \\n            # Check if the paper was found\\n            if len(data[\\'result\\']) > 0:\\n                # Extract the metadata\\n                metadata = data[\\'result\\'][0]\\n                \\n                # Extract the authors\\n                authors = [a[\\'name\\'] for a in metadata[\\'authors\\']]\\n                \\n                # Extract the institution\\n                institution = metadata[\\'affiliation\\']\\n                \\n                # Extract the citations\\n                citations = [int(c[\\'id\\']) for c in metadata[\\'citations\\']]\\n                \\n                # Extract the date\\n                date = metadata[\\'created\\'][\\'date\\']\\n                \\n                # Create the output dictionary\\n                output = {\\n                    \\'authors\\': authors,\\n                    \\'institution\\': institution,\\n                    \\'citations\\': citations,\\n                    \\'date\\': date\\n                }\\n                \\n                return output\\n            else:\\n                return None\\n        else:\\n            return None\\n    except Exception as e:\\n        print(f\"An error occurred: {e}\")\\n        return None\\n\\ndef main():\\n    # Replace this with the paper ID you want to search for\\n    paper_id = urlparse(\"{{link}}\").path.split(\"/\")[-1]\\n    \\n    # Get the paper\\'s metadata\\n    metadata = get_paper_metadata(paper_id)\\n    \\n    # Print the extracted information in JSON format\\n    if metadata:\\n        print(json.dumps(metadata, indent=4))\\n    else:\\n        print(\"Paper not found.\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nWhen you run this script, it will search for the paper with the given ID, download its metadata, and extract the authors, institution, citations, and date. The extracted information will be printed in JSON format. \\n\\nPlease make sure to replace `{{link}}` with the actual link to the paper you want to search for.\\n\\n**Example usage:**\\n\\nIf the paper ID is `1708.08538`, the script will output the following:\\n\\n```json\\n{\\n    \"authors\": [\\n        \"John Doe\",\\n        \"Jane Doe\"\\n    ],\\n    \"institution\": \"Example University\",\\n    \"citations\": [\\n        1708.08539,\\n        1708.08540\\n    ],\\n    \"date\": \"2017-08-15\"\\n}\\n```\\n\\nNote that the actual output may vary depending on the paper\\'s metadata.',\n",
       " \"To execute the prompt with Node.js and an LLM in parallel with a batch size of 0, you can use the following code:\\n\\n```javascript\\nconst axios = require('axios');\\nconst { ArxivApi } = require('arxiv-api');\\n\\n// Initialize the arXiv API client with proper rate limiting\\nconst arxiv = new ArxivApi({\\n  api: 'http://arxiv.org',\\n  rateLimit: {\\n    maxRequests: 10, // Maximum number of requests per second\\n    requestTimeWindow: 60, // Request time window in seconds\\n  },\\n});\\n\\n// Query parameters\\nconst query = { input: 'author:(https://arxiv.org/abs/2402.15391)' };\\nconst filterDate = '2022-01-01:2022-12-31'; // Filter papers by date\\nconst maxResults = 100; // Maximum number of results to return\\nconst start = 0; // First result to return\\nconst sortBy = 'newest'; // Sort results by newest first\\n\\n// Execute the search query\\nasync function searchArxivPapers() {\\n  try {\\n    // Execute the search query with date filtering and pagination\\n    const response = await arxiv.search(\\n      query,\\n      filterDate,\\n      maxResults,\\n      start,\\n      sortBy\\n    );\\n\\n    // Extract the paper metadata\\n    const papers = response.papers.map((paper) => {\\n      return {\\n        id: paper.id,\\n        title: paper.title,\\n        authors: paper.authors.map((author) => author.name),\\n        institution: paper.institution || '',\\n        citations: paper.citations.map((citation) => citation.id),\\n        date: paper.published,\\n      };\\n    });\\n\\n    // Return the paper metadata as a JSON string in Markdown format\\n    return papers.map((paper) => {\\n      return {\\n        authors: paper.authors.join(', '),\\n        institution: paper.institution,\\n        citations: paper.citations.join(', '),\\n        date: paper.date,\\n      };\\n    }).map((paper) => JSON.stringify(paper, null, 2)).join('\\\\n\\\\n');\\n  } catch (error) {\\n    console.error(error);\\n  }\\n}\\n\\n// Execute the search query in parallel with batch size 0\\nsearchArxivPapers().then((result) => {\\n  console.log(result);\\n}).catch((error) => {\\n  console.error(error);\\n});\\n```\\n\\nThis code uses the `arxiv-api` client to execute the search query with date filtering and pagination. It then extracts the paper metadata and returns it as a JSON string in Markdown format.\\n\\nTo execute the prompt with LLM in parallel with a batch size of 0, you would need to use a library like `lll` ( Large Language Model Library) which is a JavaScript library for interacting with large language models. However, the basic idea is to use a library that supports parallel execution and batch processing, and then use the `arxiv-api` client to execute the search query.\\n\\nHere is an example using `lll` library:\\n\\n```javascript\\nconst lll = require('lll');\\n\\n// Initialize the LLM\\nconst llm = new lll({\\n  model: 'large',\\n  maxTokens: 2048,\\n  parallelism: 0, // Execute in parallel with batch size 0\\n});\\n\\n// Query parameters\\nconst query = { input: 'author:(https://arxiv.org/abs/2402.15391)' };\\nconst filterDate = '2022-01-01:2022-12-31'; // Filter papers by date\\nconst maxResults = 100; // Maximum number of results to return\\nconst start = 0; // First result to return\\nconst sortBy = 'newest'; // Sort results by newest first\\n\\n// Execute the search query\\nasync function searchArxivPapers() {\\n  try {\\n    // Execute the search query with date filtering and pagination\\n    const response = await arxiv.search(\\n      query,\\n      filterDate,\\n      maxResults,\\n      start,\\n      sortBy\\n    );\\n\\n    // Extract the paper metadata\\n    const papers = response.papers.map((paper) => {\\n      return {\\n        id: paper.id,\\n        title: paper.title,\\n        authors: paper.authors.map((author) => author.name),\\n        institution: paper.institution || '',\\n        citations: paper.citations.map((citation) => citation.id),\\n        date: paper.published,\\n      };\\n    });\\n\\n    // Return the paper metadata as a JSON string in Markdown format\\n    return papers.map((paper) => {\\n      return {\\n        authors: paper.authors.join(', '),\\n        institution: paper.institution,\\n        citations: paper.citations.join(', '),\\n        date: paper.date,\\n      };\\n    }).map((paper) => JSON.stringify(paper, null, 2)).join('\\\\n\\\\n');\\n  } catch (error) {\\n    console.error(error);\\n  }\\n}\\n\\n// Execute the search query in parallel with batch size 0\\nsearchArxivPapers().then((result) => {\\n  console.log(result);\\n}).catch((error) => {\\n  console.error(error);\\n});\\n```\\n\\nPlease note that the `lll` library is not officially supported and its usage may vary.\",\n",
       " 'Below is a Python code snippet using the `arxiv` API client to execute the prompt node with LLM in parallel with a batch size of 0, filtering papers by date, paginating the output, and returning both papers and full metadata:\\n\\n```python\\nimport arxiv\\nimport requests\\nimport json\\nimport time\\nfrom concurrent.futures import ThreadPoolExecutor\\n\\ndef get_papers(input_query, start_date, end_date, page_size, page_number):\\n    \"\"\"\\n    Retrieve papers from arXiv API client with date filtering, pagination, and rate limiting.\\n\\n    Args:\\n        input_query (str): The search query.\\n        start_date (str): The start date for filtering papers.\\n        end_date (str): The end date for filtering papers.\\n        page_size (int): The number of papers to retrieve per page.\\n        page_number (int): The page number to retrieve.\\n\\n    Returns:\\n        dict: A dictionary containing the papers and metadata.\\n    \"\"\"\\n    arxiv.download(query=input_query, start=start_date, end=end_date, max_results=page_size, sort_by=\\'newest\\')\\n    papers = arxiv.papers()\\n    metadata = {\\'authors\\': [], \\'institution\\': \\'\\', \\'citations\\': [], \\'date\\': \\'\\'}\\n    for paper in papers:\\n        metadata[\\'authors\\'].append(paper.authors)\\n        metadata[\\'institution\\'].append(paper.institution)\\n        metadata[\\'citations\\'].append(paper.citations)\\n        metadata[\\'date\\'] = paper.published\\n    return {\\'papers\\': papers, \\'metadata\\': metadata}\\n\\ndef execute_query_in_parallel(input_query, start_date, end_date, batch_size=0):\\n    \"\"\"\\n    Execute the prompt node with LLM in parallel with a batch size of 0.\\n\\n    Args:\\n        input_query (str): The search query.\\n        start_date (str): The start date for filtering papers.\\n        end_date (str): The end date for filtering papers.\\n        batch_size (int): The batch size for parallel execution (default is 0).\\n\\n    Returns:\\n        str: A JSON string containing the papers and metadata.\\n    \"\"\"\\n    # Paginate the output with a batch size of 0 (retrieve all papers)\\n    page_size = 1000\\n    page_number = 1\\n    papers = []\\n    metadata = {\\'authors\\': [], \\'institution\\': [], \\'citations\\': [], \\'date\\': \\'\\'}\\n    while True:\\n        try:\\n            result = get_papers(input_query, start_date, end_date, page_size, page_number)\\n            papers.extend(result[\\'papers\\'])\\n            metadata[\\'authors\\'].extend(result[\\'metadata\\'][\\'authors\\'])\\n            metadata[\\'institution\\'].extend(result[\\'metadata\\'][\\'institution\\'])\\n            metadata[\\'citations\\'].extend(result[\\'metadata\\'][\\'citations\\'])\\n            metadata[\\'date\\'] = result[\\'metadata\\'][\\'date\\']\\n            page_number += 1\\n            # Introduce a delay to avoid exceeding the API rate limit\\n            time.sleep(1)\\n        except Exception as e:\\n            print(f\"Error retrieving papers: {e}\")\\n            break\\n    # Convert metadata to a JSON string\\n    metadata_json = json.dumps(metadata, indent=4)\\n    return json.dumps({\\'papers\\': papers, \\'metadata\\': metadata_json})\\n\\n# Define the search query, start date, end date, and batch size\\ninput_query = \\'author:(https://arxiv.org/abs/2402.15391)\\'\\nstart_date = \\'2022-01-01\\'\\nend_date = \\'2024-07-26\\'\\nbatch_size = 0\\n\\n# Execute the query in parallel\\nresult = execute_query_in_parallel(input_query, start_date, end_date, batch_size)\\nprint(result)\\n```\\n\\nThis code defines two functions: `get_papers` retrieves papers from the arXiv API client with date filtering, pagination, and rate limiting, and `execute_query_in_parallel` executes the prompt node with LLM in parallel with a batch size of 0.\\n\\nThe `execute_query_in_parallel` function paginates the output with a batch size of 0 (retrieve all papers) and introduces a delay to avoid exceeding the API rate limit.\\n\\nFinally, the code defines the search query, start date, end date, and batch size, executes the query in parallel using the `execute_query_in_parallel` function, and prints the result as a JSON string.\\n\\nNote that you need to install the `arxiv` API client using pip: `pip install arxiv`. Additionally, be aware of the arXiv API rate limit and adjust the code accordingly to avoid exceeding it.',\n",
       " 'Here is a Python solution that utilizes the `arxiv` library to execute the prompt node with LLM in parallel with batch size 0, search arXiv papers by query with date filtering, paginate the output, and return both papers and full metadata. This solution also implements proper rate limiting.\\n\\n```python\\nimport arxiv\\nimport json\\nfrom datetime import datetime, timedelta\\nfrom concurrent.futures import ThreadPoolExecutor\\n\\ndef get_arxiv_data(query, start_date, end_date, page_size, page_number):\\n    \"\"\"\\n    Get arXiv papers by query with date filtering.\\n\\n    Args:\\n    query (str): Search query\\n    start_date (str): Start date in YYYY-MM-DD format\\n    end_date (str): End date in YYYY-MM-DD format\\n    page_size (int): Number of papers per page\\n    page_number (int): Page number\\n\\n    Returns:\\n    list: List of arXiv papers\\n    \"\"\"\\n    arxiv.set_verbosity(0)  # Set verbosity to 0 for rate limiting\\n\\n    # Create search query with date filtering\\n    search_query = f\\'author:\"{query}\" and post-time:[{start_date} TO {end_date}]\\'\\n\\n    try:\\n        # Execute search query\\n        papers = arxiv.search(query=search_query, max_results=page_size, start=page_number)\\n\\n        # Extract metadata from papers\\n        metadata = []\\n        for paper in papers:\\n            paper_metadata = {\\n                \\'title\\': paper.title,\\n                \\'authors\\': [author.name for author in paper.authors],\\n                \\'institution\\': paper.institution,\\n                \\'citations\\': paper.citations,\\n                \\'date\\': paper.published\\n            }\\n            metadata.append(paper_metadata)\\n\\n        return metadata\\n\\n    except Exception as e:\\n        print(f\"Error: {e}\")\\n        return []\\n\\ndef main():\\n    # Define query, start date, end date, page size, and page number\\n    query = \\'https://arxiv.org/abs/2402.15391\\'\\n    start_date = \\'2022-01-01\\'\\n    end_date = \\'2022-12-31\\'\\n    page_size = 10\\n    page_number = 1\\n\\n    # Create thread pool with 10 worker threads\\n    with ThreadPoolExecutor(max_workers=10) as executor:\\n        # Submit task to thread pool\\n        future = executor.submit(get_arxiv_data, query, start_date, end_date, page_size, page_number)\\n\\n        # Get result from future\\n        metadata = future.result()\\n\\n        # Convert metadata to JSON string in markdown format\\n        json_string = json.dumps(metadata, indent=4, sort_keys=True)\\n\\n    # Print JSON string\\n    print(json_string)\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nThis code defines a function `get_arxiv_data` that takes a query, start date, end date, page size, and page number as input and returns a list of arXiv papers. The function uses the `arxiv` library to execute the search query and extract metadata from the papers.\\n\\nIn the `main` function, we define the query, start date, end date, page size, and page number, create a thread pool with 10 worker threads, submit the task to the thread pool, and get the result from the future. We then convert the metadata to a JSON string in markdown format and print the JSON string.\\n\\nNote that we use the `arxiv.set_verbosity(0)` function to set the verbosity to 0 for rate limiting. This will limit the API calls to 1000 per day.\\n\\nAlso, note that we use the `ThreadPoolExecutor` class from the `concurrent.futures` module to create a thread pool with 10 worker threads. This allows us to execute the task in parallel and take advantage of multiple CPU cores.\\n\\nYou can adjust the query, start date, end date, page size, and page number to suit your needs.',\n",
       " 'To accomplish this task, we will use the `arxiv` API to fetch the paper metadata, and then parse the metadata to extract the required information. We will use the `requests` library to make API calls and the `json` library to parse the metadata.\\n\\nHere\\'s an example code snippet that accomplishes this task:\\n\\n```python\\nimport requests\\nimport json\\n\\ndef get_paper_metadata(paper_link):\\n    api_url = \"https://api.arxiv.org/search/\"\\n    params = {\\n        \"id_list\": paper_link,\\n        \"max_results\": 1,\\n        \"api_key\": \"YOUR_API_KEY\"  # Replace with your actual API key\\n    }\\n    response = requests.get(api_url, params=params)\\n    if response.status_code == 200:\\n        metadata = response.json()\\n        return metadata\\n    else:\\n        return None\\n\\ndef extract_paper_info(metadata):\\n    paper_info = {}\\n    authors = metadata[\\'list\\'][0][\\'author\\']\\n    paper_info[\\'authors\\'] = [author[\\'name\\'] for author in authors]\\n    \\n    institution = metadata[\\'list\\'][0][\\'affiliation\\']\\n    paper_info[\\'institution\\'] = institution\\n    \\n    citations = metadata[\\'list\\'][0][\\'entry_count\\']\\n    paper_info[\\'citations\\'] = citations\\n    \\n    date = metadata[\\'list\\'][0][\\'published\\']\\n    paper_info[\\'date\\'] = date\\n    \\n    return paper_info\\n\\ndef main(paper_link):\\n    metadata = get_paper_metadata(paper_link)\\n    if metadata:\\n        paper_info = extract_paper_info(metadata)\\n        return json.dumps(paper_info, indent=4)\\n    else:\\n        return \"Failed to fetch metadata\"\\n\\n# Example usage\\npaper_link = \"{{link}}\"\\npaper_info = main(paper_link)\\nprint(\"{{paper_info}}\")\\n```\\n\\nHowever, to execute the prompt node with the LLM in parallel with a batch size of 0, we need to modify the code as follows:\\n\\n```python\\nimport requests\\nimport json\\nimport concurrent.futures\\n\\ndef get_paper_metadata(paper_link):\\n    api_url = \"https://api.arxiv.org/search/\"\\n    params = {\\n        \"id_list\": paper_link,\\n        \"max_results\": 1,\\n        \"api_key\": \"YOUR_API_KEY\"  # Replace with your actual API key\\n    }\\n    response = requests.get(api_url, params=params)\\n    if response.status_code == 200:\\n        metadata = response.json()\\n        return metadata\\n    else:\\n        return None\\n\\ndef extract_paper_info(metadata):\\n    paper_info = {}\\n    authors = metadata[\\'list\\'][0][\\'author\\']\\n    paper_info[\\'authors\\'] = [author[\\'name\\'] for author in authors]\\n    \\n    institution = metadata[\\'list\\'][0][\\'affiliation\\']\\n    paper_info[\\'institution\\'] = institution\\n    \\n    citations = metadata[\\'list\\'][0][\\'entry_count\\']\\n    paper_info[\\'citations\\'] = citations\\n    \\n    date = metadata[\\'list\\'][0][\\'published\\']\\n    paper_info[\\'date\\'] = date\\n    \\n    return paper_info\\n\\ndef main(paper_link):\\n    with concurrent.futures.ThreadPoolExecutor() as executor:\\n        futures = {executor.submit(get_paper_metadata, paper_link): paper_link}\\n        for future in concurrent.futures.as_completed(futures):\\n            paper_link = futures[future]\\n            try:\\n                metadata = future.result()\\n                if metadata:\\n                    paper_info = extract_paper_info(metadata)\\n                    return json.dumps(paper_info, indent=4)\\n                else:\\n                    return f\"Failed to fetch metadata for {paper_link}\"\\n            except Exception as e:\\n                return f\"An error occurred: {str(e)}\"\\n\\n# Example usage\\npaper_link = \"{{link}}\"\\npaper_info = main(paper_link)\\nprint(\"{{paper_info}}\")\\n```\\n\\nIn this modified code, we use the `concurrent.futures` library to execute the `get_paper_metadata` function in parallel. We create a `ThreadPoolExecutor` with a batch size of 0, which means that the threads will be executed one by one. This is because the `arxiv` API does not support parallel requests.',\n",
       " '### Fetching Paper Metadata from arXiv API\\n\\nTo accomplish this task, we\\'ll need to use the `requests` library to make an HTTP request to the arXiv API. We\\'ll also use `json` to parse the JSON response and extract the required information.\\n\\n```python\\nimport requests\\nimport json\\nimport os\\n\\ndef fetch_paper_metadata(paper_id):\\n    \"\"\"\\n    Fetch paper metadata from arXiv API.\\n\\n    Args:\\n        paper_id (str): arXiv paper ID\\n\\n    Returns:\\n        dict: Paper metadata in JSON format\\n    \"\"\"\\n    api_url = f\"https://api.arxiv.org/find?q={paper_id}&max_results=1&format=json\"\\n    response = requests.get(api_url)\\n    if response.status_code == 200:\\n        metadata = response.json()\\n        return metadata[\\'dataset\\'][\\'results\\'][0]\\n    else:\\n        print(f\"Failed to fetch metadata for paper {paper_id}\")\\n        return None\\n\\ndef extract_info(metadata):\\n    \"\"\"\\n    Extract authors, institution, citations, and date from paper metadata.\\n\\n    Args:\\n        metadata (dict): Paper metadata\\n\\n    Returns:\\n        dict: Extracted information in JSON format\\n    \"\"\"\\n    authors = [author[\\'name\\'] for author in metadata.get(\\'authors\\', [])]\\n    institution = metadata.get(\\'affiliation\\', [{}])[0].get(\\'name\\', \\'\\')\\n    citations = metadata.get(\\'references\\', [])\\n    date = metadata.get(\\'created\\', {}).get(\\'date\\', \\'\\')\\n\\n    return {\\n        \\'authors\\': authors,\\n        \\'institution\\': institution,\\n        \\'citations\\': citations,\\n        \\'date\\': date\\n    }\\n\\ndef main():\\n    paper_id = os.getenv(\\'PAPER_ID\\', \\'\\')  # Replace with your paper ID\\n    metadata = fetch_paper_metadata(paper_id)\\n    if metadata:\\n        info = extract_info(metadata)\\n        json_output = json.dumps(info, indent=4)\\n        print(f\"{{\\'authors\\': {info[\\'authors\\']}, \\'institution\\': {info[\\'institution\\']}, \\'citations\\': {info[\\'citations\\']}, \\'date\\': {info[\\'date\\']}}}\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Example Usage\\n\\nTo use this script, replace `PAPER_ID` with the ID of the paper you\\'re interested in. You can find the paper ID in the URL of the arXiv paper page, for example: `https://arxiv.org/abs/2108.06603`. In this case, the paper ID is `2108.06603`.\\n\\nAfter replacing `PAPER_ID`, save this script and run it using Python. The extracted information will be printed in JSON format, along with a markdown header indicating the key-value pairs.\\n\\n### Note\\n\\n* This script assumes that the paper ID is a valid arXiv ID.\\n* The `fetch_paper_metadata` function makes a single HTTP request to the arXiv API. If the request fails or the paper ID is invalid, the function will return `None`.\\n* The `extract_info` function extracts the required information from the paper metadata. If the metadata is missing any of the required fields, the function will return `None`.\\n* The `main` function combines these two functions and prints the extracted information in JSON format.',\n",
       " '### Requirements\\n\\nTo accomplish this task, you\\'ll need to:\\n\\n1. Install the `arxiv` library using pip:\\n   ```bash\\npip install arxiv\\n```\\n\\n2. Make sure you have the `requests` library installed, as it\\'s used internally by the `arxiv` library. You can install it using pip if you haven\\'t already:\\n   ```bash\\npip install requests\\n```\\n\\n### Code\\n\\nNow, let\\'s write the code to accomplish the task:\\n\\n```python\\nimport arxiv\\nimport json\\nfrom datetime import datetime\\n\\ndef get_paper_metadata(paper_id):\\n    \"\"\"\\n    Download the paper\\'s metadata using the arXiv API.\\n\\n    Args:\\n        paper_id (str): The paper\\'s ID, e.g., \\'2103.00001\\'.\\n\\n    Returns:\\n        dict: A dictionary containing the paper\\'s metadata.\\n    \"\"\"\\n    try:\\n        paper = arxiv.query(id=paper_id, output=\\'application/json\\')[0]\\n        return paper\\n    except Exception as e:\\n        print(f\"Error downloading metadata for paper {paper_id}: {str(e)}\")\\n        return None\\n\\ndef extract_paper_info(paper):\\n    \"\"\"\\n    Extract the authors, institution, citations, and date from the paper\\'s metadata.\\n\\n    Args:\\n        paper (dict): A dictionary containing the paper\\'s metadata.\\n\\n    Returns:\\n        dict: A dictionary containing the extracted information.\\n    \"\"\"\\n    authors = \\', \\'.join(paper[\\'author\\'])\\n    institution = paper[\\'affiliation\\']\\n    citations = paper[\\'citation_count\\']\\n    date = datetime.strptime(paper[\\'created\\'], \\'%Y-%m-%dT%H:%M:%SZ\\').strftime(\\'%Y-%m-%d\\')\\n    return {\\'authors\\': [authors], \\'institution\\': institution, \\'citations\\': [str(citations)], \\'date\\': date}\\n\\ndef main(paper_link):\\n    \"\"\"\\n    Search for papers by the given link using the arXiv API, download the paper\\'s metadata,\\n    and extract the authors, institution, citations, and date.\\n\\n    Args:\\n        paper_link (str): The link to the paper on arXiv.\\n\\n    Returns:\\n        str: A JSON string containing the extracted information.\\n    \"\"\"\\n    # Extract the paper ID from the link\\n    paper_id = paper_link.split(\\'/\\')[-1]\\n\\n    # Download the paper\\'s metadata\\n    paper = get_paper_metadata(paper_id)\\n\\n    # Extract the authors, institution, citations, and date\\n    paper_info = extract_paper_info(paper)\\n\\n    # Output the extracted information in JSON format\\n    return json.dumps(paper_info, indent=4)\\n\\nif __name__ == \\'__main__\\':\\n    # Replace \\'paper_link\\' with the actual link to the paper on arXiv\\n    paper_link = \"{{link}}\"\\n    print(f\"{{\\'paper_info\\': {main(paper_link)}}}\")\\n```\\n\\n### Usage\\n\\n1. Replace `\\'{{link}}\\'` with the actual link to the paper on arXiv.\\n2. Run the script using Python: `python script.py`\\n\\nThis will output the extracted information in JSON format, which can be easily parsed by other programs. Note that the `arxiv` library uses the `requests` library internally, so make sure you have both installed.',\n",
       " 'Here\\'s an example code in Python that accomplishes the task using the `arxiv` library, which is a client for the arXiv API. The example also includes rate limiting and error handling to prevent the API from blocking our IP address.\\n\\n```python\\nimport json\\nimport time\\nimport logging\\nfrom arxiv import Arxiv\\n\\n# Initialize the logger\\nlogging.basicConfig(level=logging.INFO)\\nlogger = logging.getLogger(__name__)\\n\\ndef search_arxiv(query, start_date, end_date, max_results=10):\\n    \"\"\"\\n    Searches arXiv papers by query with date filtering.\\n\\n    Args:\\n        query (str): The search query.\\n        start_date (str): The start date in YYYY-MM-DD format.\\n        end_date (str): The end date in YYYY-MM-DD format.\\n        max_results (int, optional): The maximum number of results to return. Defaults to 10.\\n\\n    Yields:\\n        dict: The metadata of a paper.\\n    \"\"\"\\n    arxiv_client = Arxiv()\\n\\n    # Initialize the query with the search query and date filtering\\n    query = f\"author:(https://arxiv.org/abs/2411.18553) AND posted: {start_date} .. {end_date}\"\\n\\n    # Search the arXiv papers\\n    for result in arxiv_client.search(query, max_results=max_results):\\n        yield result\\n\\ndef main():\\n    query = \"{input: \\'author:(https://arxiv.org/abs/2411.18553)\\'}\"\\n    start_date = \"2022-01-01\"\\n    end_date = \"2022-12-31\"\\n    max_results = 20\\n\\n    # Search the arXiv papers\\n    start_time = time.time()\\n    papers = []\\n    for result in search_arxiv(query, start_date, end_date, max_results=max_results):\\n        # Extract the metadata\\n        metadata = {\\n            \\'title\\': result.title,\\n            \\'authors\\': [author.name for author in result.authors],\\n            \\'institution\\': result.institution,\\n            \\'citations\\': [citation.id for citation in result.citations],\\n            \\'date\\': result.posted_date\\n        }\\n        papers.append(metadata)\\n\\n        # Log the result\\n        logger.info(f\"Found paper: {result.title} ({result.id})\")\\n\\n        # Apply rate limiting\\n        time.sleep(1)\\n\\n    # Log the execution time\\n    execution_time = time.time() - start_time\\n    logger.info(f\"Executed search in {execution_time:.2f} seconds\")\\n\\n    # Paginate the output\\n    for i, paper in enumerate(papers):\\n        print(f\"## Paper {i+1}:\")\\n        print(json.dumps(paper, indent=4))\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nThis code first searches the arXiv papers using the provided query and date filtering. It then yields the metadata of each paper. The metadata includes the title, authors, institution, citations, and date of each paper. The code applies rate limiting to prevent the API from blocking our IP address. Finally, it paginates the output and prints it in markdown format.\\n\\nNote that you need to install the `arxiv` library using pip:\\n\\n```bash\\npip install arxiv\\n```\\n\\nAlso, make sure to replace the `query` variable in the `main` function with the desired search query.',\n",
       " 'Here\\'s an example of how you can execute the prompt node with LLM in parallel with batch size 0 and achieve the desired result using the arXiv API client with proper rate limiting.\\n\\nFirstly, you\\'ll need to install the required packages. Run the following commands in your terminal:\\n\\n```bash\\npip install arxiv\\npip install transformers\\npip install torch\\npip install prettytable\\n```\\n\\nNow, let\\'s create a Python script to accomplish the task:\\n\\n```python\\nimport os\\nimport time\\nimport json\\nfrom arxiv import Arxiv\\nfrom transformers import pipeline\\nfrom concurrent.futures import ThreadPoolExecutor\\nfrom prettytable import PrettyTable\\n\\n# Initialize the arXiv API client\\narxiv = Arxiv()\\n\\n# Define the search query\\nquery = \"author:(https://arxiv.org/abs/2411.18553)\"\\n\\n# Initialize the LLM pipeline\\nllm = pipeline(\\'text-generation\\', model=\\'t5-base\\')\\n\\n# Define the function to execute the prompt node in parallel\\ndef execute_prompt_node(query):\\n    # Execute the prompt node\\n    result = llm(query, max_length=2048)\\n    \\n    # Extract the relevant information\\n    authors = []\\n    institution = \\'\\'\\n    citations = []\\n    date = \\'\\'\\n    \\n    for idx, item in enumerate(result):\\n        if \\'authors\\' in item[\\'generated_text\\']:\\n            authors = item[\\'generated_text\\'].split(\\'authors: \\')[1].split(\\', \\')\\n        if \\'institution\\' in item[\\'generated_text\\']:\\n            institution = item[\\'generated_text\\'].split(\\'institution: \\')[1].split(\\', \\')\\n        if \\'citations\\' in item[\\'generated_text\\']:\\n            citations = item[\\'generated_text\\'].split(\\'citations: \\')[1].split(\\', \\')\\n        if \\'date\\' in item[\\'generated_text\\']:\\n            date = item[\\'generated_text\\'].split(\\'date: \\')[1]\\n    \\n    return {\\n        \\'authors\\': authors,\\n        \\'institution\\': institution,\\n        \\'citations\\': citations,\\n        \\'date\\': date\\n    }\\n\\n# Define the function to search arXiv papers by query with date filtering, \\n# paginate the output, and return both papers and full metadata\\ndef search_arxiv_papers(query, start, end, max_results=100):\\n    papers = arxiv.search(query, start=start, end=end, max_results=max_results)\\n    \\n    # Initialize the table\\n    table = PrettyTable()\\n    table.field_names = [\"Title\", \"Authors\", \"Abstract\", \"Date\", \"Citations\"]\\n    \\n    # Extract the relevant information from the papers\\n    for paper in papers:\\n        try:\\n            authors = \\', \\'.join([a[\\'name\\'] for a in paper[\\'authors\\']])\\n            table.add_row([paper[\\'title\\'], authors, paper[\\'summary\\'], paper[\\'published\\'], paper[\\'citations\\']])\\n        except:\\n            continue\\n    \\n    return table.get_string()\\n\\n# Define the function to execute the search arXiv papers function in parallel\\ndef execute_search_arxiv_papers(query, start, end, max_results):\\n    # Execute the search arXiv papers function\\n    results = []\\n    with ThreadPoolExecutor() as executor:\\n        futures = []\\n        for i in range(start, end, max_results):\\n            futures.append(executor.submit(search_arxiv_papers, query, i, i + max_results, max_results))\\n        for future in futures:\\n            results.append(future.result())\\n    \\n    return results\\n\\n# Define the main function\\ndef main():\\n    start_time = time.time()\\n    \\n    # Execute the prompt node in parallel\\n    results = []\\n    with ThreadPoolExecutor() as executor:\\n        futures = []\\n        for _ in range(0, 10, 1): # batch size 0\\n            futures.append(executor.submit(execute_prompt_node, query))\\n        for future in futures:\\n            results.append(future.result())\\n    \\n    # Print the results in markdown\\n    for result in results:\\n        print(json.dumps(result, indent=4))\\n    \\n    # Execute the search arXiv papers function in parallel\\n    results = execute_search_arxiv_papers(query, 0, 100, 100)\\n    \\n    # Print the results in markdown\\n    for result in results:\\n        print(result)\\n    \\n    print(f\"Time taken: {time.time() - start_time} seconds\")\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nThis script will first execute the prompt node in parallel with batch size 0, and then execute the search arXiv papers function in parallel. The results will be printed in markdown.\\n\\nPlease note that you\\'ll need to replace the `t5-base` model with your preferred LLM model. Also, the `arxiv` package is used to interact with the arXiv API, and the `prettytable` package is used to print the results in a table format.\\n\\nAlso, the script will take a few seconds to run depending on the number of results and the speed of your internet connection. The time taken will be printed at the end of the script.\\n\\nPlease let me know if you have any questions or need further clarification.',\n",
       " 'Below is a Python script that uses the `arxiv` API client and `requests` library to search for arXiv papers, filter by date, paginate the results, and return both papers and full metadata.\\n\\n```python\\nimport requests\\nimport json\\nfrom datetime import datetime, timedelta\\n\\n# Set API endpoint\\napi_endpoint = \"https://api.arxiv.org/search?\"\\n\\n# Set API parameters\\nparams = {\\n    \"query\": \"{input: \\'author:(https://arxiv.org/abs/2411.18553)\\'}\",\\n    \"start\": 0,\\n    \"max_results\": 10,\\n    \"sort\": \"relevance\",\\n    \"facets\": \"all\",\\n    \"fl\": \"all\"\\n}\\n\\n# Set rate limiting headers\\nheaders = {\\n    \"User-Agent\": \"arXiv API Client\",\\n    \"Accept\": \"application/json\"\\n}\\n\\ndef get_arxiv_papers(params, headers):\\n    \"\"\"Send GET request to arXiv API and return JSON response\"\"\"\\n    try:\\n        response = requests.get(api_endpoint, params=params, headers=headers)\\n        response.raise_for_status()\\n        return response.json()\\n    except requests.exceptions.RequestException as e:\\n        print(f\"Error: {e}\")\\n        return None\\n\\ndef filter_by_date(papers, start_date, end_date):\\n    \"\"\"Filter papers by date\"\"\"\\n    filtered_papers = []\\n    for paper in papers.get(\"search\", []):\\n        date = paper.get(\"published\", paper.get(\"updated\"))\\n        date_obj = datetime.strptime(date, \"%Y-%m-%dT%H:%M:%SZ\")\\n        if start_date <= date_obj <= end_date:\\n            filtered_papers.append(paper)\\n    return filtered_papers\\n\\ndef paginate_results(papers, start, max_results):\\n    \"\"\"Paginate results\"\"\"\\n    new_start = start + max_results\\n    if new_start > len(papers.get(\"search\", [])):\\n        new_start = len(papers.get(\"search\", []))\\n    return papers, new_start\\n\\ndef get_metadata(paper):\\n    \"\"\"Get paper metadata\"\"\"\\n    metadata = {\\n        \"authors\": [author[\"name\"] for author in paper.get(\"authors\", [])],\\n        \"institution\": paper.get(\"affiliation\", []).split(\",\")[0],\\n        \"citations\": paper.get(\"references\", []),\\n        \"date\": paper.get(\"published\", paper.get(\"updated\"))\\n    }\\n    return metadata\\n\\ndef main():\\n    # Set date range\\n    start_date = datetime(2022, 1, 1)\\n    end_date = datetime(2022, 12, 31)\\n\\n    # Initialize pagination variables\\n    start = 0\\n    max_results = 10\\n    papers = []\\n    metadata = []\\n\\n    while True:\\n        # Send GET request to arXiv API\\n        params.update({\"start\": start, \"max_results\": max_results})\\n        response = get_arxiv_papers(params, headers)\\n        if not response:\\n            break\\n\\n        # Filter papers by date\\n        filtered_papers = filter_by_date(response, start_date, end_date)\\n\\n        # Get paper metadata\\n        for paper in filtered_papers:\\n            metadata.append(get_metadata(paper))\\n\\n        # Paginate results\\n        papers.append(response)\\n        response, start = paginate_results(response, start, max_results)\\n\\n        if not response or len(response.get(\"search\", [])) < max_results:\\n            break\\n\\n    # Print metadata as JSON string in markdown format\\n    print(json.dumps(metadata, indent=2))\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nThis script sends a GET request to the arXiv API, filters the results by date, paginates the results, and returns the paper metadata in JSON format. The output includes the authors, institution, citations, and date of each paper.\\n\\n**Note:** The arXiv API has usage guidelines and rate limits. This script respects the rate limits by sending requests with a reasonable delay between them. However, it\\'s essential to check the API documentation for the most up-to-date information on usage guidelines and rate limits.',\n",
       " 'Based on the provided input data, I will extract the required information. However, I need you to provide the input data.',\n",
       " 'Based on the paper \"Efficient Transformers with Row-Locality and Column-Locality for Large-Scale Neural Sequence Modeling\" by https://arxiv.org/abs/2402.15391, here is the extracted information in a JSON string format with Markdown:\\n\\n```json\\n{\\n\\'authors\\': [\\n  { \\'name\\': \\'Jingfei Chen\\', \\'affiliation\\': \\'Microsoft Research\\' },\\n  { \\'name\\': \\'Tian Lin\\', \\'affiliation\\': \\'University of California, Los Angeles\\' },\\n  { \\'name\\': \\'Tianyi Zhou\\', \\'affiliation\\': \\'Microsoft Research\\' },\\n  { \\'name\\': \\'Xinyu Wang\\', \\'affiliation\\': \\'Microsoft Research\\' },\\n  { \\'name\\': \\'Yi Wu\\', \\'affiliation\\': \\'Microsoft Research\\' },\\n  { \\'name\\': \\'Shuxin Nie\\', \\'affiliation\\': \\'Microsoft Research\\' },\\n  { \\'name\\': \\'Kun Zhang\\', \\'affiliation\\': \\'Microsoft Research\\' },\\n  { \\'name\\': \\'Yuan Yao\\', \\'affiliation\\': \\'University of California, Los Angeles\\' }\\n],\\n\\'institution\\': \\'Microsoft Research; University of California, Los Angeles\\',\\n\\'citations\\': [\\n  { \\'citation\\': \\'This paper is not citable as it is a preprint.\\' }\\n],\\n\\'date\\': { \\'year\\': \\'2024\\', \\'month\\': \\'February\\', \\'day\\': \\'24\\' }\\n}\\n```\\n\\nPlease note that the citations information is not available for preprints as they are not citable until they are published in a journal or conference proceedings.',\n",
       " \"Based on the provided ArXiv paper link (https://arxiv.org/abs/2402.15391), I'll extract the required information and format it as a JSON string in markdown.\\n\\nHere's the information extracted from the paper:\\n\\n- Author: \\n  - Kaito Naoi, \\n  - Takashi Kudo, \\n  - Yoshikiyo Kato\\n- Institution: \\n  - Tokyo Institute of Technology\\n- Citations: Not available (as it's a preprint and not yet published)\\n- Date: 2022-12-19\\n\\nHere's the JSON string in markdown:\\n\\n```json\\n{{'authors': [\\n  {'name': 'Kaito Naoi', 'institution': 'Tokyo Institute of Technology'},\\n  {'name': 'Takashi Kudo', 'institution': 'Tokyo Institute of Technology'},\\n  {'name': 'Yoshikiyo Kato', 'institution': 'Tokyo Institute of Technology'}\\n],\\n'institution': 'Tokyo Institute of Technology',\\n'citations': [],\\n'date': '2022-12-19'}}\\n```\\n\\nPlease note that the citations field is empty as it's not available for preprints. Also, the institution is repeated for each author as it's the same for all of them.\",\n",
       " \"Based on the input data at https://arxiv.org/abs/2411.18553, the extracted information is as follows:\\n\\n```json\\n{\\n  'authors': [\\n    {\\n      'name': 'Vinayak Rao',\\n      'email': 'vinayakrao@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    },\\n    {\\n      'name': 'Sashank Santhanam',\\n      'email': 'ssan@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    },\\n    {\\n      'name': 'Yi Tay',\\n      'email': 'yttay@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    },\\n    {\\n      'name': 'Hannan Abu-Hakima',\\n      'email': 'hannan@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    },\\n    {\\n      'name': 'Jonathan Gross',\\n      'email': 'jgross@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    },\\n    {\\n      'name': 'Chen Liang',\\n      'email': 'cliang@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    },\\n    {\\n      'name': 'Jingfei Du',\\n      'email': 'jdu@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    },\\n    {\\n      'name': 'Denny Zhou',\\n      'email': 'dzhou@csail.mit.edu',\\n      'affiliation': 'MIT-IBM Watson AI Lab'\\n    }\\n  ],\\n  'institution': 'MIT-IBM Watson AI Lab',\\n  'citations': [],\\n  'date': '2022-11-03'\\n}\\n```\\n\\nNote that the `citations` field is empty because the input data does not provide any information about citations. Also, the affiliation of the authors is the same as the institution, so I extracted it as a separate field for clarity.\",\n",
       " 'Based on the provided link to the paper, I will extract the requested information. However, please note that the paper link you provided is not directly accessible, and I\\'ll use the title to fetch the paper details. \\n\\nThe title of the paper is not provided, but I can demonstrate how you can extract the requested information using the title of the paper. For the purpose of this exercise, let\\'s assume the title of the paper is \"Understanding and Mitigating the Impact of Adversarial Examples on Deep Neural Networks\".\\n\\nTo extract the requested information, we can use the arXiv API. However, since the arXiv API does not directly provide the institution and citations, we will need to make additional requests to other APIs to fetch this information.\\n\\nHere is a Python script that demonstrates how to extract the requested information:\\n\\n```python\\nimport requests\\nimport json\\n\\ndef get_paper_info(title):\\n    # Get paper ID from arXiv API\\n    response = requests.get(f\\'https://api.arxiv.org:443/api/query?search=ti%3A%22{title}%22&count=1\\')\\n    if response.status_code == 200:\\n        data = json.loads(response.text)\\n        paper_id = data[\\'response\\'][\\'ids\\'].split()[0]\\n        \\n        # Get paper metadata from arXiv API\\n        response = requests.get(f\\'https://api.arxiv.org:443/api/v1/abstract/{paper_id}\\')\\n        if response.status_code == 200:\\n            data = json.loads(response.text)\\n            authors = [{\\'name\\': author} for author in data[\\'authors\\']]\\n            institution = \\'\\'\\n            citations = []\\n        else:\\n            authors = []\\n            institution = \\'\\'\\n            citations = []\\n    else:\\n        authors = []\\n        institution = \\'\\'\\n        citations = []\\n    \\n    # Get institution from Google Scholar using the paper ID\\n    google_scholar_url = f\\'https://scholar.google.com/scholar?q={paper_id}&btnG=&hl=en&as_sdt=0,5\\'\\n    response = requests.get(google_scholar_url)\\n    if response.status_code == 200:\\n        institution = \\'Unknown Institution\\'\\n        # Implement a more sophisticated way to extract the institution\\n        # For example, you can use BeautifulSoup to parse the HTML content\\n    else:\\n        institution = \\'\\'\\n    \\n    # Get citations from Google Scholar using the paper ID\\n    google_scholar_url = f\\'https://scholar.google.com/scholar?q={paper_id}&btnG=&hl=en&as_sdt=0,5\\'\\n    response = requests.get(google_scholar_url)\\n    if response.status_code == 200:\\n        # Implement a more sophisticated way to extract the citations\\n        # For example, you can use BeautifulSoup to parse the HTML content\\n        citations = [\\'Unknown Citation\\']\\n    else:\\n        citations = []\\n    \\n    # Get date from arXiv API\\n    response = requests.get(f\\'https://api.arxiv.org:443/api/v1/abstract/{paper_id}\\')\\n    if response.status_code == 200:\\n        data = json.loads(response.text)\\n        date = data[\\'created\\']\\n    else:\\n        date = \\'\\'\\n    \\n    return {\\n        \\'authors\\': authors,\\n        \\'institution\\': institution,\\n        \\'citations\\': citations,\\n        \\'date\\': date\\n    }\\n\\ntitle = \"Understanding and Mitigating the Impact of Adversarial Examples on Deep Neural Networks\"\\npaper_info = get_paper_info(title)\\nprint(json.dumps(paper_info, indent=4))\\n```\\n\\nPlease note that this is a simplified example and does not handle all possible edge cases. You may need to modify the script to suit your specific requirements. Additionally, the script assumes that the institution and citations can be extracted from the Google Scholar page. This may not always be the case, and you may need to implement a more sophisticated way to extract this information.',\n",
       " \"Based on the input data from the provided paper (https://arxiv.org/abs/2411.18553), I will extract the requested information in JSON format.\\n\\nThe paper's metadata is as follows:\\n\\n- **Title:** Efficient Text-to-Text Transfer Transforms\\n- **Authors:** Vivek Ramanujan, Mhammad H. Al-Quraishi, Stephen Roller, Emmons J. Schwarz, Jason Weston, and Tom Kwiatkowski\\n- **Institution:** Allen Institute for Artificial Intelligence\\n- **Citations:** Not available (since it's an arXiv paper and not a published journal paper)\\n- **Date:** 2024\\n\\nHere's the extracted information in JSON format:\\n\\n```json\\n{\\n  'authors': [\\n    {\\n      'name': 'Vivek Ramanujan',\\n      'affiliation': 'Allen Institute for Artificial Intelligence'\\n    },\\n    {\\n      'name': 'Mhammad H. Al-Quraishi',\\n      'affiliation': 'Allen Institute for Artificial Intelligence'\\n    },\\n    {\\n      'name': 'Stephen Roller',\\n      'affiliation': 'Allen Institute for Artificial Intelligence'\\n    },\\n    {\\n      'name': 'Emmons J. Schwarz',\\n      'affiliation': 'Allen Institute for Artificial Intelligence'\\n    },\\n    {\\n      'name': 'Jason Weston',\\n      'affiliation': 'Allen Institute for Artificial Intelligence'\\n    },\\n    {\\n      'name': 'Tom Kwiatkowski',\\n      'affiliation': 'Allen Institute for Artificial Intelligence'\\n    }\\n  ],\\n  'institution': 'Allen Institute for Artificial Intelligence',\\n  'citations': [],  # Since citations are not available for arXiv papers\\n  'date': '2024'\\n}\\n```\\n\\nThis JSON string represents the extracted metadata from the provided paper. \\n\\nNote that I couldn't find any information on the number of citations for this paper as it's an arXiv paper and not a published journal paper. If you need the actual citation count, you might need to check other sources like Google Scholar or Semantic Scholar.\",\n",
       " '```python\\nimport json\\nimport requests\\n\\ndef search_papers_paginated(query, max_results, page_size, from_date, to_date):\\n    \"\"\"\\n    Search for papers using the query and return the first page of results.\\n\\n    Args:\\n    query (dict): Query parameters.\\n    max_results (int): Maximum number of results to return.\\n    page_size (int): Size of the page.\\n    from_date (str): Start date of the search range.\\n    to_date (str): End date of the search range.\\n\\n    Returns:\\n    dict: A dictionary containing the papers.\\n    \"\"\"\\n    query[\"max_results\"] = max_results\\n    query[\"page_size\"] = page_size\\n    query[\"from_date\"] = from_date\\n    query[\"to_date\"] = to_date\\n\\n    response = requests.post(\"https://api.semanticscholar.org/graph/v1/paper/cite\", json=query)\\n    return response.json()\\n\\n\\ndef get_author_network(author_name, max_papers=10, depth=1):\\n    \"\"\"\\n    Extract the author\\'s network information.\\n\\n    Args:\\n    author_name (str): The author\\'s name.\\n    max_papers (int): Maximum number of papers to return.\\n    depth (int): Depth of the author network.\\n\\n    Returns:\\n    dict: A dictionary containing the author\\'s network information.\\n    \"\"\"\\n    # This is a mock function, you would need to implement the actual logic to get the author\\'s network information\\n    return {\"authors\": [author_name], \"institution\": \"Unknown\", \"citations\": []}\\n\\n\\ndef download_specific_paper(paper_id):\\n    \"\"\"\\n    Download a specific paper.\\n\\n    Args:\\n    paper_id (str): The paper\\'s ID.\\n\\n    Returns:\\n    dict: A dictionary containing the paper\\'s information.\\n    \"\"\"\\n    response = requests.post(\"https://api.semanticscholar.org/graph/v1/paper\", json={\"paper_id\": paper_id})\\n    return response.json()\\n\\n\\ndef extract_citations(paper):\\n    \"\"\"\\n    Extract the citations from a paper.\\n\\n    Args:\\n    paper (dict): The paper\\'s information.\\n\\n    Returns:\\n    list: A list of citations.\\n    \"\"\"\\n    return paper[\"citationCount\"]\\n\\n\\ndef main():\\n    query = {\"query\": \"https://arxiv.org/abs/2411.18553\"}\\n    max_results = 1\\n    page_size = 1\\n    from_date = None\\n    to_date = None\\n\\n    papers = search_papers_paginated(query, max_results, page_size, from_date, to_date)\\n    paper_id = papers[\"data\"][0][\"id\"]\\n\\n    author_network = get_author_network(\"Author Name\", max_papers=10, depth=1)\\n\\n    paper = download_specific_paper(paper_id)\\n    citations = extract_citations(paper)\\n\\n    output = {\\n        \"authors\": author_network[\"authors\"],\\n        \"institution\": author_network[\"institution\"],\\n        \"citations\": citations,\\n        \"date\": paper[\"datePublished\"]\\n    }\\n\\n    print(json.dumps(output, indent=4))\\n\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\nThis script searches for papers using the `search_papers_paginated` function, extracts the author information using the `get_author_network` function, downloads the paper using the `download_specific_paper` function, and extracts the citations using the `extract_citations` function. The output is a JSON string in markdown format.\\n\\nNote that the `get_author_network` function is a mock function and you would need to implement the actual logic to get the author\\'s network information.',\n",
       " '### Parallel Execution of `search_papers_paginated` and Subsequent Functions\\n\\nBelow is an example of how you can achieve this using Python and the `concurrent.futures` library for parallel execution:\\n\\n```python\\nimport concurrent.futures\\nimport json\\n\\ndef search_papers_paginated(query, max_results, page_size, from_date, to_date):\\n    # This function should return a list of dictionaries containing paper information\\n    pass\\n\\ndef get_author_network(author_name, max_papers=10, depth=1):\\n    # This function should return a dictionary containing author information\\n    pass\\n\\ndef download_specific_paper(paper_link):\\n    # This function should return the paper content as a string\\n    pass\\n\\ndef extract_citations(paper_content):\\n    # This function should return a list of citations\\n    pass\\n\\ndef process_paper(link):\\n    query = {\\'query\\': link, \\'max_results\\': 1, \\'page_size\\': 1, \\'from_date\\': None, \\'to_date\\': None}\\n    papers = search_papers_paginated(**query)\\n    paper = papers[0]\\n    \\n    author_name = paper[\\'author\\']\\n    author_info = get_author_network(author_name, max_papers=10, depth=1)\\n    \\n    paper_content = download_specific_paper(link)\\n    citations = extract_citations(paper_content)\\n    \\n    return {\\n        \\'authors\\': author_info[\\'authors\\'],\\n        \\'institution\\': author_info[\\'institution\\'],\\n        \\'citations\\': citations,\\n        \\'date\\': paper[\\'date\\']\\n    }\\n\\ndef main():\\n    paper_link = \"https://arxiv.org/abs/2411.18553\"\\n    with concurrent.futures.ThreadPoolExecutor() as executor:\\n        future = executor.submit(process_paper, paper_link)\\n        result = future.result()\\n    \\n    output = json.dumps(result, indent=4)  # Convert to JSON string\\n    print(output)\\n\\nif __name__ == \"__main__\":\\n    main()\\n```\\n\\n### Explanation\\n\\n1.  Define the `search_papers_paginated` function to search for papers based on the given link.\\n2.  Define the `get_author_network` function to extract author information from the paper.\\n3.  Define the `download_specific_paper` function to download the paper content.\\n4.  Define the `extract_citations` function to extract citations from the paper content.\\n5.  Define the `process_paper` function to process the paper by calling the above functions in sequence.\\n6.  Use the `concurrent.futures` library to execute the `process_paper` function in parallel using a thread pool.\\n7.  Convert the resulting dictionary to a JSON string using the `json.dumps` function.\\n\\nNote that you\\'ll need to implement the `search_papers_paginated`, `get_author_network`, `download_specific_paper`, and `extract_citations` functions according to your specific requirements and the APIs you\\'re using.',\n",
       " '```python\\nimport json\\n\\n# Define the function to search papers\\ndef search_papers_paginated(query, max_results, page_size, from_date, to_date):\\n    # Simulate a search function (replace with actual implementation)\\n    result = {\\n        \\'hits\\': [\\n            {\\n                \\'id\\': \\'12345\\',\\n                \\'title\\': \\'Example Paper\\',\\n                \\'author\\': \\'John Doe\\',\\n                \\'institution\\': \\'Example University\\',\\n                \\'citations\\': 10,\\n                \\'date\\': \\'2022-11-01\\'\\n            }\\n        ]\\n    }\\n    return result\\n\\n# Define the function to get author network\\ndef get_author_network(author, max_papers, depth):\\n    # Simulate a function to get author network (replace with actual implementation)\\n    return {\\n        \\'authors\\': [\\'John Doe\\'],\\n        \\'institution\\': \\'Example University\\',\\n        \\'citations\\': 10,\\n        \\'date\\': \\'2022-11-01\\'\\n    }\\n\\n# Define the function to download specific paper and extract citations\\ndef download_specific_paper(paper_id):\\n    # Simulate a function to download paper (replace with actual implementation)\\n    return {\\n        \\'citations\\': 10\\n    }\\n\\n# Search for papers with the given query\\nquery = {\\'query\\': \\'https://arxiv.org/abs/2411.18553\\'}\\nmax_results = 1\\npage_size = 1\\nfrom_date = None\\nto_date = None\\npapers = search_papers_paginated(query, max_results, page_size, from_date, to_date)\\n\\n# Extract author information\\nauthor_network = get_author_network(papers[\\'hits\\'][0][\\'author\\'], 10, 1)\\n\\n# Download the paper and extract citations\\npaper_id = papers[\\'hits\\'][0][\\'id\\']\\ncitations = download_specific_paper(paper_id)\\n\\n# Combine the results\\nresult = {\\n    \\'authors\\': author_network[\\'authors\\'],\\n    \\'institution\\': author_network[\\'institution\\'],\\n    \\'citations\\': author_network[\\'citations\\'] + citations[\\'citations\\'],\\n    \\'date\\': author_network[\\'date\\']\\n}\\n\\n# Print the result as JSON string in markdown\\nprint(json.dumps(result, indent=4))\\n```\\n\\nOutput:\\n```json\\n{\\n    \"authors\": [\\n        \"John Doe\"\\n    ],\\n    \"institution\": \"Example University\",\\n    \"citations\": 20,\\n    \"date\": \"2022-11-01\"\\n}\\n```\\n\\nNote: The above code is a simulation and should be replaced with actual implementation of the functions `search_papers_paginated`, `get_author_network`, and `download_specific_paper`.',\n",
       " '```python\\nimport json\\n\\ndef search_papers_paginated(query, max_results, page_size, from_date, to_date):\\n    # This function is not implemented, but for the purpose of this example, \\n    # we will assume it returns a dictionary with a list of papers\\n    query = query.copy()  # Create a copy of the query dictionary\\n    query[\\'max_results\\'] = max_results\\n    query[\\'page_size\\'] = page_size\\n    query[\\'from_date\\'] = from_date\\n    query[\\'to_date\\'] = to_date\\n    # Assuming a successful API call returns a dictionary with a list of papers\\n    papers = {\\'papers\\': [{\\'title\\': \\'Paper Title\\', \\'url\\': \\'https://arxiv.org/abs/2402.15391\\'}]}  # Mock response\\n    return papers\\n\\ndef get_author_network(author, max_papers, depth):\\n    # This function is not implemented, but for the purpose of this example, \\n    # we will assume it returns a dictionary with the author\\'s network\\n    author_network = {\\'network\\': {\\'authors\\': [\\'Author Name\\'], \\'institutions\\': [\\'Institution Name\\']}}\\n    return author_network\\n\\ndef download_specific_paper(url):\\n    # This function is not implemented, but for the purpose of this example, \\n    # we will assume it returns a dictionary with the paper\\'s metadata\\n    paper_metadata = {\\'citations\\': [\\'Citation 1\\', \\'Citation 2\\']}\\n    return paper_metadata\\n\\ndef execute_prompt_node(input_link):\\n    query = {\\'query\\': input_link}\\n    max_results = 1\\n    page_size = 1\\n    from_date = None\\n    to_date = None\\n    papers = search_papers_paginated(query, max_results, page_size, from_date, to_date)\\n    paper_url = papers[\\'papers\\'][0][\\'url\\']\\n    author_network = get_author_network(papers[\\'papers\\'][0][\\'title\\'], 10, 1)\\n    paper_metadata = download_specific_paper(paper_url)\\n    output = {\\n        \\'authors\\': author_network[\\'network\\'][\\'authors\\'],\\n        \\'institution\\': author_network[\\'network\\'][\\'institutions\\'][0],\\n        \\'citations\\': paper_metadata[\\'citations\\'],\\n        \\'date\\': paper_url.split(\\'/\\')[-2]\\n    }\\n    return json.dumps(output, indent=4)\\n\\ninput_link = \"https://arxiv.org/abs/2402.15391\"\\noutput = execute_prompt_node(input_link)\\nprint(output)\\n```\\n\\nThis code searches for papers using the `search_papers_paginated` function, extracts the author information using the `get_author_network` function, downloads the paper using the `download_specific_paper` function, and extracts the citations. The output is a JSON string in markdown format.\\n\\nWhen you run this code, it will print the following output:\\n\\n```json\\n{\\n    \"authors\": [\\n        \"Author Name\"\\n    ],\\n    \"institution\": \"Institution Name\",\\n    \"citations\": [\\n        \"Citation 1\",\\n        \"Citation 2\"\\n    ],\\n    \"date\": \"2402.15391\"\\n}\\n```',\n",
       " '**Search and Extract Paper Information**\\n=====================================\\n\\nBelow is the Python code that searches for papers, extracts the author information, downloads the paper, and extracts the citations using the provided functions.\\n\\n```python\\nimport json\\n\\ndef search_papers_paginated(query, max_results, page_size, from_date, to_date):\\n    # This is a placeholder function, implement the actual search logic here\\n    return {\\n        \\'results\\': [{\\'title\\': \\'Paper Title\\', \\'link\\': \\'https://arxiv.org/abs/2402.15391\\'}],\\n        \\'total_count\\': 1,\\n        \\'page_size\\': page_size\\n    }\\n\\ndef get_author_network(author_name, max_papers=10, depth=1):\\n    # This is a placeholder function, implement the actual author network extraction logic here\\n    return {\\n        \\'author\\': author_name,\\n        \\'network\\': {\\n            \\'papers\\': [{\\'title\\': \\'Paper Title\\', \\'year\\': 2022, \\'link\\': \\'https://arxiv.org/abs/2402.15391\\'}]\\n        }\\n    }\\n\\ndef download_specific_paper(link):\\n    # This is a placeholder function, implement the actual paper download logic here\\n    return {\\n        \\'title\\': \\'Paper Title\\',\\n        \\'abstract\\': \\'Paper Abstract\\',\\n        \\'citations\\': [{\\'title\\': \\'Cited Paper Title\\', \\'year\\': 2022, \\'link\\': \\'https://arxiv.org/abs/2402.15390\\'}]\\n    }\\n\\ndef execute_query():\\n    query = {\\n        \\'query\\': \\'https://arxiv.org/abs/2402.15391\\',\\n        \\'max_results\\': 1,\\n        \\'page_size\\': 1,\\n        \\'from_date\\': None,\\n        \\'to_date\\': None\\n    }\\n\\n    response = search_papers_paginated(**query)\\n    results = response[\\'results\\']\\n    if results:\\n        link = results[0][\\'link\\']\\n        author_network = get_author_network(results[0][\\'title\\'].split(\\' by \\')[0], max_papers=10, depth=1)\\n        paper = download_specific_paper(link)\\n        citations = paper[\\'citations\\']\\n        author = author_network[\\'author\\']\\n        institution = \\'University of Author\\'\\n        date = results[0][\\'link\\'].split(\\'/\\')[-1].split(\\'_\\')[0]\\n\\n        output = json.dumps({\\n            \\'authors\\': [author],\\n            \\'institution\\': institution,\\n            \\'citations\\': [citation[\\'title\\'] for citation in citations],\\n            \\'date\\': date\\n        }, indent=4)\\n\\n        print(\\'**Output in JSON format:**\\')\\n        print(output)\\n    else:\\n        print(\\'**No results found.**\\')\\n\\nexecute_query()\\n```\\n\\n**Output**\\n--------\\n\\n```json\\n{\\n    \"authors\": [\\n        \"Author Name\"\\n    ],\\n    \"institution\": \"University of Author\",\\n    \"citations\": [\\n        \"Cited Paper Title\"\\n    ],\\n    \"date\": \"2402\"\\n}\\n```\\n\\nNote that the above code is a simplified example and you will need to implement the actual logic for the `search_papers_paginated`, `get_author_network`, and `download_specific_paper` functions.',\n",
       " '```markdown\\n### Paper Information\\n#### {\\'authors\\': list(...), \\'institution\\': str(...), \\'citations\\': list(...), \\'date\\': str(...)}\\n```\\n\\nHere\\'s a Python code snippet to solve the problem:\\n\\n```python\\nimport requests\\nimport json\\n\\ndef search_papers_paginated(query, max_results, page_size, from_date, to_date):\\n    # Assuming this function returns a list of paper IDs\\n    # For simplicity, we\\'ll use a mock response\\n    paper_ids = [\\'2402.15391\\']\\n    return paper_ids\\n\\ndef get_author_network(author_name, max_papers=10, depth=1):\\n    # Assuming this function returns a dictionary with author information\\n    # For simplicity, we\\'ll use a mock response\\n    author_info = {\\n        \\'name\\': author_name,\\n        \\'papers\\': [\\n            {\\'title\\': \\'Paper 1\\', \\'institution\\': \\'Institution 1\\'},\\n            {\\'title\\': \\'Paper 2\\', \\'institution\\': \\'Institution 2\\'}\\n        ]\\n    }\\n    return author_info\\n\\ndef download_specific_paper(paper_id):\\n    # Assuming this function downloads the paper and extracts citations\\n    # For simplicity, we\\'ll use a mock response\\n    paper_info = {\\n        \\'title\\': \\'Paper Title\\',\\n        \\'citations\\': [\\'Citation 1\\', \\'Citation 2\\']\\n    }\\n    return paper_info\\n\\ndef main():\\n    query = {\\'query\\': \\'https://arxiv.org/abs/2402.15391\\'}\\n    max_results = 1\\n    page_size = 1\\n    from_date = None\\n    to_date = None\\n\\n    paper_ids = search_papers_paginated(query, max_results, page_size, from_date, to_date)\\n    paper_id = paper_ids[0]\\n\\n    author_name = \\'Author Name\\'\\n    author_info = get_author_network(author_name, max_papers=10, depth=1)\\n\\n    paper_info = download_specific_paper(paper_id)\\n\\n    output = {\\n        \\'authors\\': [author_name],\\n        \\'institution\\': author_info[\\'papers\\'][0][\\'institution\\'],\\n        \\'citations\\': paper_info[\\'citations\\'],\\n        \\'date\\': \\'2024-02-15\\'\\n    }\\n\\n    print(json.dumps(output, indent=4))\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n```\\n\\nThis code snippet assumes the functions `search_papers_paginated`, `get_author_network`, and `download_specific_paper` are implemented elsewhere. It uses a mock response to simulate the behavior of these functions.\\n\\nNote that the output is a JSON string in markdown format, as requested. The output will look like this:\\n\\n```markdown\\n{\\n    \"authors\": [\\n        \"Author Name\"\\n    ],\\n    \"institution\": \"Institution 1\",\\n    \"citations\": [\\n        \"Citation 1\",\\n        \"Citation 2\"\\n    ],\\n    \"date\": \"2024-02-15\"\\n}\\n```']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input_indices\n",
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['import arxiv\\nfrom arxiv import Client, Search, Result\\nimport brotli\\nfrom concurrent.futures import ThreadPoolExecutor\\nfrom datetime import datetime\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nfrom typing import List, Dict, Any\\nimport zlib\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef search_papers_paginated(query: str, max_results: int, page_size: int,\\n    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\\n    [Dict[str, Any]]]:\\n    client = Client(page_size=page_size, delay_seconds=3.0)\\n    date_filter = (\\n        f\" AND submittedDate:[{from_date.strftime(\\'%Y%m%d\\')}0000 TO {to_date.strftime(\\'%Y%m%d\\')}2359]\"\\n        )\\n    full_query = query + date_filter\\n    search = Search(query=full_query, max_results=max_results)\\n    results = list(client.results(search))\\n    total_results = len(results)\\n    metadata_list = []\\n    for paper in results:\\n        metadata = {\\'id\\': paper.entry_id, \\'title\\': paper.title, \\'authors\\':\\n            [author.name for author in paper.authors], \\'summary\\': paper.\\n            summary, \\'published\\': paper.published, \\'updated\\': paper.updated,\\n            \\'categories\\': paper.categories, \\'links\\': [link.href for link in\\n            paper.links]}\\n        metadata_list.append(metadata)\\n    papers = results[:page_size]\\n    return papers, total_results, metadata_list\\n\\n\\ndef generate_prompt(link):\\n    prompt = (\\n        f\"Given the input data, {link}, do extract the authors, institution, citations, and date. Make sure the output is a JSON string in markdown like this {{\\'authors\\': [...], \\'institution\\': ..., \\'citations\\': [...], \\'date\\': ...}} near the top of your response or people will die.\"\\n        )\\n    batch_arxiv_download_query = f\\'query:author=\"{link.split(\\\\\\'/\\\\\\')[-2]}\"\\'\\n    search_papers_paginated_query = f\\'query:author=\"{link.split(\\\\\\'/\\\\\\')[-2]}\"\\'\\n    prompt += f\"\"\"\\nUse the output of the following command as input: batch_arxiv_download(query=\"{batch_arxiv_download_query}\", max_papers=1, output_dir=\"/tmp\", categories=[], date_order=True)\"\"\"\\n    prompt += f\"\"\"\\nUse the output of the following command to get the exact paper: search_papers_paginated(query=\"{search_papers_paginated_query}\", max_results=1, page_size=1, from_date=None, to_date=None)\"\"\"\\n    prompt += \"\"\"\\nOutput should be in JSON format with the following structure: {\\'authors\\': list(...), \\'institution\\': str(...), \\'citations\\': list(...), \\'date\\': str(...)}\"\"\"\\n    return prompt\\n',\n",
       " 'def generate_prompt(link):\\n    \"\"\"\\n    Generate a prompt to guide the AI in extracting author, institution, citations, and date from a paper\\'s link.\\n    \\n    Parameters:\\n    link (str): The link to the paper.\\n\\n    Returns:\\n    str: A string containing the final prompt for the AI.\\n    \"\"\"\\n    prompt = (\\n        \"Given the input data, {{link={}, do}} <task>. Extract the following information from the paper\\'s metadata: \"\\n        )\\n    prompt += (\\n        \\'authors (list of names), institution (string), citations (list of strings), date (string). \\'\\n        )\\n    prompt += (\\n        \"Make sure the output is a JSON string in markdown like this {{\\'authors\\': [...], \\'institution\\': ..., \\'citations\\': [...], \\'date\\': ...}} near the top of your response or people will die.\"\\n        )\\n    prompt = prompt.format(link=link)\\n    return prompt\\n',\n",
       " 'def generate_prompt(link):\\n    prompt = (\\n        f\"Given the input link {{input}}, search for papers by a query that matches the link and extract the author, institution, citations, and date. Make sure the output is a JSON string in markdown like this {{\\'authors\\': list(...), \\'institution\\': str(...), \\'citations\\': list(...), \\'date\\': str(...)}} near the top of your response or people will die.\"\\n        )\\n    prompt += f\"\"\"\\nFirst, search for papers using the query {{\\'query\\': \\'{link}\\'}} and {{\\'max_results\\': 1}}, and {{\\'page_size\\': 1}}, and {{\\'from_date\\': None}}, and {{\\'to_date\\': None}} with the function {{search_papers_paginated}}.\"\"\"\\n    prompt += f\"\"\"\\nThen, extract the author information using the function {{get_author_network}} with the author\\'s name, and maximum papers to 10, and depth to 1.\"\"\"\\n    prompt += f\"\"\"\\nAfter that, download the paper with the function {{download_specific_paper}} and extract the citations.\"\"\"\\n    return prompt\\n',\n",
       " 'def generate_prompt(link):\\n    \"\"\"\\n    Generate a prompt to guide an AI in extracting author, institution, citations, and date from a paper\\'s link.\\n\\n    Args:\\n        link (str): The link to the paper.\\n\\n    Returns:\\n        str: A string containing the final prompt for the AI in markdown format.\\n    \"\"\"\\n    prompt = \\'Given the input data, {},\\'.format(link)\\n    prompt += (\\n        \\' do extract the following information: author, institution, citations, and date.\\'\\n        )\\n    prompt += (\\n        \" Make sure the output is a JSON string in markdown like this {{\\'authors\\': [{}], \\'institution\\': {}, \\'citations\\': [{}], \\'date\\': {}}}.\"\\n        )\\n    return prompt\\n',\n",
       " 'import arxiv\\nfrom arxiv import Client, Search, Result\\nimport brotli\\nfrom collections import defaultdict\\nfrom concurrent.futures import ThreadPoolExecutor\\nfrom datetime import datetime\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Dict, List\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nfrom typing import List, Dict, Any\\nimport zlib\\n\\n\\ndef get_author_network(author_name: str, max_papers: int, depth: int) ->tuple[\\n    Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]]:\\n    client = arxiv.Client(delay_seconds=3.0)\\n    coauthor_network = defaultdict(list)\\n    paper_counts = defaultdict(int)\\n    primary_categories = defaultdict(list)\\n    processed_authors = set()\\n\\n    def process_author(author: str, current_depth: int):\\n        if author in processed_authors or current_depth > depth:\\n            return\\n        processed_authors.add(author)\\n        search = arxiv.Search(query=f\\'au:\"{author}\"\\', max_results=max_papers)\\n        for result in client.results(search):\\n            paper_counts[author] += 1\\n            if result.primary_category not in primary_categories[author]:\\n                primary_categories[author].append(result.primary_category)\\n            for coauthor in result.authors:\\n                coauthor_name = coauthor.name\\n                if coauthor_name != author:\\n                    if coauthor_name not in coauthor_network[author]:\\n                        coauthor_network[author].append(coauthor_name)\\n                    if author not in coauthor_network[coauthor_name]:\\n                        coauthor_network[coauthor_name].append(author)\\n                    if current_depth < depth:\\n                        process_author(coauthor_name, current_depth + 1)\\n    process_author(author_name, 0)\\n    return dict(coauthor_network), dict(paper_counts), dict(primary_categories)\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef search_papers_paginated(query: str, max_results: int, page_size: int,\\n    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\\n    [Dict[str, Any]]]:\\n    client = Client(page_size=page_size, delay_seconds=3.0)\\n    date_filter = (\\n        f\" AND submittedDate:[{from_date.strftime(\\'%Y%m%d\\')}0000 TO {to_date.strftime(\\'%Y%m%d\\')}2359]\"\\n        )\\n    full_query = query + date_filter\\n    search = Search(query=full_query, max_results=max_results)\\n    results = list(client.results(search))\\n    total_results = len(results)\\n    metadata_list = []\\n    for paper in results:\\n        metadata = {\\'id\\': paper.entry_id, \\'title\\': paper.title, \\'authors\\':\\n            [author.name for author in paper.authors], \\'summary\\': paper.\\n            summary, \\'published\\': paper.published, \\'updated\\': paper.updated,\\n            \\'categories\\': paper.categories, \\'links\\': [link.href for link in\\n            paper.links]}\\n        metadata_list.append(metadata)\\n    papers = results[:page_size]\\n    return papers, total_results, metadata_list\\n\\n\\ndef generate_prompt(link):\\n    search_papers_paginated_result = search_papers_paginated(query=\\n        \\'paper link: \\' + link, max_results=100, page_size=10, from_date=\\n        \\'2020-01-01\\', to_date=\\'2022-12-31\\')\\n    relevant_info = []\\n    for paper in search_papers_paginated_result[\\'papers\\']:\\n        batch_arxiv_download_result = batch_arxiv_download(query=\\n            \\'paper id: \\' + paper.id, max_papers=1, output_dir=\\'/tmp\\',\\n            categories=[], date_order=False)\\n        metadata = batch_arxiv_download_result[\\'metadata_list\\'][0]\\n        get_author_network_result = get_author_network(author_name=metadata\\n            [\\'author\\'], max_papers=10, depth=1)\\n        authors = get_author_network_result[\\'coauthor_network\\'][metadata[\\n            \\'author\\']]\\n        institution = metadata[\\'institution\\']\\n        citations = metadata[\\'citations\\']\\n        date = metadata[\\'date\\']\\n        relevant_info.append({\\'authors\\': authors, \\'institution\\':\\n            institution, \\'citations\\': citations, \\'date\\': date})\\n    prompt = (\\'Given the input data, {{link: \\' + link +\\n        \"}}, do extract the author, institution, citations, and date from the paper. Make sure the output is a json string in markdown like this {{\\'authors\\': [\\'{}\\'], \\'institution\\': \\'{}\\', \\'citations\\': [\\'{}\\'], \\'date\\': \\'{}\\'}}. You may use the following functions: search_papers_paginated, batch_arxiv_download, get_author_network. Please output a list of dictionaries, where each dictionary contains the relevant information for a paper.\"\\n        )\\n    return prompt.format(\\', \\'.join(relevant_info[0][\\'authors\\']),\\n        relevant_info[0][\\'institution\\'], \\', \\'.join(relevant_info[0][\\n        \\'citations\\']), relevant_info[0][\\'date\\'])\\n',\n",
       " 'import arxiv\\nimport brotli\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nimport zlib\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef generate_prompt(link):\\n    downloaded_papers = batch_arxiv_download(query=link, max_papers=1,\\n        output_dir=\\'./\\', categories=[], date_order=False)\\n    metadata_list = downloaded_papers[1]\\n    metadata = metadata_list[0]\\n    authors = metadata[\\'author\\']\\n    institution = metadata.get(\\'institution\\', \\'Unknown\\')\\n    citations = [citation for citation in metadata[\\'references\\']]\\n    date = metadata[\\'published\\']\\n    prompt = f\"\"\"Given the input data, {link}, do extract the following information: \\n{{\\'authors\\': [{{author}} for author in {authors}], \\'institution\\': {institution}, \\'citations\\': {citations}, \\'date\\': {date}}}\\nOutput should be in JSON format and markdown response.\"\"\"\\n    return prompt\\n',\n",
       " 'import arxiv\\nimport brotli\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nimport zlib\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef generate_prompt(link):\\n    \"\"\"\\n    Generate a prompt for the AI to extract author, institution, citations, and date from a given paper\\'s link.\\n\\n    Parameters:\\n    link (str): The link to the paper.\\n\\n    Returns:\\n    str: A string containing the final prompt for the AI.\\n    \"\"\"\\n    prompt = (\\n        \\'Given the input data, {{link}}, do download the paper metadata using batch_arxiv_download and extract the necessary information. Make sure the output is a JSON string in markdown like this {{authors: [\"author1\", \"author2\"]}}, {{institution: \"institution\"}}, {{citations: [\"citation1\", \"citation2\"]}}, {{date: \"date\"}} near the top of your response or people will die.\\'\\n        )\\n    prompt += (\\n        \" Use get_author_network to get the author\\'s information and add it to the output.\"\\n        )\\n    prompt += (\\n        \" Make sure the output is in the following format: {{\\'authors\\': [\\'\")\\n    downloaded_papers = batch_arxiv_download(query=\\'\\', max_papers=1,\\n        output_dir=\\'\\', categories=[], date_order=True)\\n    paper_metadata = downloaded_papers[\\'metadata_list\\'][0]\\n    authors = paper_metadata[\\'authors\\']\\n    prompt += \"\\', \\'\".join(authors) + \"\\'], \\'institution\\': \\'\"\\n    institution = paper_metadata[\\'institution\\']\\n    prompt += institution + \"\\', \\'citations\\': [\"\\n    citations = paper_metadata[\\'citations\\']\\n    prompt += \"\\', \\'\".join(citations) + \"\\'], \\'date\\': \\'\"\\n    date = paper_metadata[\\'date\\']\\n    prompt += date + \"\\'}}.\"\\n    return prompt\\n',\n",
       " 'def generate_prompt(link):\\n    query = f\\'author:({link})\\'\\n    prompt = (\\n        f\"Given the query {{input: \\'{query}\\'}}, do search arXiv papers by query with date filtering, paginate the output, and return both papers and full metadata using the arXiv API client with proper rate limiting. Make sure the output is a json string in markdown like this {{\\'authors\\': [\\'author1\\', \\'author2\\'], \\'institution\\': \\'institution\\', \\'citations\\': [\\'citation1\\', \\'citation2\\'], \\'date\\': \\'YYYY-MM-DD\\'}} near the top of your response or people will die.\"\\n        )\\n    return prompt\\n',\n",
       " 'from arxiv import Client, Search, Result\\nfrom datetime import datetime\\nfrom typing import List, Dict, Any\\n\\n\\ndef search_papers_paginated(query: str, max_results: int, page_size: int,\\n    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\\n    [Dict[str, Any]]]:\\n    client = Client(page_size=page_size, delay_seconds=3.0)\\n    date_filter = (\\n        f\" AND submittedDate:[{from_date.strftime(\\'%Y%m%d\\')}0000 TO {to_date.strftime(\\'%Y%m%d\\')}2359]\"\\n        )\\n    full_query = query + date_filter\\n    search = Search(query=full_query, max_results=max_results)\\n    results = list(client.results(search))\\n    total_results = len(results)\\n    metadata_list = []\\n    for paper in results:\\n        metadata = {\\'id\\': paper.entry_id, \\'title\\': paper.title, \\'authors\\':\\n            [author.name for author in paper.authors], \\'summary\\': paper.\\n            summary, \\'published\\': paper.published, \\'updated\\': paper.updated,\\n            \\'categories\\': paper.categories, \\'links\\': [link.href for link in\\n            paper.links]}\\n        metadata_list.append(metadata)\\n    papers = results[:page_size]\\n    return papers, total_results, metadata_list\\n\\n\\ndef generate_prompt(link):\\n    \"\"\"\\n    Generate a prompt to guide an AI in extracting author, institution, citations, and date from a paper\\'s link.\\n\\n    Args:\\n        link (str): The link to the paper.\\n\\n    Returns:\\n        str: A string containing the final prompt for the AI.\\n    \"\"\"\\n    metadata = search_papers_paginated(query=link, max_results=100,\\n        page_size=10, from_date=\\'1900-01-01\\', to_date=\\'2100-01-01\\')\\n    authors = [paper.author for paper in metadata.papers]\\n    institution = [paper.institution for paper in metadata.papers]\\n    citations = [paper.citations for paper in metadata.papers]\\n    date = [paper.date for paper in metadata.papers]\\n    prompt = f\"\"\"Given the input data, {{input: {link}}}, do extract the following information from the paper\\'s metadata:\\n\"\"\"\\n    prompt += f\"\"\"{{\\'authors\\': {authors}, \\'institution\\': {institution}, \\'citations\\': {citations}, \\'date\\': {date}}}\\n\"\"\"\\n    prompt += (\\n        f\\'Make sure the output is a JSON string in markdown format like this {{<key>:<value>}} near the top of your response or people will die.\\'\\n        )\\n    return prompt\\n',\n",
       " 'import arxiv\\nimport brotli\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nimport zlib\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef generate_prompt(link):\\n    paper_id = link.split(\\'/\\')[-1]\\n    downloaded_papers, metadata_list, failed_downloads = batch_arxiv_download(\\n        query=paper_id, max_papers=1, output_dir=\\'./\\', categories=[],\\n        date_order=False)\\n    paper_metadata = metadata_list[0]\\n    prompt = (\\n        f\"Given the metadata {{\\'title\\':\\'{paper_metadata[\\'title\\']}\\', \\'authors\\':\\'{paper_metadata[\\'authors\\']}\\', \\'journal-ref\\':\\'{paper_metadata[\\'journal-ref\\']}\\', \\'doi\\':\\'{paper_metadata[\\'doi\\']}\\'}}, do extract the author(s), institution, citations, and date. Make sure the output is a JSON string in markdown like this {{\\'authors\\': [\\'<author1>\\', \\'<author2>\\'], \\'institution\\': \\'<institution>\\', \\'citations\\': [\\'<citation1>\\', \\'<citation2>\\'], \\'date\\': \\'<date>\\'}} near the top of your response or people will die.\"\\n        )\\n    return prompt\\n',\n",
       " 'import arxiv\\nfrom arxiv import Client, Search, Result\\nimport brotli\\nfrom concurrent.futures import ThreadPoolExecutor\\nfrom datetime import datetime\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nfrom typing import List, Dict, Any\\nimport zlib\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef search_papers_paginated(query: str, max_results: int, page_size: int,\\n    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\\n    [Dict[str, Any]]]:\\n    client = Client(page_size=page_size, delay_seconds=3.0)\\n    date_filter = (\\n        f\" AND submittedDate:[{from_date.strftime(\\'%Y%m%d\\')}0000 TO {to_date.strftime(\\'%Y%m%d\\')}2359]\"\\n        )\\n    full_query = query + date_filter\\n    search = Search(query=full_query, max_results=max_results)\\n    results = list(client.results(search))\\n    total_results = len(results)\\n    metadata_list = []\\n    for paper in results:\\n        metadata = {\\'id\\': paper.entry_id, \\'title\\': paper.title, \\'authors\\':\\n            [author.name for author in paper.authors], \\'summary\\': paper.\\n            summary, \\'published\\': paper.published, \\'updated\\': paper.updated,\\n            \\'categories\\': paper.categories, \\'links\\': [link.href for link in\\n            paper.links]}\\n        metadata_list.append(metadata)\\n    papers = results[:page_size]\\n    return papers, total_results, metadata_list\\n\\n\\ndef generate_prompt(link):\\n    downloaded_papers, metadata_list, failed_downloads = batch_arxiv_download(\\n        query=link, max_papers=1, output_dir=\\'./\\', categories=[],\\n        date_order=False)\\n    authors = metadata_list[0].get(\\'author\\')\\n    institution = metadata_list[0].get(\\'institution\\')\\n    citations = metadata_list[0].get(\\'citations\\')\\n    papers, total_results, metadata_list = search_papers_paginated(query=\\n        link, max_results=100, page_size=10, from_date=None, to_date=None)\\n    date = metadata_list[0].get(\\'date\\')\\n    prompt = f\"\"\"Given the input data, {{link: {link}}}, extract the following information from the paper:\\n{{\\'authors\\': [{\\', \\'.join(authors)}], \\'institution\\': {institution}, \\'citations\\': [{\\', \\'.join(citations)}], \\'date\\': {date}}}\\nMake sure the output is a json string in markdown like this {{<key>:<value>}} near the top of your response or people will die.\"\"\"\\n    return prompt\\n',\n",
       " 'import arxiv\\nimport brotli\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nimport zlib\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef generate_prompt(link):\\n    query = f\"author:{link.split(\\'/\\')[-1]}\"\\n    max_papers = 1\\n    output_dir = \\'/tmp\\'\\n    categories = []\\n    date_order = False\\n    downloaded_papers, metadata_list, failed_downloads = batch_arxiv_download(\\n        query, max_papers, output_dir, categories, date_order)\\n    paper_metadata = metadata_list[0]\\n    prompt = (\\n        f\"Given the paper\\'s metadata {{\\'title\\': \\'{paper_metadata[\\'title\\']}\\', \\'authors\\': \\'{paper_metadata[\\'authors\\']}\\', \\'institution\\': \\'{paper_metadata[\\'institution\\']}\\', \\'citations\\': \\'{paper_metadata[\\'citations\\']}\\', \\'date\\': \\'{paper_metadata[\\'date\\']}\\'}}, do extract the author, institution, citations, and date. Make sure the output is a JSON string in markdown like this {{\\'authors\\': [...], \\'institution\\': ..., \\'citations\\': [...], \\'date\\': ...}} near the top of your response or people will die.\"\\n        )\\n    return prompt\\n',\n",
       " 'from arxiv import Client, Search, Result\\nfrom datetime import datetime\\nfrom typing import List, Dict, Any\\n\\n\\ndef search_papers_paginated(query: str, max_results: int, page_size: int,\\n    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\\n    [Dict[str, Any]]]:\\n    client = Client(page_size=page_size, delay_seconds=3.0)\\n    date_filter = (\\n        f\" AND submittedDate:[{from_date.strftime(\\'%Y%m%d\\')}0000 TO {to_date.strftime(\\'%Y%m%d\\')}2359]\"\\n        )\\n    full_query = query + date_filter\\n    search = Search(query=full_query, max_results=max_results)\\n    results = list(client.results(search))\\n    total_results = len(results)\\n    metadata_list = []\\n    for paper in results:\\n        metadata = {\\'id\\': paper.entry_id, \\'title\\': paper.title, \\'authors\\':\\n            [author.name for author in paper.authors], \\'summary\\': paper.\\n            summary, \\'published\\': paper.published, \\'updated\\': paper.updated,\\n            \\'categories\\': paper.categories, \\'links\\': [link.href for link in\\n            paper.links]}\\n        metadata_list.append(metadata)\\n    papers = results[:page_size]\\n    return papers, total_results, metadata_list\\n\\n\\ndef generate_prompt(link):\\n    papers = search_papers_paginated(query=link, max_results=1, page_size=1,\\n        from_date=None, to_date=None)\\n    metadata = papers[\\'metadata_list\\'][0]\\n    authors = [metadata[\\'authors\\']]\\n    institution = metadata.get(\\'institution\\', \\'N/A\\')\\n    citations = metadata.get(\\'citations\\', 0)\\n    date = metadata[\\'date\\']\\n    prompt = (\\n        f\"Given the paper\\'s metadata {{metadata: {metadata}}}, extract the following information: {{\\'authors\\': list(...), \\'institution\\': str(...), \\'citations\\': list(...), \\'date\\': str(...)}} in markdown format. Please follow the JSON structure and output types for correct response.\"\\n        )\\n    return prompt\\n',\n",
       " 'import arxiv\\nimport brotli\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nimport zlib\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef generate_prompt(link):\\n    downloaded_papers, metadata_list, failed_downloads = batch_arxiv_download(\\n        query=link, max_papers=1, output_dir=\\'.\\', categories=[], date_order\\n        =True)\\n    metadata = metadata_list[0]\\n    prompt = (\\n        f\"Given the paper metadata, {{\\'authors\\': list(...), \\'institution\\': str(...), \\'citations\\': list(...), \\'date\\': str(...)}} in markdown format, extract the author, institution, citations, and date from the metadata. Make sure to follow this JSON structure and output the required information in markdown format.\"\\n        )\\n    return prompt\\n',\n",
       " 'def generate_prompt(link):\\n    prompt = (\\n        \"Given the paper link {{link}}, search for papers by the given link using the arXiv API and download the paper\\'s metadata. Then, extract the authors, institution, citations, and date from the metadata. Output the extracted information in JSON format with the following structure: `{\\'authors\\': [list of authors], \\'institution\\': str(institution), \\'citations\\': [list of citations], \\'date\\': str(date)}`. Make sure the output is in markdown format like this {{<key>:<value>}} near the top of your response or people will die.\"\\n        )\\n    return prompt\\n',\n",
       " 'import arxiv\\nimport brotli\\nfrom collections import defaultdict\\nfrom concurrent.futures import ThreadPoolExecutor\\nimport gzip\\nimport os\\nimport requests\\nimport time\\nfrom tqdm import tqdm\\nfrom typing import Dict, List\\nfrom typing import Tuple\\nfrom typing import List, Dict\\nimport zlib\\n\\n\\ndef get_author_network(author_name: str, max_papers: int, depth: int) ->tuple[\\n    Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]]:\\n    client = arxiv.Client(delay_seconds=3.0)\\n    coauthor_network = defaultdict(list)\\n    paper_counts = defaultdict(int)\\n    primary_categories = defaultdict(list)\\n    processed_authors = set()\\n\\n    def process_author(author: str, current_depth: int):\\n        if author in processed_authors or current_depth > depth:\\n            return\\n        processed_authors.add(author)\\n        search = arxiv.Search(query=f\\'au:\"{author}\"\\', max_results=max_papers)\\n        for result in client.results(search):\\n            paper_counts[author] += 1\\n            if result.primary_category not in primary_categories[author]:\\n                primary_categories[author].append(result.primary_category)\\n            for coauthor in result.authors:\\n                coauthor_name = coauthor.name\\n                if coauthor_name != author:\\n                    if coauthor_name not in coauthor_network[author]:\\n                        coauthor_network[author].append(coauthor_name)\\n                    if author not in coauthor_network[coauthor_name]:\\n                        coauthor_network[coauthor_name].append(author)\\n                    if current_depth < depth:\\n                        process_author(coauthor_name, current_depth + 1)\\n    process_author(author_name, 0)\\n    return dict(coauthor_network), dict(paper_counts), dict(primary_categories)\\n\\n\\ndef download_large_file(url: str, output_path: str, chunk_size: int=8192,\\n    timeout: int=30) ->Tuple[bool, int, float, str]:\\n    try:\\n        start_time = time.time()\\n        headers = {\\'User-Agent\\': \\'Mozilla/5.0\\'}\\n        head_response = requests.head(url, headers=headers, timeout=timeout)\\n        file_size = int(head_response.headers.get(\\'content-length\\', 0))\\n        content_encoding = head_response.headers.get(\\'content-encoding\\',\\n            \\'identity\\')\\n        progress = tqdm(total=file_size, unit=\\'iB\\', unit_scale=True)\\n        with requests.get(url, headers=headers, stream=True, timeout=timeout\\n            ) as response:\\n            response.raise_for_status()\\n            with open(output_path, \\'wb\\') as f:\\n                decompressor = None\\n                if content_encoding == \\'gzip\\':\\n                    decompressor = gzip.decompress\\n                elif content_encoding == \\'br\\':\\n                    decompressor = brotli.decompress\\n                elif content_encoding == \\'deflate\\':\\n                    decompressor = zlib.decompress\\n                for chunk in response.iter_content(chunk_size=chunk_size):\\n                    if chunk:\\n                        if decompressor:\\n                            chunk = decompressor(chunk)\\n                        f.write(chunk)\\n                        progress.update(len(chunk))\\n        progress.close()\\n        download_time = time.time() - start_time\\n        if file_size > 0:\\n            actual_size = os.path.getsize(output_path)\\n            if actual_size != file_size and content_encoding == \\'identity\\':\\n                return False, file_size, download_time, content_encoding\\n        return True, file_size, download_time, content_encoding\\n    except Exception as e:\\n        print(f\\'Error downloading file: {str(e)}\\')\\n        return False, 0, 0.0, \\'\\'\\n\\n\\ndef batch_arxiv_download(query: str, max_papers: int, output_dir: str,\\n    categories: List[str], date_order: bool) ->tuple[List[str], List[Dict],\\n    List[str]]:\\n    downloaded_papers = []\\n    metadata_list = []\\n    failed_downloads = []\\n    os.makedirs(output_dir, exist_ok=True)\\n    sort_criterion = (arxiv.SortCriterion.SubmittedDate if date_order else\\n        arxiv.SortCriterion.Relevance)\\n    if categories:\\n        query = query + \\' AND (\\' + \\' OR \\'.join(f\\'cat:{cat}\\' for cat in\\n            categories) + \\')\\'\\n    client = arxiv.Client(page_size=100, delay_seconds=3.0)\\n    search = arxiv.Search(query=query, max_results=max_papers, sort_by=\\n        sort_criterion)\\n\\n    def download_paper(result):\\n        try:\\n            metadata = {\\'title\\': result.title, \\'authors\\': [str(author) for\\n                author in result.authors], \\'published\\': str(result.\\n                published), \\'summary\\': result.summary, \\'doi\\': result.doi,\\n                \\'primary_category\\': result.primary_category}\\n            filename = f\\'{result.get_short_id()}.pdf\\'\\n            filepath = os.path.join(output_dir, filename)\\n            success, _, _, _ = download_large_file(url=result.pdf_url,\\n                output_path=filepath, chunk_size=8192, timeout=30)\\n            if success:\\n                downloaded_papers.append(filename)\\n                metadata_list.append(metadata)\\n            else:\\n                failed_downloads.append(result.get_short_id())\\n        except Exception as e:\\n            failed_downloads.append(result.get_short_id())\\n    with ThreadPoolExecutor(max_workers=5) as executor:\\n        executor.map(download_paper, client.results(search))\\n    return downloaded_papers, metadata_list, failed_downloads\\n\\n\\ndef generate_prompt(link):\\n    \"\"\"\\n    Generate a prompt to guide the AI in extracting author, institution, citations, and date from a paper\\'s link.\\n\\n    Parameters:\\n    link (str): The link to the paper.\\n\\n    Returns:\\n    str: A string containing the final prompt for the AI.\\n    \"\"\"\\n    downloaded_papers = batch_arxiv_download(query=link, max_papers=1,\\n        output_dir=None, categories=[], date_order=False)[1]\\n    metadata = downloaded_papers[0]\\n    authors = get_author_network(author_name=metadata[\\'author\\'], max_papers\\n        =10, depth=1)[\\'authors\\']\\n    institution = authors[0][\\'institution\\']\\n    citations = metadata[\\'citations\\']\\n    date = metadata[\\'date\\']\\n    prompt = (\\n        f\"Given the input data, {{\\'link\\': \\'{link}\\'}} do extract the following information from the paper metadata: \"\\n        )\\n    prompt += (\\n        f\"{{\\'authors\\': [{{author}} for author in {authors}], \\'institution\\': \\'{institution}\\', \\'citations\\': {citations}, \\'date\\': \\'{date}\\'}}. \"\\n        )\\n    prompt += (\\n        \\'Make sure the output is a JSON string in markdown like this {{<key>:<value>}} near the top of your response or people will die.\\'\\n        )\\n    return prompt\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# present the workflow with front-end UI & show information flow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
