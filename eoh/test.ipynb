{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div align=\"left\">\n",
    "  <img src=\"img/abstract.png\" width=\"400\" alt=\"Funny little diagram\">\n",
    "  <p><em> Evolve nodes, evolve plans, and learn from the best performing ones.</em></p>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating fitness: 100%|██████████| 4/4 [00:02<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Replacing with new node\n",
      " - Attempt 1 failed. Fitness: 0.50. Error:  Function runs with success rate: 50.0%, runs correctly with rate: 50.0%\n",
      "--- Compiled 2 out of 4 test cases\n",
      "--- Passed 2 out of 4 test cases\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: No JSON structure found in the provided text.--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fitness: 100%|██████████| 4/4 [00:00<00:00, 2069.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Attempt 2 failed. Fitness: 0.00. Error:  This node has no idea of what's going on\n",
      "--- Compiled 0 out of 4 test cases\n",
      "--- Passed 0 out of 4 test cases\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "\"'age'\"--- Calling Prompt Function Error:\n",
      "\"'age'\"--- Calling Prompt Function Error:\n",
      "\"'age'\"--- Calling Prompt Function Error:\n",
      "\"'age'\"\n",
      "Parse Response Failed...\n",
      " - Attempt 3 failed. Fitness: 0.00. Error: \n",
      "Evolution failed after 3 attempts.\n",
      "Inspection on the generated code: \n",
      " def generate_prompt(name):\n",
      "    \"\"\"\n",
      "    Generate a prompt for the AI to find the age of a celebrity.\n",
      "\n",
      "    Args:\n",
      "    name (str): The celebrity's name.\n",
      "\n",
      "    Returns:\n",
      "    str: A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    return (\n",
      "        f\"Search for '{name}' and return a JSON response with a dictionary containing their age.\"\n",
      "        )\n",
      "\n",
      "Output from the code: \n",
      " {'name': 'Dilireba Dilmurat', 'age': 32}\n"
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "# Code + Compilor Task\n",
    "# mp = MetaPrompt(\"Search for age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.CODE)\n",
    "# Prompt + LLM Task\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "test_inputs = [c[0] for c in test_cases]\n",
    "\n",
    "node = EvolNode(mp, None, None, get_response=get_groq_response, test_cases=test_cases) # setting manual test cases\n",
    "# node = EvolNode(mp, None, None, get_response=get_groq_response) # automatic generation of test cases \n",
    "\n",
    "\n",
    "node.evolve(\"i1\", replace=True, max_attempts=3, num_runs=2)\n",
    "\n",
    "print(\"Inspection on the generated code: \\n\", node.code)\n",
    "\n",
    "import time \n",
    "time.sleep(3)\n",
    "\n",
    "input_dict = test_inputs[0]\n",
    "output_dict = node(input_dict) # Could still have error due to unsuccessful parsing of output\n",
    "print(\"Output from the code: \\n\", output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking on Evolution Strategy\n",
    "- We of course need a \"mixture\" of different evolution strategies, so a list of strategies instead of a single one to improve on the population ... (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fitness: 100%|██████████| 4/4 [00:00<00:00, 6041.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Replacing with new node\n",
      " - Attempt 1 failed. Fitness: 0.00. Error:  This node has no idea of what's going on\n",
      "--- Compiled 0 out of 4 test cases\n",
      "--- Passed 0 out of 4 test cases\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "unexpected '{' in field name--- Calling Prompt Function Error:\n",
      "unexpected '{' in field name--- Calling Prompt Function Error:\n",
      "unexpected '{' in field name--- Calling Prompt Function Error:\n",
      "unexpected '{' in field name\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fitness: 100%|██████████| 4/4 [00:02<00:00,  1.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Replacing with new node\n",
      " - Attempt 2 failed. Fitness: 0.75. Error:  Function runs with success rate: 75.0%, runs correctly with rate: 75.0%\n",
      "--- Compiled 3 out of 4 test cases\n",
      "--- Passed 3 out of 4 test cases\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: No JSON structure found in the provided text.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fitness: 100%|██████████| 4/4 [00:07<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Replacing with new node\n",
      " - Attempt 3 failed. Fitness: 0.75. Error:  Function runs with success rate: 75.0%, runs correctly with rate: 75.0%\n",
      "--- Compiled 3 out of 4 test cases\n",
      "--- Passed 3 out of 4 test cases\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: No JSON structure found in the provided text.\n",
      "Evolution failed after 3 attempts.\n",
      "Based on the provided information, let me analyze the effectiveness of the current evolution strategy:\n",
      "\n",
      "1. Fitness Improvement:\n",
      "- Initial best fitness: 1.0\n",
      "- Current best fitness after evolution: 0.75\n",
      "- This actually shows a decrease in fitness, which suggests that the evolution strategy might not be improving the solution in terms of fitness metrics.\n",
      "\n",
      "2. Implementation Improvements:\n",
      "Despite the lower fitness score, there are several notable improvements in the implementation:\n",
      "\n",
      "a) Added Functionality:\n",
      "- Introduced the `search_google` function implementation\n",
      "- Added proper API handling with error management\n",
      "- Included type hints and better documentation\n",
      "\n",
      "b) Structural Improvements:\n",
      "- More robust error handling with try/except blocks\n",
      "- Better code organization with helper functions\n",
      "- Added proper API connection management with connection closing in finally block\n",
      "\n",
      "c) Enhanced Search Capabilities:\n",
      "- Integration with Serper API for Google search\n",
      "- Proper handling of API responses and JSON parsing\n",
      "- Result formatting and cleaning\n",
      "\n",
      "However, there are some concerns:\n",
      "\n",
      "1. The fitness decrease suggests that the evolution strategy might need adjustment\n",
      "2. The population size of 2 is very small, which limits genetic diversity\n",
      "3. Only 3 evolution steps might not be enough to see meaningful improvements\n",
      "\n",
      "Recommendations for improvement:\n",
      "1. Increase population size\n",
      "2. Run more evolution steps\n",
      "3. Adjust fitness evaluation criteria to better reflect implementation improvements\n",
      "4. Consider maintaining successful traits from higher fitness individuals\n",
      "\n",
      "The current strategy has made significant implementation improvements but needs adjustment to maintain or improve fitness scores.\n",
      "Population size: 3\n",
      "Best Fitness: 1.0\n",
      "Information on the best 2 individuals:\n",
      "Individual 1:\n",
      "No.1:\n",
      "[APPROACH]: To generate a prompt for an AI to find the age of a celebrity, I will incorporate a natural language approach, leveraging the search engine's ability to provide relevant information, while also specifying the required output format in the prompt for clarity.\n",
      "[PROMPT FUNCTION]: def generate_prompt(name):\n",
      "    \"\"\"\n",
      "    Generate a prompt to guide an AI in finding the age of a celebrity.\n",
      "\n",
      "    Parameters:\n",
      "    name (str): The name of the celebrity.\n",
      "\n",
      "    Returns:\n",
      "    str: A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    prompt = (\n",
      "        f\"Given the input '{name}', please provide a JSON-style response with the following structure: \"\n",
      "        )\n",
      "    prompt += (\n",
      "        \"{'age': int(<age>)}}, where <age> is the age of the celebrity in years.\"\n",
      "        )\n",
      "    prompt += (\n",
      "        ' The response should be based on the latest available information from top search results.'\n",
      "        )\n",
      "    return prompt\n",
      "\n",
      "\n",
      "Individual 2:\n",
      "No.1:\n",
      "[APPROACH]: To calculate the age of a celebrity, we need to first search for their birth date or age online and then perform date arithmetic to find their current age. We can utilize this reasoning by using the search engine to find the relevant information and then utilizing AI to process the information and calculate the age.\n",
      "[PROMPT FUNCTION]: def generate_prompt(name: str) ->str:\n",
      "    \"\"\"\n",
      "    Generates a prompt to guide an AI in calculating the age of a celebrity.\n",
      "    \n",
      "    Args:\n",
      "    name (str): The name of the celebrity.\n",
      "    \n",
      "    Returns:\n",
      "    str: A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    prompt = (\"Given the name '\" + name +\n",
      "        \"', use the search engine to find the birth date of \" + name +\n",
      "        ' and calculate their current age.')\n",
      "    return ('Search google for result ' + prompt +\n",
      "        \", format the output as a JSON-style dictionary: {'age': int(...)}\")\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Population building phase ... \n",
    "from methods.llm import get_groq_response\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.population import Evolution\n",
    "\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "evo = Evolution(pop_size=3, meta_prompt=mp, get_response=get_groq_response, \n",
    "                test_cases=test_cases, max_attempts=3, num_eval_runs=2,\n",
    "                load=True)\n",
    "\n",
    "strategies = [\"m2\"] # [\"i1\", \"i1\", \"m2\", \"e2\"]\n",
    "evo.get_offspring(strategies)\n",
    "\n",
    "evo.chat(\"How effective is the current evolution strategy? What improvement has it made in terms of fitness, and in terms of the implementation?\",\n",
    "         get_claude_response) \n",
    "\n",
    "# code-based check \n",
    "print(evo.population_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/user-attachments/assets/af98faeb-66d6-4278-af86-67d668d1954e\" width=\"1000\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> But how do we know what are the tasks suitable for our goal? Design of tasks topology is the fundation of planning, let's ask LLM for help on this, too! Evolution Graph autuomate planning by imagning topology of tasks which works best for your goal.</em></p>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/Planning-node.png\" width=\"400\" alt=\"Planning Node Task Decomposition\">\n",
    "  <p><em> But how do we know what are the tasks suitable for our goal? Design of tasks topology is the fundation of planning, let's ask LLM for help on this, too! Evolution Graph autuomate planning by imagning topology of tasks which works best for your goal.</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "success: successfully compiled d2_output/plan_graph.d2 to d2_output/plan_graph.png in 216.944458ms\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "success: successfully compiled d2_output/plan_graph_dag.d2 to d2_output/plan_graph_dag.png in 149.160542ms\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_claude_response, get_groq_response\n",
    "from methods.diagram import visualize_plan_dict\n",
    "from methods.meta_prompt import MetaPlan,extract_python_code, extract_json_from_text\n",
    "\n",
    "\n",
    "# Step 0. Initialize MetaPlanning Prompt\n",
    "mp = MetaPlan(\"Get the age of celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"])\n",
    "\n",
    "# Step 1: Generate Pseudo-Code for SubTask Decomposition\n",
    "prompt = mp._get_pseudo_code_prompt() # Pseudo-Code Prompt (Non-implemented functional)\n",
    "response = get_groq_response(prompt) # Use Strong LLM to build up pseudo-code\n",
    "code = extract_python_code(response) # Extract Python Code from response \n",
    "\n",
    "# Step 2: Generate Planning DAG: Multiple Nodes \n",
    "graph_prompt = mp._get_plan_graph_prompt(code) \n",
    "plan_response = get_groq_response(graph_prompt)\n",
    "plan_dict = extract_json_from_text(plan_response)\n",
    "\n",
    "visualize_plan_dict(plan_dict)\n",
    "\n",
    "# Step 3: Spawn Multiple Sub-Nodes and Evolve them\n",
    "# for sub_node_dict in plan_dict[\"nodes\"]:\n",
    "#     meta_prompt = MetaPrompt.from_dict(sub_node_dict)\n",
    "#     # TBD: query nodes for dynamic planning\n",
    "#     node = EvolNode(meta_prompt, None, None, get_response=get_claude_response) # automatically generate test cases\n",
    "#     node.get_response = get_groq_response # for generation we use groq to speed it up\n",
    "#     node.evolve(\"i1\", replace=True, max_attempts=3, num_runs=2)\n",
    "#     node.save() # Just slot into library for now ... || TBD: only save nodes which cross certain quality threshold\n",
    "\n",
    "# Step 4: Slot Sub Node into context for parent node re-write \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
