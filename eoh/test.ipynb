{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div align=\"left\">\n",
    "  <img src=\"img/abstract.png\" width=\"400\" alt=\"Funny little diagram\">\n",
    "  <p><em> Evolve nodes, evolve plans, and learn from the best performing ones.</em></p>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Evaluating fitness: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Replacing with new node\n",
      " - Attempt 1 failed. Fitness: 0.50. Error:  Function runs with success rate: 50.0%, runs correctly with rate: 50.0%\n",
      "--- Compiled 2 out of 4 test cases\n",
      "--- Passed 2 out of 4 test cases\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "No code block found in the response.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating fitness: 100%|██████████| 4/4 [00:00<00:00, 6379.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Attempt 2 failed. Fitness: 0.00. Error:  This node has no idea of what's going on\n",
      "--- Compiled 0 out of 4 test cases\n",
      "--- Passed 0 out of 4 test cases\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "'generate_prompt'--- Calling Prompt Function Error:\n",
      "'generate_prompt'--- Calling Prompt Function Error:\n",
      "'generate_prompt'--- Calling Prompt Function Error:\n",
      "'generate_prompt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating fitness: 100%|██████████| 4/4 [00:02<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Attempt 3 failed. Fitness: 0.00. Error:  This node has no idea of what's going on\n",
      "--- Compiled 0 out of 4 test cases\n",
      "--- Passed 0 out of 4 test cases\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "Input: {'name': 'Dilireba'}, Output is missing or of wrong type, Expected: {'age': 32}\n",
      "Input: {'name': 'ChengXiao'}, Output is missing or of wrong type, Expected: {'age': 26}\n",
      "\n",
      "Error Message:\n",
      "--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: No JSON structure found in the provided text.--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: No JSON structure found in the provided text.--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: No JSON structure found in the provided text.--- Calling Prompt Function Error:\n",
      "Failed to parse LLM response: No JSON structure found in the provided text.\n",
      "Evolution failed after 3 attempts.\n",
      "Inspection on the generated code: \n",
      " def generate_prompt(name):\n",
      "    \"\"\"\n",
      "    Generate a prompt to guide an AI in finding the age of a celebrity.\n",
      "\n",
      "    Args:\n",
      "    - name (str): The name of the celebrity.\n",
      "\n",
      "    Returns:\n",
      "    - prompt (str): A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    prompt = (\n",
      "        f\"Search for top search results on Google for the query '{name}'\\n\")\n",
      "    prompt += f\"\"\"Search for top search results on Google for the query '{name} birth year' or '{name} age'\n",
      "\"\"\"\n",
      "    prompt += \"\"\"Expect a JSON-style response in the format of a dictionary with the key 'age' and its value as an integer.\n",
      "\"\"\"\n",
      "    return prompt\n",
      "\n",
      "Output from the code: \n",
      " {'age': 32}\n"
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "# Code + Compilor Task\n",
    "# mp = MetaPrompt(\"Search for age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.CODE)\n",
    "# Prompt + LLM Task\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "test_inputs = [c[0] for c in test_cases]\n",
    "\n",
    "node = EvolNode(mp, None, None, get_response=get_groq_response, test_cases=test_cases) # setting manual test cases\n",
    "# node = EvolNode(mp, None, None, get_response=get_groq_response) # automatic generation of test cases \n",
    "\n",
    "\n",
    "node.evolve(\"i1\", replace=True, max_attempts=3, num_runs=2)\n",
    "\n",
    "print(\"Inspection on the generated code: \\n\", node.code)\n",
    "\n",
    "import time \n",
    "time.sleep(3)\n",
    "\n",
    "input_dict = test_inputs[0]\n",
    "output_dict = node(input_dict) # Could still have error due to unsuccessful parsing of output\n",
    "print(\"Output from the code: \\n\", output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'age': 31}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Benchmarking on Evolution Strategy\n",
    "- We of course need a \"mixture\" of different evolution strategies, so a list of strategies instead of a single one to improve on the population ... (TBD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    }
   ],
   "source": [
    "# Population building phase ... \n",
    "from methods.llm import get_groq_response\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.population import Evolution\n",
    "\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "evo = Evolution(pop_size=3, meta_prompt=mp, get_response=get_groq_response, \n",
    "                test_cases=test_cases, max_attempts=3, num_eval_runs=2,\n",
    "                load=True)\n",
    "\n",
    "strategies = [\"m2\"] # [\"i1\", \"i1\", \"m2\", \"e2\"]\n",
    "evo.get_offspring(strategies)\n",
    "\n",
    "evo.chat(\"How effective is the current evolution strategy? What improvement has it made in terms of fitness, and in terms of the implementation?\",\n",
    "         get_claude_response) \n",
    "\n",
    "# code-based check \n",
    "print(evo.population_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/user-attachments/assets/af98faeb-66d6-4278-af86-67d668d1954e\" width=\"1000\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> But how do we know what are the tasks suitable for our goal? Design of tasks topology is the fundation of planning, let's ask LLM for help on this, too! Evolution Graph autuomate planning by imagning topology of tasks which works best for your goal.</em></p>\n",
    "</div>\n",
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/Planning-node.png\" width=\"400\" alt=\"Planning Node Task Decomposition\">\n",
    "  <p><em> But how do we know what are the tasks suitable for our goal? Design of tasks topology is the fundation of planning, let's ask LLM for help on this, too! Evolution Graph autuomate planning by imagning topology of tasks which works best for your goal.</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "success: successfully compiled d2_output/plan_graph.d2 to d2_output/plan_graph.png in 163.391625ms\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "success: successfully compiled d2_output/plan_graph_dag.d2 to d2_output/plan_graph_dag.png in 150.068084ms\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_claude_response\n",
    "from methods.diagram import visualize_plan_dict\n",
    "from methods.meta_prompt import MetaPlan,extract_python_code, extract_json_from_text\n",
    "\n",
    "\n",
    "# Step 0. Initialize MetaPlanning Prompt\n",
    "mp = MetaPlan(\"Get the age of celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"])\n",
    "\n",
    "# Step 1: Generate Pseudo-Code for SubTask Decomposition\n",
    "prompt = mp._get_pseudo_code_prompt() # Pseudo-Code Prompt (Non-implemented functional)\n",
    "response = get_claude_response(prompt) # Use Strong LLM to build up pseudo-code\n",
    "code = extract_python_code(response) # Extract Python Code from response \n",
    "\n",
    "# Step 2: Generate Planning DAG: Multiple Nodes \n",
    "graph_prompt = mp._get_plan_graph_prompt(code) \n",
    "plan_response = get_claude_response(graph_prompt)\n",
    "plan_dict = extract_json_from_text(plan_response)\n",
    "\n",
    "visualize_plan_dict(plan_dict)\n",
    "\n",
    "# Step 3: Spawn Multiple Sub-Nodes and Evolve them\n",
    "# for sub_node_dict in plan_dict[\"nodes\"]:\n",
    "#     meta_prompt = MetaPrompt.from_dict(sub_node_dict)\n",
    "#     # TBD: query nodes for dynamic planning\n",
    "#     node = EvolNode(meta_prompt, None, None, get_response=get_claude_response) # automatically generate test cases\n",
    "#     node.get_response = get_groq_response # for generation we use groq to speed it up\n",
    "#     node.evolve(\"i1\", replace=True, max_attempts=3, num_runs=2)\n",
    "#     node.save() # Just slot into library for now ... || TBD: only save nodes which cross certain quality threshold\n",
    "\n",
    "# Step 4: Slot Sub Node into context for parent node re-write \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
