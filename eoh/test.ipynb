{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution Node \n",
    "- Task, Input, Output\n",
    "- LLM guided evolution\n",
    "- LLM-engineered Code Function or Prompt\n",
    "\n",
    "Note: \n",
    "* Ok to have error, as evolution is decoupled with evaluation for now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'List'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m reasoning, code \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39m_evolve(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi1\u001b[39m\u001b[38;5;124m\"\u001b[39m, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m input_dict \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m}\n\u001b[0;32m---> 10\u001b[0m node(input_dict)\n",
      "File \u001b[0;32m~/Implementation/research/heuristic-search/eoh/methods/eoh_evolution.py:72\u001b[0m, in \u001b[0;36mEvolNode.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mTBD: Inheritance to accumulated codebase with 'file_path' | Graph Topology naturally enables inheritance\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mTBD: Stricter input / output type checking to ensure composibility\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_prompt\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m PromptMode\u001b[38;5;241m.\u001b[39mCODE:\n\u001b[0;32m---> 72\u001b[0m     output_value \u001b[38;5;241m=\u001b[39m call_func_code(inputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcode, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_prompt\u001b[38;5;241m.\u001b[39mfunc_name, file_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     73\u001b[0m     output_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_prompt\u001b[38;5;241m.\u001b[39moutput[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {output_name: output_value}\n",
      "File \u001b[0;32m~/Implementation/research/heuristic-search/eoh/methods/meta_execute.py:54\u001b[0m, in \u001b[0;36mcall_func_code\u001b[0;34m(input_data, code, func_name, file_path)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;66;03m# Use the existing code string approach\u001b[39;00m\n\u001b[1;32m     53\u001b[0m     mod \u001b[38;5;241m=\u001b[39m types\u001b[38;5;241m.\u001b[39mModuleType(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdynamic_module\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 54\u001b[0m     exec(code, mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Get the function from the module\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mod\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'List'"
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.eoh_evolution import EvolNode\n",
    "\n",
    "\n",
    "# Code + Compilor Task\n",
    "mp = MetaPrompt(\"Generate Fibonacci sequence.\", \"fibonacci\", [\"n\"], [\"sequence\"], [\"int\"], [\"list\"], PromptMode.CODE) # \n",
    "node = EvolNode(mp, None, None)\n",
    "reasoning, code = node._evolve(\"i1\", replace=True)\n",
    "input_dict = {\"n\": 10}\n",
    "\n",
    "node(input_dict) # Ok we need a output dictionary here as well ...\n",
    "\n",
    "\n",
    "# Prompt + LLM Task\n",
    "# mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "# node = EvolNode(mp, None, None)\n",
    "# reasoning, code = node._evolve(\"i1\", replace=True)\n",
    "# input_dict = {\"name\": \"Dilireba\"}\n",
    "# node(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import List\n",
      "\n",
      "def fibonacci(n: int) -> List[int]:\n",
      "    if n <= 0:\n",
      "        return []\n",
      "    \n",
      "    sequence = [0, 1]\n",
      "    for i in range(2, n):\n",
      "        sequence.append(sequence[-1] + sequence[-2])\n",
      "    \n",
      "    return sequence[:n]\n"
     ]
    }
   ],
   "source": [
    "print(node.code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evolution Graph\n",
    "- Take care of connecting each components (unfolding dictionary object etc.)\n",
    "- Planning topology with evolution node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaPrompt I1: \n",
      "Goal: Help me date Dilireba, I'm a white colar worker living on salary.\n",
      "First, describe the intuition for your tactics and main steps in one sentence. The description must be inside a brace.\n",
      "Generate a JSON-style plan represented as a Directed Acyclic Graph (DAG) to achieve the goal. Use creative topology in the DAG, include parallel tasks if required.\n",
      "\n",
      "The plan should include:\n",
      "\n",
      "- **Nodes**: Each node represents a key action or step and must contain the following attributes:\n",
      "  - `task`: Description of the task.\n",
      "  - `name`: Concise name used for the task function.\n",
      "  - `input`: The resources, information, or prerequisites needed to perform the action.\n",
      "  - `output`: The immediate result or outcome of the action.\n",
      "  - `target`: The purpose or goal that the action contributes to.\n",
      "  - `mode`: The execution mode for this task (\"CODE\" or \"PROMPT\").\n",
      "\n",
      "- **Edges**: Each edge represents a dependency or relationship between nodes, indicating that one step supports or leads to another.\n",
      "  - `source`: The `id` of the source node (the preceding action).\n",
      "  - `target`: The `id` of the target node (the subsequent action).\n",
      "\n",
      "**Output Format:**\n",
      "\n",
      "Provide the output in the following JSON structure:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"task\": \"Task 1\",\n",
      "      \"name\": \"task_1\"\n",
      "      \"input\": \"Inputs required for Action 1\",\n",
      "      \"output\": \"Outputs/result of Action 1\",\n",
      "      \"target\": \"Purpose of Action 1\"\n",
      "      \"mode\": \"CODE\"\n",
      "    },\n",
      "    {\n",
      "      \"task\": \"Task 2\",\n",
      "      \"name\": \"task_2\",\n",
      "      \"input\": \"Inputs required for Action 2\",\n",
      "      \"output\": \"Outputs/result of Action 2\",\n",
      "      \"target\": \"Purpose of Action 2\",\n",
      "      \"mode\": \"PROMPT\"\n",
      "    }\n",
      "    // Add more nodes as needed\n",
      "  ],\n",
      "  \"edges\": [\n",
      "    {\n",
      "      \"source\": \"task_1\",\n",
      "      \"target\": \"task_2\"\n",
      "    }\n",
      "    // Add more edges as needed\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPlan, extract_json_from_text\n",
    "\n",
    "mp = MetaPlan(\"Help me date Dilireba, I'm a white colar worker living on salary.\")\n",
    "prompt = mp._get_prompt_i1()\n",
    "print(\"MetaPrompt I1: \")\n",
    "print(prompt)\n",
    "\n",
    "from methods.llm import get_openai_response\n",
    "response = get_openai_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "# from methods.meta_prompt import parse_json_from_response\n",
    "\n",
    "tactic = re.findall(r\"\\{(.*)\\}\", response, re.DOTALL)\n",
    "plan_dict = extract_json_from_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<methods.eoh_evolution.EvolNode at 0x2893758d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, MetaPlan, extract_json_from_text\n",
    "from methods.eoh_evolution import EvolNode\n",
    "from methods.llm import get_openai_response as get_response\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# Collect MetaPrompt from parsed Plan-Graph Response\n",
    "meta_node_prompts = []\n",
    "for node in plan_dict[\"nodes\"]:\n",
    "    node_prompt = MetaPrompt(task=node.get(\"task\"),  func_name=node.get(\"name\"), input=node.get(\"input\"), output=node.get(\"output\"), mode=node.get(\"mode\").lower())\n",
    "    meta_node_prompts.append(node_prompt)\n",
    "    \n",
    "edges = plan_dict[\"edges\"]\n",
    "\n",
    "EvolNode(meta_prompt = node_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EvalNode Test: \n",
    "At the beginning level, generation of EvalNode (when code is used here) should compile with success, and it should takes input and produce output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
