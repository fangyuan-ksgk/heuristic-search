{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div align=\"left\">\n",
    "  <img src=\"img/abstract.png\" width=\"400\" alt=\"Funny little diagram\">\n",
    "  <p><em> Evolve nodes, evolve plans, and learn from the best performing ones.</em></p>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Initialization (Refactoring ...)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    }
   ],
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-8sz1f7zg7oy0ui\"\n",
<<<<<<< HEAD
    "api_key = os.environ[\"RUNPOD_API_KEY\"]\n",
=======
    "api_key = \"rpa_EPOJED42G59S80Y6SKMCOI330EQU4JPPMKV2UD2W7j0uku\"\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [01:11<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 71.70s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 24/24 [00:30<00:00,  1.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 30.18s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 0.00s, 0 errors\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
      "  ‚ö° Structural fitness: 0.33\n",
      "  üéØ Functional fitness: 1.00\n",
      "  ‚≠ê Global fitness:     0.67\n",
      "  üîÑ Compiled solutions:        16\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 2.94s\n",
      "     :: Evolution time: 71.73s\n",
      "     :: Evaluation time: 53.85s\n",
      "     :: Total time: 128.53s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 1: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "list index out of range\n",
      "invalid literal for int() with base 10: 'WJSN'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 2: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 3: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "'str' object has no attribute 'result'\n",
      "'str' object has no attribute 'result'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 4: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- No JSON structure found in the provided text.\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 5: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "invalid literal for int() with base 10: '{'\n",
      "invalid literal for int() with base 10: '{'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 6: Fitness: 8.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'name': 'ChengXiao'}, Error message: Key age not found in prediction output\n",
      "\n",
      "\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 7: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 8: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Invalid format specifier '<value>' for object of type 'str'\n",
      "Invalid format specifier '<value>' for object of type 'str'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 9: Fitness: 16.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'name': 'ChengXiao', 'birthdate': 'July 15, 1998'}, Error message: Key age not found in prediction output\n",
      "\n",
      "\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Invalid control character at: line 1 column 17 (char 16)AstLiteralError : \n",
      "unterminated string literal (detected at line 1) (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 10: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 11: Fitness: 16.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'age': 31}, Error message: \n",
      "Value mismatch for key age: 31 != 32\n",
      "\n",
      "\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 25}, Error message: \n",
      "Value mismatch for key age: 25 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 12: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 13: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 14: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 15: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "Error occurred during API request: Function execution timed out (> 3 seconds)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:25<00:00,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 25.43s, 0 errors\n",
      "Output dict:  {'age': 32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "# Code + Compilor Task\n",
    "# mp = MetaPrompt(\"Search for age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.CODE)\n",
    "# Prompt + LLM Task\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "test_inputs = [c[0] for c in test_cases]\n",
    "\n",
    "node = EvolNode(mp, None, None, get_response=get_endpoint_response, test_cases=test_cases) # setting manual test cases\n",
    "\n",
    "node.evolve(\"i1\", replace=True, batch_size=20, num_runs=2, print_summary=True) # Scale up batch size\n",
    "\n",
    "\n",
    "input_dict = {\"name\": \"Dilireba\"}\n",
<<<<<<< HEAD
    "node.get_response = get_groq_response # fast sequential inference \n",
    "output_dict = node(input_dict) # use node as a function\n",
    "print(\"Output dict: \", output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shah.mahir/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-11-28 17:26:31.390863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n",
      "```json\n",
      "{\n",
      "\"nodes\": [\n",
      "    {\n",
      "        \"task\": \"Download multiple files in parallel with custom retry logic, progress tracking, and error handling using Session objects\",\n",
      "        \"name\": \"parallel_download_with_retry\",\n",
      "        \"inputs\": [\"urls\", \"output_dir\", \"max_retries\", \"timeout\", \"max_workers\"],\n",
      "        \"input_types\": [\"List[str]\", \"str\", \"int\", \"int\", \"int\"], \n",
      "        \"outputs\": [\"successful_downloads\", \"failed_downloads\", \"download_stats\"],\n",
      "        \"output_types\": [\"List[str]\", \"Dict[str,str]\", \"Dict[str,Any]\"],\n",
      "        \"target\": \"Robust parallel file downloading with retries and detailed stats\",\n",
      "        \"mode\": \"CODE\",\n",
      "        \"relevant_docs\": \"Uses requests.Session() and concurrent.futures for parallel downloads. Key functions:\\n- session.mount() for custom retry adapters\\n- session.get(url, stream=True) for streaming downloads\\n- response.iter_content(chunk_size) for chunked downloads\\nExample usage:\\nsession = requests.Session()\\nretries = Retry(total=3, backoff_factor=0.1)\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nImport requirements:\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter \\nfrom urllib3.util.retry import Retry\\nfrom concurrent.futures import ThreadPoolExecutor\"\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Stream and process large files with custom header management and authentication while monitoring memory usage\", \n",
      "        \"name\": \"streaming_file_processor\",\n",
      "        \"inputs\": [\"url\", \"chunk_size\", \"auth_token\", \"custom_headers\", \"processor_func\"],\n",
      "        \"input_types\": [\"str\", \"int\", \"str\", \"Dict[str,str]\", \"Callable\"],\n",
      "        \"outputs\": [\"processed_chunks\", \"total_bytes\", \"processing_stats\"],\n",
      "        \"output_types\": [\"List[Any]\", \"int\", \"Dict[str,float]\"],\n",
      "        \"target\": \"Memory-efficient processing of large files with authentication\",\n",
      "        \"mode\": \"CODE\",\n",
      "        \"relevant_docs\": \"Uses requests streaming functionality with custom processing:\\n- requests.get(stream=True) for streaming response\\n- response.iter_content() for chunk iteration\\n- requests.auth for authentication handling\\nExample usage:\\nheaders = {'Authorization': f'Bearer {token}'}\\nwith requests.get(url, stream=True, headers=headers) as r:\\n    for chunk in r.iter_content(chunk_size=8192):\\n        process_chunk(chunk)\\nImport requirements:\\nimport requests\\nfrom requests.auth import AuthBase\"\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Create a custom transport adapter for advanced SSL configuration including certificate pinning and version control\",\n",
      "        \"name\": \"custom_ssl_adapter\",\n",
      "        \"inputs\": [\"cert_path\", \"ssl_version\", \"verify_fingerprint\", \"hostname\"],\n",
      "        \"input_types\": [\"str\", \"str\", \"str\", \"str\"],\n",
      "        \"outputs\": [\"adapter\", \"verification_info\"],\n",
      "        \"output_types\": [\"HTTPAdapter\", \"Dict[str,bool]\"],\n",
      "        \"target\": \"Enhanced SSL security with certificate pinning\",\n",
      "        \"mode\": \"CODE\", \n",
      "        \"relevant_docs\": \"Example directly from documentation with modifications:\\nimport ssl\\nfrom urllib3.poolmanager import PoolManager\\nfrom requests.adapters import HTTPAdapter\\n\\nclass CustomSSLAdapter(HTTPAdapter):\\n    def __init__(self, ssl_version=None, **kwargs):\\n        self.ssl_version = ssl_version\\n        super().__init__(**kwargs)\\n\\n    def init_poolmanager(self, connections, maxsize, block=False):\\n        self.poolmanager = PoolManager(\\n            num_pools=connections,\\n            maxsize=maxsize,\\n            block=block,\\n            ssl_version=self.ssl_version)\\n\\nUsage:\\ns = requests.Session()\\nadapter = CustomSSLAdapter(ssl_version=ssl.PROTOCOL_TLSv1_2)\\ns.mount('https://', adapter)\"\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Implement automatic request retries with exponential backoff and custom status code handling\",\n",
      "        \"name\": \"retry_with_backoff\",\n",
      "        \"inputs\": [\"url\", \"max_retries\", \"backoff_factor\", \"status_forcelist\", \"allowed_methods\"],\n",
      "        \"input_types\": [\"str\", \"int\", \"float\", \"List[int]\", \"Set[str]\"],\n",
      "        \"outputs\": [\"response\", \"retry_stats\"],\n",
      "        \"output_types\": [\"Response\", \"Dict[str,int]\"],\n",
      "        \"target\": \"Reliable request handling with smart retries\",\n",
      "        \"mode\": \"CODE\",\n",
      "        \"relevant_docs\": \"Direct example from documentation:\\nfrom urllib3.util import Retry\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter\\n\\nretries = Retry(\\n    total=3,\\n    backoff_factor=0.1,\\n    status_forcelist=[502, 503, 504],\\n    allowed_methods={'GET', 'POST'}\\n)\\n\\nUsage:\\nsession = Session()\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nresponse = session.get(url)\\n\\nThe backoff_factor creates exponentially increasing delays:\\n{backoff_factor} * (2 ** ({retry} - 1)) seconds\"\n",
      "    }\n",
      "]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:19<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19d81b400>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:38<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19d818040>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:58<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19a86c340>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [01:17<?, ?case/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19d81af40>\n",
      "--- Generated 0 test cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:15<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 11 column 18 (char 366)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 4) <traceback object at 0x19d5a7800>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases: 5case [00:30,  6.16s/case]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "unhashable type: 'dict' <traceback object at 0x19d637480>\n",
      "--- Generated 5 test cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:18<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "'adapter' <traceback object at 0x19d621c00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:34<00:00, 11.40s/case]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated 3 test cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases: 5case [00:19,  3.83s/case]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "unhashable type: 'list' <traceback object at 0x19d621c80>\n",
      "--- Generated 5 test cases\n",
      "Uses requests.Session() and concurrent.futures for parallel downloads. Key functions:\n",
      "- session.mount() for custom retry adapters\n",
      "- session.get(url, stream=True) for streaming downloads\n",
      "- response.iter_content(chunk_size) for chunked downloads\n",
      "Example usage:\n",
      "session = requests.Session()\n",
      "retries = Retry(total=3, backoff_factor=0.1)\n",
      "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
      "Import requirements:\n",
      "from requests import Session\n",
      "from requests.adapters import HTTPAdapter \n",
      "from urllib3.util.retry import Retry\n",
      "from concurrent.futures import ThreadPoolExecutor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Download multiple files in parallel with custom retry logic, progress tracking, and error handling using Session objects\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named parallel_download_with_retry. This function should accept 5 input(s): 'urls', 'output_dir', 'max_retries', 'timeout', 'max_workers' with types List[str], str, int, int, int. The function should return 3 output(s): 'successful_downloads', 'failed_downloads', 'download_stats' with types List[str], Dict[str,str], Dict[str,Any].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: get_author_network, this fuction accept 3 input(s): 'author_name', 'max_papers', 'depth' with types str, int, int. The function return 3 output(s): 'coauthor_network', 'paper_counts', 'primary_categories' with types Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]. This function is used for Search for papers by a specific author and get their collaboration network from co-authors\\nIntuition: Search arXiv for papers by the specified author, build a network of co-authors by traversing collaborator papers up to given depth, and track paper counts and categories for each author\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Uses requests.Session() and concurrent.futures for parallel downloads. Key functions:\\n- session.mount() for custom retry adapters\\n- session.get(url, stream=True) for streaming downloads\\n- response.iter_content(chunk_size) for chunked downloads\\nExample usage:\\nsession = requests.Session()\\nretries = Retry(total=3, backoff_factor=0.1)\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nImport requirements:\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter \\nfrom urllib3.util.retry import Retry\\nfrom concurrent.futures import ThreadPoolExecutor\"]\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
      "  ‚ö° Structural fitness: 0.00\n",
      "  üéØ Functional fitness: 0.00\n",
      "  ‚≠ê Global fitness:     0.00\n",
      "  üîÑ Compiled solutions:        1\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 10.36s\n",
      "     :: Evolution time: 15.97s\n",
      "     :: Evaluation time: 0.00s\n",
      "     :: Total time: 26.33s\n",
      "\n",
      "Uses requests streaming functionality with custom processing:\n",
      "- requests.get(stream=True) for streaming response\n",
      "- response.iter_content() for chunk iteration\n",
      "- requests.auth for authentication handling\n",
      "Example usage:\n",
      "headers = {'Authorization': f'Bearer {token}'}\n",
      "with requests.get(url, stream=True, headers=headers) as r:\n",
      "    for chunk in r.iter_content(chunk_size=8192):\n",
      "        process_chunk(chunk)\n",
      "Import requirements:\n",
      "import requests\n",
      "from requests.auth import AuthBase\n",
      "[\"Stream and process large files with custom header management and authentication while monitoring memory usage\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named streaming_file_processor. This function should accept 5 input(s): 'url', 'chunk_size', 'auth_token', 'custom_headers', 'processor_func' with types str, int, str, Dict[str,str], Callable. The function should return 3 output(s): 'processed_chunks', 'total_bytes', 'processing_stats' with types List[Any], int, Dict[str,float].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: get_author_network, this fuction accept 3 input(s): 'author_name', 'max_papers', 'depth' with types str, int, int. The function return 3 output(s): 'coauthor_network', 'paper_counts', 'primary_categories' with types Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]. This function is used for Search for papers by a specific author and get their collaboration network from co-authors\\nIntuition: Search arXiv for papers by the specified author, build a network of co-authors by traversing collaborator papers up to given depth, and track paper counts and categories for each author\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Uses requests streaming functionality with custom processing:\\n- requests.get(stream=True) for streaming response\\n- response.iter_content() for chunk iteration\\n- requests.auth for authentication handling\\nExample usage:\\nheaders = {'Authorization': f'Bearer {token}'}\\nwith requests.get(url, stream=True, headers=headers) as r:\\n    for chunk in r.iter_content(chunk_size=8192):\\n        process_chunk(chunk)\\nImport requirements:\\nimport requests\\nfrom requests.auth import AuthBase\"]\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
      "  ‚ö° Structural fitness: 0.00\n",
      "  üéØ Functional fitness: 0.00\n",
      "  ‚≠ê Global fitness:     0.00\n",
      "  üîÑ Compiled solutions:        1\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 3.27s\n",
      "     :: Evolution time: 12.16s\n",
      "     :: Evaluation time: 0.01s\n",
      "     :: Total time: 15.44s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "Input data type mismatch for parameter 'auth_token'. Expected <class 'str'>, got <class 'NoneType'>\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "================================================================================\n",
      "\n",
      "Example directly from documentation with modifications:\n",
      "import ssl\n",
      "from urllib3.poolmanager import PoolManager\n",
      "from requests.adapters import HTTPAdapter\n",
      "\n",
      "class CustomSSLAdapter(HTTPAdapter):\n",
      "    def __init__(self, ssl_version=None, **kwargs):\n",
      "        self.ssl_version = ssl_version\n",
      "        super().__init__(**kwargs)\n",
      "\n",
      "    def init_poolmanager(self, connections, maxsize, block=False):\n",
      "        self.poolmanager = PoolManager(\n",
      "            num_pools=connections,\n",
      "            maxsize=maxsize,\n",
      "            block=block,\n",
      "            ssl_version=self.ssl_version)\n",
      "\n",
      "Usage:\n",
      "s = requests.Session()\n",
      "adapter = CustomSSLAdapter(ssl_version=ssl.PROTOCOL_TLSv1_2)\n",
      "s.mount('https://', adapter)\n",
      "[\"Create a custom transport adapter for advanced SSL configuration including certificate pinning and version control\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named custom_ssl_adapter. This function should accept 4 input(s): 'cert_path', 'ssl_version', 'verify_fingerprint', 'hostname' with types str, str, str, str. The function should return 2 output(s): 'adapter', 'verification_info' with types HTTPAdapter, Dict[str,bool].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: get_author_network, this fuction accept 3 input(s): 'author_name', 'max_papers', 'depth' with types str, int, int. The function return 3 output(s): 'coauthor_network', 'paper_counts', 'primary_categories' with types Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]. This function is used for Search for papers by a specific author and get their collaboration network from co-authors\\nIntuition: Search arXiv for papers by the specified author, build a network of co-authors by traversing collaborator papers up to given depth, and track paper counts and categories for each author\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Example directly from documentation with modifications:\\nimport ssl\\nfrom urllib3.poolmanager import PoolManager\\nfrom requests.adapters import HTTPAdapter\\n\\nclass CustomSSLAdapter(HTTPAdapter):\\n    def __init__(self, ssl_version=None, **kwargs):\\n        self.ssl_version = ssl_version\\n        super().__init__(**kwargs)\\n\\n    def init_poolmanager(self, connections, maxsize, block=False):\\n        self.poolmanager = PoolManager(\\n            num_pools=connections,\\n            maxsize=maxsize,\\n            block=block,\\n            ssl_version=self.ssl_version)\\n\\nUsage:\\ns = requests.Session()\\nadapter = CustomSSLAdapter(ssl_version=ssl.PROTOCOL_TLSv1_2)\\ns.mount('https://', adapter)\"]\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
      "  ‚ö° Structural fitness: 0.00\n",
      "  üéØ Functional fitness: 0.00\n",
      "  ‚≠ê Global fitness:     0.00\n",
      "  üîÑ Compiled solutions:        1\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 3.22s\n",
      "     :: Evolution time: 17.71s\n",
      "     :: Evaluation time: 0.10s\n",
      "     :: Total time: 21.04s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "[Errno 2] No such file or directory\n",
      "[Errno 2] No such file or directory\n",
      "[Errno 2] No such file or directory\n",
      "================================================================================\n",
      "\n",
      "Direct example from documentation:\n",
      "from urllib3.util import Retry\n",
      "from requests import Session\n",
      "from requests.adapters import HTTPAdapter\n",
      "\n",
      "retries = Retry(\n",
      "    total=3,\n",
      "    backoff_factor=0.1,\n",
      "    status_forcelist=[502, 503, 504],\n",
      "    allowed_methods={'GET', 'POST'}\n",
      ")\n",
      "\n",
      "Usage:\n",
      "session = Session()\n",
      "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
      "response = session.get(url)\n",
      "\n",
      "The backoff_factor creates exponentially increasing delays:\n",
      "{backoff_factor} * (2 ** ({retry} - 1)) seconds\n",
      "[\"Implement automatic request retries with exponential backoff and custom status code handling\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named retry_with_backoff. This function should accept 5 input(s): 'url', 'max_retries', 'backoff_factor', 'status_forcelist', 'allowed_methods' with types str, int, float, List[int], Set[str]. The function should return 2 output(s): 'response', 'retry_stats' with types Response, Dict[str,int].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: search_google, this fuction accept 1 input(s): 'query' with types str. The function return 1 output(s): 'result' with types str. This function is used for Search Google for result\\nIntuition: Search google for top search results\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Direct example from documentation:\\nfrom urllib3.util import Retry\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter\\n\\nretries = Retry(\\n    total=3,\\n    backoff_factor=0.1,\\n    status_forcelist=[502, 503, 504],\\n    allowed_methods={'GET', 'POST'}\\n)\\n\\nUsage:\\nsession = Session()\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nresponse = session.get(url)\\n\\nThe backoff_factor creates exponentially increasing delays:\\n{backoff_factor} * (2 ** ({retry} - 1)) seconds\"]\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
      "  ‚ö° Structural fitness: 0.00\n",
      "  üéØ Functional fitness: 0.00\n",
      "  ‚≠ê Global fitness:     0.00\n",
      "  üîÑ Compiled solutions:        1\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 3.11s\n",
      "     :: Evolution time: 8.95s\n",
      "     :: Evaluation time: 0.00s\n",
      "     :: Total time: 12.06s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from methods.evolnode import nodes_from_api\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "nodes = nodes_from_api(\"https://requests.readthedocs.io/en/latest/user/advanced/\", get_response=get_claude_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import brotli\n",
      "from concurrent.futures import ThreadPoolExecutor\n",
      "import gzip\n",
      "from pathlib import Path\n",
      "import requests\n",
      "from requests import Session\n",
      "from requests.adapters import HTTPAdapter\n",
      "import time\n",
      "from tqdm import tqdm\n",
      "from typing import Tuple\n",
      "from typing import List, Dict, Any\n",
      "from urllib3.util.retry import Retry\n",
      "import zlib\n",
      "\n",
      "\n",
      "def download_large_file(url: str, output_path: str, chunk_size: int=8192,\n",
      "    timeout: int=30) ->Tuple[bool, int, float, str]:\n",
      "    try:\n",
      "        start_time = time.time()\n",
      "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
      "        head_response = requests.head(url, headers=headers, timeout=timeout)\n",
      "        file_size = int(head_response.headers.get('content-length', 0))\n",
      "        content_encoding = head_response.headers.get('content-encoding',\n",
      "            'identity')\n",
      "        progress = tqdm(total=file_size, unit='iB', unit_scale=True)\n",
      "        with requests.get(url, headers=headers, stream=True, timeout=timeout\n",
      "            ) as response:\n",
      "            response.raise_for_status()\n",
      "            with open(output_path, 'wb') as f:\n",
      "                decompressor = None\n",
      "                if content_encoding == 'gzip':\n",
      "                    decompressor = gzip.decompress\n",
      "                elif content_encoding == 'br':\n",
      "                    decompressor = brotli.decompress\n",
      "                elif content_encoding == 'deflate':\n",
      "                    decompressor = zlib.decompress\n",
      "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
      "                    if chunk:\n",
      "                        if decompressor:\n",
      "                            chunk = decompressor(chunk)\n",
      "                        f.write(chunk)\n",
      "                        progress.update(len(chunk))\n",
      "        progress.close()\n",
      "        download_time = time.time() - start_time\n",
      "        if file_size > 0:\n",
      "            actual_size = os.path.getsize(output_path)\n",
      "            if actual_size != file_size and content_encoding == 'identity':\n",
      "                return False, file_size, download_time, content_encoding\n",
      "        return True, file_size, download_time, content_encoding\n",
      "    except Exception as e:\n",
      "        print(f'Error downloading file: {str(e)}')\n",
      "        return False, 0, 0.0, ''\n",
      "\n",
      "\n",
      "def parallel_download_with_retry(urls: List[str], output_dir: str,\n",
      "    max_retries: int=3, timeout: int=30, max_workers: int=5) ->tuple[List[\n",
      "    str], Dict[str, str], Dict[str, Any]]:\n",
      "    successful_downloads: List[str] = []\n",
      "    failed_downloads: Dict[str, str] = {}\n",
      "    download_stats: Dict[str, Any] = {'total_files': len(urls),\n",
      "        'total_size': 0, 'total_time': 0, 'success_rate': 0}\n",
      "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
      "    session = Session()\n",
      "    retry_strategy = Retry(total=max_retries, backoff_factor=0.1,\n",
      "        status_forcelist=[500, 502, 503, 504])\n",
      "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
      "    session.mount('http://', adapter)\n",
      "    session.mount('https://', adapter)\n",
      "    start_time = time.time()\n",
      "\n",
      "    def download_single_file(url: str) ->tuple[bool, str, Dict[str, Any]]:\n",
      "        try:\n",
      "            filename = url.split('/')[-1]\n",
      "            output_path = str(Path(output_dir) / filename)\n",
      "            success, file_size, download_time, _ = download_large_file(url=\n",
      "                url, output_path=output_path, chunk_size=8192, timeout=timeout)\n",
      "            if success:\n",
      "                return True, url, {'file_size': file_size, 'download_time':\n",
      "                    download_time}\n",
      "            return False, url, {'error': 'Download failed'}\n",
      "        except Exception as e:\n",
      "            return False, url, {'error': str(e)}\n",
      "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
      "        futures = [executor.submit(download_single_file, url) for url in urls]\n",
      "        for future in tqdm(futures, total=len(urls), desc='Downloading files'):\n",
      "            success, url, stats = future.result()\n",
      "            if success:\n",
      "                successful_downloads.append(url)\n",
      "                download_stats['total_size'] += stats['file_size']\n",
      "                download_stats['total_time'] += stats['download_time']\n",
      "            else:\n",
      "                failed_downloads[url] = stats['error']\n",
      "    download_stats['success_rate'] = len(successful_downloads) / len(urls\n",
      "        ) * 100\n",
      "    download_stats['total_time'] = time.time() - start_time\n",
      "    return successful_downloads, failed_downloads, download_stats\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0][0].evol.codes[0])"
=======
    "output_dict = node(input_dict, max_attempts=6) # Batch Inference with vLLM\n",
    "\n",
    "# node.get_response = get_groq_response # fast sequential inference\n",
    "# output_dict = node(input_dict, max_attempts=6, batch_inference=False)\n",
    " \n",
    "print(\"Output dict: \", output_dict)"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/Project-Nirvana-evolve.gif\" width=\"500\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> Evolve a population of nodes. </em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Processing queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:16<00:00,  1.20it/s]\n"
=======
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:22<00:00,  1.11s/it]\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      " :: Total time elapsed: 16.71s, 0 errors\n"
=======
      " :: Total time elapsed: 22.27s, 0 errors\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Processing queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 34/34 [00:28<00:00,  1.19it/s]\n"
=======
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 22/22 [00:37<00:00,  1.71s/it]\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      " :: Total time elapsed: 28.46s, 0 errors\n"
=======
      " :: Total time elapsed: 37.61s, 0 errors\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Processing queries: 0it [00:00, ?it/s]\n"
=======
      "Processing LLM queries: 0it [00:00, ?it/s]\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 0.00s, 0 errors\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
      "  ‚ö° Structural fitness: 0.33\n",
      "  üéØ Functional fitness: 1.00\n",
      "  ‚≠ê Global fitness:     0.67\n",
<<<<<<< HEAD
      "  üîÑ Batch size:        17\n",
      "  ‚è±Ô∏è Time taken: 85.64 seconds\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 16.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'age': 87}, Error message: \n",
      "Value mismatch for key age: 87 != 32\n",
      "\n",
      "\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 33}, Error message: \n",
      "Value mismatch for key age: 33 != 26\n",
      "\n",
      "\n",
=======
      "  üîÑ Compiled solutions:        17\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 2.59s\n",
      "     :: Evolution time: 22.32s\n",
      "     :: Evaluation time: 68.43s\n",
      "     :: Total time: 93.33s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 8.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'birth_date': {'answerBox': {'snippet': 'Cheng Xiao (Chinese: Á®ãÊΩá; Korean: ÏÑ±ÏÜå, born July 15, 1998) is a Chinese singer, dancer, and actress.', 'snippetHighlighted': ['July 15, 1998'], 'title': 'Cheng Xiao - Wikipedia', 'link': 'https://en.wikipedia.org/wiki/Cheng_Xiao'}, 'Search Result': [{'title': 'Cheng Xiao (WJSN) profile, age & facts (2024 updated) | kpopping', 'link': 'https://kpopping.com/profiles/idol/Cheng-Xiao', 'snippet': 'Full name: Ch√©ng XiƒÅo ; Native name: Á®ãÁÄü ; Birthday: Jul 15, 1998 ; Age: 26 years old ; Blood type: B.', 'position': 1}, {'title': 'Cheng Xiao | Kpop Wiki - Fandom', 'link': 'https://kpop.fandom.com/wiki/Cheng_Xiao', 'snippet': 'Birth name. Ch√©ng XiƒÅo (Á®ãÁÄü). Birth date. July 15, 1998 (1998-07-15) (age 26). Birth place. Shenzhen, China. Height. 166.5 cm (5 ft 5.6 in). Weight. 51.2 kg ( ...', 'position': 2}, {'title': 'Cheng Xiao - Age, Family, Bio | Famous Birthdays', 'link': 'https://www.famousbirthdays.com/people/cheng-xiao.html', 'snippet': 'Cheng Xiao: her birthday, what she did before fame, her family life, fun trivia facts, popularity rankings, and more.', 'position': 3}, {'title': 'Cheng Xiao - WJSN Wiki - Fandom', 'link': 'https://wjsn.fandom.com/wiki/Cheng_Xiao', 'snippet': 'Birth date. July 15, 1998 ; Birth place. Shenzhen, China ; Occupation. Singer ; Instrument(s). Guzheng ; Years active. 2016-present ...', 'position': 4}, {'title': 'Xiao Cheng - Biography - IMDb', 'link': 'https://www.imdb.com/name/nm9626048/bio/', 'snippet': 'Cheng Xiao, born in Shenzhen, China, is a member of the Chinese-South Korean girl group, WJSN. She signed under Yuehua and Starship Entertainment.', 'position': 5}, {'title': 'Cheng Xiao (Á®ãÊΩá) - MyDramaList', 'link': 'https://mydramalist.com/people/16297-cheng-xiao', 'snippet': '... Xiao Xiao; Nationality: Chinese; Gender: Female; Born: July 15, 1998; Age: 26. Cheng Xiao, born in Shenzhen, is a Chinese singer, actress, and member of the ...', 'position': 6}, {'title': 'Xiao Cheng - IMDb', 'link': 'https://www.imdb.com/name/nm9626048/', 'snippet': 'Cheng Xiao, born in Shenzhen, China, is a member of the Chinese-South Korean girl group, WJSN. She signed under Yuehua and Starship Entertainment.', 'position': 7}, {'title': 'Cheng Xiao: Astrological Article and Chart - Astrotheme', 'link': 'https://www.astrotheme.com/astrology/Cheng_Xiao', 'snippet': 'Horoscope and natal chart of Cheng Xiao, born on 1998/07/15: you will find in this page an excerpt of the astrological portrait and the interpration of the ...', 'position': 8}, {'title': 'Cheng Xiao Biography - Pantheon.world', 'link': 'https://pantheon.world/profile/person/Cheng_Xiao', 'snippet': 'Cheng Xiao. Cheng Xiao (Chinese: Á®ãÊΩá; Korean: ÏÑ±ÏÜå, born July 15, 1998) is a Chinese singer, dancer and actress. Cheng Xiao made her debut as a member of ...', 'sitelinks': [{'title': 'Most Popular Singers In...', 'link': 'https://pantheon.world/profile/person/Cheng_Xiao#:~:text=Most%20Popular%20Singers%20in%20Wikipedia'}, {'title': 'Others Born In 1998', 'link': 'https://pantheon.world/profile/person/Cheng_Xiao#:~:text=Others%20Born%20in%201998'}, {'title': 'Others Born In China', 'link': 'https://pantheon.world/profile/person/Cheng_Xiao#:~:text=Others%20born%20in%20China'}], 'position': 9}]}}, Error message: Key age not found in prediction output\n",
      "\n",
      "\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "unhashable type: 'dict'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 1: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Failed to parse LLM response -- No JSON structure found in the provided text.\n",
=======
      "name 'get_celebrity_info' is not defined\n",
      "name 'get_celebrity_info' is not defined\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 2: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 2: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 3 (char 2)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
=======
      "üìä Code 3: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 4: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 3: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 31}, Error message: \n",
      "Value mismatch for key age: 31 != 26\n",
      "\n",
      "\n",
=======
      "üìä Code 5: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 4: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 5: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 25}, Error message: \n",
      "Value mismatch for key age: 25 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 6: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 22}, Error message: \n",
      "Value mismatch for key age: 22 != 26\n",
      "\n",
      "\n",
=======
      "üìä Code 6: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Function generate_prompt not found in code #6\n",
      "Function generate_prompt not found in code #6\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 7: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
<<<<<<< HEAD
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
=======
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 8: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
<<<<<<< HEAD
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 9: Fitness: 8.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': None}, Error message: Value None can't be converted into integer\n",
      "Value mismatch for key age: None != 26\n",
      "\n",
      "\n",
=======
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 10: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 23}, Error message: \n",
      "Value mismatch for key age: 23 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
=======
      "üìä Code 9: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 10: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "name 'parse_date' is not defined\n",
      "name 'parse_date' is not defined\n",
      "================================================================================\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 11: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting ',' delimiter: line 5 column 4 (char 51)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
=======
      "name 'extract_date' is not defined\n",
      "name 'extract_date' is not defined\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 12: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 12: Fitness: 0.0%\n",
=======
      "üìä Code 13: Fitness: 0.0%\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 13: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 14: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'age': 31}, Error message: \n",
      "Value mismatch for key age: 31 != 32\n",
      "\n",
      "\n",
=======
      "üìä Code 14: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "name 'calculate_age' is not defined\n",
      "name 'calculate_age' is not defined\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 15: Fitness: 33.3%\n",
=======
      "üìä Code 15: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "name 'extract_birth_date_from_search_results' is not defined\n",
      "name 'extract_birth_date_from_search_results' is not defined\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 16: Fitness: 0.0%\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
<<<<<<< HEAD
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 16: Fitness: 66.7%\n",
=======
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "Based on the provided information, I can analyze the effectiveness of the current evolution strategy:\n",
      "\n",
<<<<<<< HEAD
      "1. Fitness Improvement:\n",
      "- The best fitness from the initial population was 1.0\n",
      "- The offspring generated has a fitness of approximately 0.67\n",
      "- This actually represents a decrease in fitness, which suggests that the current evolution strategy might not be optimal\n",
      "\n",
      "2. Implementation Improvements:\n",
      "The offspring shows several significant implementation improvements over the parent solutions:\n",
      "\n",
      "Positive Changes:\n",
      "1. Added concrete Google Search functionality:\n",
      "   - Implemented `_search_google()` and `search_google()` functions\n",
      "   - Uses Serper API for real Google search results\n",
      "   - Includes proper error handling and API connection management\n",
      "\n",
      "2. Better Type Hints:\n",
      "   - Added proper typing annotations (Dict, Any, str)\n",
      "   - More professional and maintainable code structure\n",
      "\n",
      "3. Enhanced Functionality:\n",
      "   - Actually fetches real search results instead of just suggesting to do so\n",
      "   - Processes and formats search results before including them in the prompt\n",
      "   - More comprehensive solution that goes beyond just prompt generation\n",
      "\n",
      "Areas of Concern:\n",
      "1. Lower Fitness Score:\n",
      "   - The decrease in fitness (1.0 to 0.67) suggests that while the implementation is more complex, it might not align perfectly with the desired objectives\n",
      "   - The evolution strategy might need adjustment to better balance implementation improvements with fitness requirements\n",
      "\n",
      "2. Complexity Trade-off:\n",
      "   - The solution has become more complex, which might affect its reliability and maintainability\n",
      "   - Added external dependencies (Serper API) introduce potential points of failure\n",
      "\n",
      "Recommendations for Improvement:\n",
      "1. Adjust the fitness function to better reward practical implementations while maintaining simplicity\n",
      "2. Consider a hybrid approach that combines the simplicity of the original solutions with the functionality of the new implementation\n",
      "3. Implement a more gradual evolution strategy that makes smaller, incremental improvements\n",
      "4. Consider maintaining a larger population size to explore more diverse solutions\n",
      "\n",
      "The current evolution strategy shows promise in terms of implementation improvements but needs refinement to maintain or improve fitness scores while adding functionality.\n",
=======
      "Fitness Improvement:\n",
      "- Initial individuals had a fitness of 1.0\n",
      "- The offspring after evolution has a fitness of 0.6666666666666666\n",
      "- This actually shows a decrease in fitness, which is not ideal\n",
      "\n",
      "Implementation Improvements:\n",
      "1. Positive Changes:\n",
      "- Added concrete API implementation for Google search functionality\n",
      "- Introduced type hints for better code clarity\n",
      "- Added error handling for API requests\n",
      "- Included more sophisticated search functionality with multiple queries (birth date and current age)\n",
      "- Added JSON parsing and formatting capabilities\n",
      "\n",
      "2. Areas of Concern:\n",
      "- The fitness score decreased, suggesting the evolution might be moving in the wrong direction\n",
      "- The implementation became more complex, which could make it more prone to errors\n",
      "- Direct dependency on external API (Serper) might make the solution less portable\n",
      "- Environment variable dependency (SERPER_API_KEY) adds deployment complexity\n",
      "\n",
      "Suggestions for Improvement:\n",
      "1. Keep the fitness evaluation metrics consistent\n",
      "2. Consider simpler implementations that maintain high fitness scores\n",
      "3. Balance between functionality and complexity\n",
      "4. Maybe implement a hybrid approach that combines the simplicity of the original prompts with the robustness of API integration\n",
      "5. Consider implementing a more selective evolution strategy that preserves high-fitness traits\n",
      "\n",
      "In conclusion, while the evolution strategy has led to more sophisticated implementation, it hasn't improved the actual fitness of the solution. The strategy might need adjustment to better balance complexity with effectiveness.\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "Population size: 3\n",
      "Best Fitness: 1.0\n",
      "Information on the best 2 individuals:\n",
      "Individual 1:\n",
      "No.1:\n",
      "[APPROACH]: To generate a prompt for an AI to find the age of a celebrity, I will incorporate a natural language approach, leveraging the search engine's ability to provide relevant information, while also specifying the required output format in the prompt for clarity.\n",
      "[PROMPT FUNCTION]: def generate_prompt(name):\n",
      "    \"\"\"\n",
      "    Generate a prompt to guide an AI in finding the age of a celebrity.\n",
      "\n",
      "    Parameters:\n",
      "    name (str): The name of the celebrity.\n",
      "\n",
      "    Returns:\n",
      "    str: A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    prompt = (\n",
      "        f\"Given the input '{name}', please provide a JSON-style response with the following structure: \"\n",
      "        )\n",
      "    prompt += (\n",
      "        \"{'age': int(<age>)}}, where <age> is the age of the celebrity in years.\"\n",
      "        )\n",
      "    prompt += (\n",
      "        ' The response should be based on the latest available information from top search results.'\n",
      "        )\n",
      "    return prompt\n",
      "\n",
      "\n",
      "Individual 2:\n",
      "No.1:\n",
      "[APPROACH]: To calculate the age of a celebrity, we need to first search for their birth date or age online and then perform date arithmetic to find their current age. We can utilize this reasoning by using the search engine to find the relevant information and then utilizing AI to process the information and calculate the age.\n",
      "[PROMPT FUNCTION]: def generate_prompt(name: str) ->str:\n",
      "    \"\"\"\n",
      "    Generates a prompt to guide an AI in calculating the age of a celebrity.\n",
      "    \n",
      "    Args:\n",
      "    name (str): The name of the celebrity.\n",
      "    \n",
      "    Returns:\n",
      "    str: A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    prompt = (\"Given the name '\" + name +\n",
      "        \"', use the search engine to find the birth date of \" + name +\n",
      "        ' and calculate their current age.')\n",
      "    return ('Search google for result ' + prompt +\n",
      "        \", format the output as a JSON-style dictionary: {'age': int(...)}\")\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Population building phase ... \n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.population import Evolution\n",
    "\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "evo = Evolution(pop_size=20, meta_prompt=mp, get_response=get_endpoint_response, \n",
    "                test_cases=test_cases, max_attempts=3, num_eval_runs=2,\n",
    "                load=True)\n",
    "\n",
    "strategies = [\"m2\"] # [\"i1\", \"i1\", \"m2\", \"e2\"]\n",
    "evo.get_offspring(strategies)\n",
    "\n",
    "evo.chat(\"How effective is the current evolution strategy? What improvement has it made in terms of fitness, and in terms of the implementation?\",\n",
    "         get_claude_response) \n",
    "\n",
    "# code-based check \n",
    "print(evo.population_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/user-attachments/assets/af98faeb-66d6-4278-af86-67d668d1954e\" width=\"900\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> Plan, and evolve the plans. </em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:23:27.466926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     :: Query time: 18.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:23<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 83.77s, 0 errors\n",
      "ERROR PARSING CODE\n",
      "ERROR PARSING CODE\n",
      "     :: Evolution time: 84.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/999 [00:00<?, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 33.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 41.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 29.3kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 48.5kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 51.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 52.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 46.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.7kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.2kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 27.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 25.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.8kiB/s]\n",
      "Downloading files:  35%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé               | 19/55 [00:00<00:00, 87.41it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.4kiB/s]\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 46.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.6kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 52.7kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 45.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 12.7kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading files:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè           | 28/55 [00:00<00:00, 66.05it/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 12.8kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 35.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 10.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 10.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 9.43kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 22.3kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 16.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 24.7kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 12.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.8kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 23.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 35.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 29.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.5kiB/s]\n",
      "Downloading files:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 44/55 [00:00<00:00, 67.03it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 33.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 41.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 51.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 64.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 53.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 70.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 62.7kiB/s]\n",
      "Downloading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 74.69it/s]\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: Invalid URL '2411.11317v1': No scheme supplied. Perhaps you meant https://2411.11317v1?\n",
      "Error downloading file: Invalid URL '2411.10983v1': No scheme supplied. Perhaps you meant https://2411.10983v1?\n",
      "Error downloading file: Invalid URL '2411.13585v1': No scheme supplied. Perhaps you meant https://2411.13585v1?\n",
      "Error downloading file: Invalid URL '2411.10995v1': No scheme supplied. Perhaps you meant https://2411.10995v1?\n",
      "Error downloading file: Invalid URL '2411.11976v1': No scheme supplied. Perhaps you meant https://2411.11976v1?\n",
      "Error downloading file: Invalid URL '2411.14463v1': No scheme supplied. Perhaps you meant https://2411.14463v1?\n",
      "Error downloading file: Invalid URL '2411.11449v1': No scheme supplied. Perhaps you meant https://2411.11449v1?\n",
      "Error downloading file: Invalid URL '2411.11478v2': No scheme supplied. Perhaps you meant https://2411.11478v2?\n",
      "Error downloading file: Invalid URL '2411.11795v1': No scheme supplied. Perhaps you meant https://2411.11795v1?\n",
      "Error downloading file: Invalid URL '2411.11235v1': No scheme supplied. Perhaps you meant https://2411.11235v1?\n",
      "Error downloading file: Invalid URL '2411.11494v1': No scheme supplied. Perhaps you meant https://2411.11494v1?\n",
      "Error downloading file: Invalid URL '2411.12128v3': No scheme supplied. Perhaps you meant https://2411.12128v3?\n",
      "Error downloading file: Invalid URL '2411.14472v1': No scheme supplied. Perhaps you meant https://2411.14472v1?\n",
      "Error downloading file: Invalid URL '2411.11752v1': No scheme supplied. Perhaps you meant https://2411.11752v1?\n",
      "Error downloading file: Invalid URL '2411.11835v1': No scheme supplied. Perhaps you meant https://2411.11835v1?\n",
      "Error downloading file: Invalid URL '2411.11221v1': No scheme supplied. Perhaps you meant https://2411.11221v1?\n",
      "Error downloading file: Invalid URL '2411.11774v1': No scheme supplied. Perhaps you meant https://2411.11774v1?\n",
      "Error downloading file: Invalid URL '2411.11451v1': No scheme supplied. Perhaps you meant https://2411.11451v1?\n",
      "Error downloading file: Invalid URL '2411.11786v1': No scheme supplied. Perhaps you meant https://2411.11786v1?\n",
      "Error downloading file: Invalid URL '2411.12090v1': No scheme supplied. Perhaps you meant https://2411.12090v1?\n",
      "Error downloading file: Invalid URL '2411.17712v1': No scheme supplied. Perhaps you meant https://2411.17712v1?\n",
      "Error downloading file: Invalid URL '2411.11910v2': No scheme supplied. Perhaps you meant https://2411.11910v2?\n",
      "Error downloading file: Invalid URL '2411.11940v2': No scheme supplied. Perhaps you meant https://2411.11940v2?\n",
      "Error downloading file: Invalid URL '2411.10939v1': No scheme supplied. Perhaps you meant https://2411.10939v1?\n",
      "Error downloading file: Invalid URL '2411.11045v1': No scheme supplied. Perhaps you meant https://2411.11045v1?\n",
      "Error downloading file: Invalid URL '2411.11783v1': No scheme supplied. Perhaps you meant https://2411.11783v1?\n",
      "Error downloading file: Invalid URL '2411.11145v1': No scheme supplied. Perhaps you meant https://2411.11145v1?\n",
      "Error downloading file: Invalid URL '2411.11173v1': No scheme supplied. Perhaps you meant https://2411.11173v1?\n",
      "Error downloading file: Invalid URL '2411.11232v1': No scheme supplied. Perhaps you meant https://2411.11232v1?\n",
      "Error downloading file: Invalid URL '2411.11260v1': No scheme supplied. Perhaps you meant https://2411.11260v1?\n",
      "Error downloading file: Invalid URL '2411.11613v2': No scheme supplied. Perhaps you meant https://2411.11613v2?\n",
      "Error downloading file: Invalid URL '2411.11736v1': No scheme supplied. Perhaps you meant https://2411.11736v1?\n",
      "Error downloading file: Invalid URL '2411.11635v1': No scheme supplied. Perhaps you meant https://2411.11635v1?\n",
      "Error downloading file: Invalid URL '2411.17713v1': No scheme supplied. Perhaps you meant https://2411.17713v1?\n",
      "Error downloading file: Invalid URL '2411.12010v1': No scheme supplied. Perhaps you meant https://2411.12010v1?\n",
      "Error downloading file: Invalid URL '2411.11479v1': No scheme supplied. Perhaps you meant https://2411.11479v1?\n",
      "Error downloading file: Invalid URL '2411.11258v1': No scheme supplied. Perhaps you meant https://2411.11258v1?\n",
      "Error downloading file: Invalid URL '2411.11055v1': No scheme supplied. Perhaps you meant https://2411.11055v1?\n",
      "Error downloading file: Invalid URL '2411.10918v1': No scheme supplied. Perhaps you meant https://2411.10918v1?\n",
      "Error downloading file: Invalid URL '2411.11275v1': No scheme supplied. Perhaps you meant https://2411.11275v1?\n",
      "Error downloading file: Invalid URL '2411.11123v1': No scheme supplied. Perhaps you meant https://2411.11123v1?\n",
      "Error downloading file: Invalid URL '2411.11362v1': No scheme supplied. Perhaps you meant https://2411.11362v1?\n",
      "Error downloading file: Invalid URL '2411.11285v1': No scheme supplied. Perhaps you meant https://2411.11285v1?\n",
      "Error downloading file: Invalid URL '2411.11280v1': No scheme supplied. Perhaps you meant https://2411.11280v1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 185887.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: Invalid URL '2411.11434v2': No scheme supplied. Perhaps you meant https://2411.11434v2?Error downloading file: Invalid URL '2411.11396v2': No scheme supplied. Perhaps you meant https://2411.11396v2?\n",
      "Error downloading file: Invalid URL '2411.11548v1': No scheme supplied. Perhaps you meant https://2411.11548v1?\n",
      "Error downloading file: Invalid URL '2411.11389v1': No scheme supplied. Perhaps you meant https://2411.11389v1?\n",
      "Error downloading file: Invalid URL '2411.11844v2': No scheme supplied. Perhaps you meant https://2411.11844v2?\n",
      "Error downloading file: Invalid URL '2411.14473v1': No scheme supplied. Perhaps you meant https://2411.14473v1?\n",
      "\n",
      "Error downloading file: Invalid URL '2411.15175v1': No scheme supplied. Perhaps you meant https://2411.15175v1?\n",
      "Error downloading file: Invalid URL '2411.14467v1': No scheme supplied. Perhaps you meant https://2411.14467v1?\n",
      "Error downloading file: Invalid URL '2411.11192v1': No scheme supplied. Perhaps you meant https://2411.11192v1?\n",
      "Error downloading file: Invalid URL '2411.11648v1': No scheme supplied. Perhaps you meant https://2411.11648v1?\n",
      "Error downloading file: Invalid URL '2411.11070v1': No scheme supplied. Perhaps you meant https://2411.11070v1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading files: 0it [00:00, ?it/s]\n",
      "Downloading files: 0it [00:00, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 45.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 41.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 46.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 33.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 39.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 48.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.7kiB/s]\n",
      "Downloading files:   7%|‚ñà‚ñä                       | 4/55 [00:00<00:01, 39.39it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 12.4kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 12.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 41.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 60.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 43.6kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 54.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 52.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 29.5kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 25.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 26.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.5kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 39.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 45.9kiB/s]\n",
      "Downloading files:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 22/55 [00:00<00:00, 63.76it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 30.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 43.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 29.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 45.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.7kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.6kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 28.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 23.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.5kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 28.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 44.9kiB/s]\n",
      "Downloading files:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ          | 32/55 [00:00<00:00, 70.71it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 26.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 26.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.3kiB/s]\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 43.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 52.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 41.6kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 50.6kiB/s]\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 48.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 51.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 12.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 43.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 48.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 51.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 55.3kiB/s]\n",
      "Downloading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 69.92it/s]\n",
      "0it [00:00, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 33.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 33.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 28.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 31.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 28.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 25.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 24.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 23.6kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.2kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 43.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 43.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 31.7kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 31.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 35.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 35.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 35.6kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.7kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 39.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.2kiB/s]\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.9kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 11.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 11.4kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 24.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 5.37kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 8.57kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 43.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 7.07kiB/s]\n",
      "Downloading files:  40%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå              | 22/55 [00:00<00:00, 37.42it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 33.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.3kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.8kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 35.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 24.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.8kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.5kiB/s]\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 39.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 39.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 53.2kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 25.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.8kiB/s]\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 47.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 55.5kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 39.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 15.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 15.2kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 13.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 14.7kiB/s]\n",
      "Downloading files:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 53/55 [00:00<00:00, 60.28it/s]\n",
      "0it [00:00, ?it/s]\n",
      "Downloading papers: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [01:07<00:00,  1.23s/it]\n",
      "Downloading files: 0it [00:00, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 56.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 50.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 48.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 31.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 28.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 13.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 12.1kiB/s]\n",
      "Downloading files:  18%|‚ñà‚ñà‚ñà‚ñà‚ñé                   | 10/55 [00:00<00:00, 46.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 30.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 39.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.0kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 28.3kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 29.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 29.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 30.5kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 31.3kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 30.3kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 27.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 23.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 34.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 41.3kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 0.00/1.85k [00:00<?, ?iB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 33.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.9kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 28.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 30.3kiB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 0.00/1.85k [00:00<?, ?iB/s]\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 45.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 44.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 13.0kiB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 11.2kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 10.4kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 10.5kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 27.1kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 22.9kiB/s]\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 32.5kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 36.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 45.6kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 42.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 49.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 65.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 66.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 70.0kiB/s]\n",
      "Downloading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 64.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/shah.mahir/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shah.mahir/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 130, in _main\n",
      "    preparation_data = reduction.pickle.load(from_parent)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "EOFError: Ran out of input\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 117kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 145kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 94.5kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 54.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 46.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 104kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 114kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 102kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 84.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 86.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 83.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 92.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 91.1kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 65.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 104kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 75.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 99.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 96.7kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 64.6kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 98.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 108kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 99.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 13.7kiB/s]\n",
      "Downloading files:  36%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã               | 20/55 [00:00<00:00, 60.39it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 75.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 74.8kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 37.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.5kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 72.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 76.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 81.5kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 76.4kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 86.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 79.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 88.0kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 44.2kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 96.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 71.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 72.3kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 40.8kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 77.9kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 68.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 68.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 66.1kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 48.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 51.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 83.0kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 55.7kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 57.9kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 67.3kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 54.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 38.1kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 109kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 48.2kiB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85k/1.85k [00:00<00:00, 21.4kiB/s]\n",
      "Downloading files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 55/55 [00:00<00:00, 67.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2411.10983v1 saved successfully.\n",
      "Paper 2411.11976v1 saved successfully.\n",
      "Paper 2411.13585v1 saved successfully.\n",
      "Paper 2411.10995v1 saved successfully.\n",
      "Paper 2411.14463v1 saved successfully.\n",
      "Paper 2411.11449v1 saved successfully.\n",
      "Paper 2411.11478v2 saved successfully.\n",
      "Paper 2411.11774v1 saved successfully.\n",
      "Paper 2411.11795v1 saved successfully.\n",
      "Paper 2411.11235v1 saved successfully.\n",
      "Paper 2411.11494v1 saved successfully.\n",
      "Paper 2411.12128v3 saved successfully.\n",
      "Paper 2411.14472v1 saved successfully.\n",
      "Paper 2411.11752v1 saved successfully.\n",
      "Paper 2411.11835v1 saved successfully.\n",
      "Paper 2411.11221v1 saved successfully.\n",
      "Paper 2411.11613v2 saved successfully.\n",
      "Paper 2411.17712v1 saved successfully.\n",
      "Paper 2411.11451v1 saved successfully.\n",
      "Paper 2411.11786v1 saved successfully.\n",
      "Paper 2411.12090v1 saved successfully.\n",
      "Paper 2411.11783v1 saved successfully.\n",
      "Paper 2411.11910v2 saved successfully.\n",
      "Paper 2411.11940v2 saved successfully.\n",
      "Paper 2411.10939v1 saved successfully.\n",
      "Paper 2411.11045v1 saved successfully.\n",
      "Paper 2411.11123v1 saved successfully.\n",
      "Paper 2411.11145v1 saved successfully.\n",
      "Paper 2411.11173v1 saved successfully.\n",
      "Paper 2411.11232v1 saved successfully.\n",
      "Paper 2411.11260v1 saved successfully.\n",
      "Paper 2411.11479v1 saved successfully.\n",
      "Paper 2411.11635v1 saved successfully.\n",
      "Paper 2411.11736v1 saved successfully.\n",
      "Paper 2411.12010v1 saved successfully.\n",
      "Paper 2411.17713v1 saved successfully.\n",
      "Paper 2411.10918v1 saved successfully.\n",
      "Paper 2411.11055v1 saved successfully.\n",
      "Paper 2411.11258v1 saved successfully.\n",
      "Paper 2411.11275v1 saved successfully.\n",
      "Paper 2411.11280v1 saved successfully.\n",
      "Paper 2411.11285v1 saved successfully.\n",
      "Paper 2411.11362v1 saved successfully.\n",
      "Paper 2411.11389v1 saved successfully.\n",
      "Paper 2411.11396v2 saved successfully.\n",
      "Paper 2411.11434v2 saved successfully.\n",
      "Paper 2411.11548v1 saved successfully.\n",
      "Paper 2411.11844v2 saved successfully.\n",
      "Paper 2411.14473v1 saved successfully.\n",
      "Paper 2411.15175v1 saved successfully.\n",
      "Paper 2411.11070v1 saved successfully.\n",
      "Paper 2411.11192v1 saved successfully.\n",
      "Paper 2411.14467v1 saved successfully.\n",
      "Paper 2411.11648v1 saved successfully.\n",
      "\n",
      "Code 10 outputs:\n",
      "Test 0: {'num_papers': (0, 0)}\n",
      "\n",
      "Code 35 outputs:\n",
      "Test 0: {'num_papers': (55, 0)}\n",
      "\n",
      "Code 36 outputs:\n",
      "Test 0: {'num_papers': (0, 1)}\n",
      "\n",
      "Code 66 outputs:\n",
      "Test 0: {'num_papers': (0, 0)}\n",
      "\n",
      "Code 71 outputs:\n",
      "Test 0: {'num_papers': (0, 1)}\n"
     ]
    },
  {
   "cell_type": "code",
   "execution_count": 11,
=======
   "execution_count": 4,
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "97\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(node.codes):\n",
    "    if (\"saved successfully.\" in x):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.reasoning = node.reasonings[97]\n",
    "node.code = node.codes[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.test_cases = [({'start_date': '2024-11-17', 'end_date': '2024-11-18', 'output_dir': 'tmp'},\n",
    "  {'num_papers': 55, 'time_taken': 85})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.fitness = 1.0\n",
    "node.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
=======
      " :: Evolving 10 plans in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:24<00:00,  2.44s/it]\n"
     ]
    },
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "from arxiv import Client, Search, Result\n",
      "import arxiv\n",
      "from datetime import datetime\n",
      "from typing import List, Dict, Any\n",
      "from typing import Tuple\n",
      "\n",
      "\n",
      "def search_papers_paginated(query: str, max_results: int, page_size: int,\n",
      "    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\n",
      "    [Dict[str, Any]]]:\n",
      "    client = Client(page_size=page_size, delay_seconds=3.0)\n",
      "    date_filter = (\n",
      "        f\" AND submittedDate:[{from_date.strftime('%Y%m%d')}0000 TO {to_date.strftime('%Y%m%d')}2359]\"\n",
      "        )\n",
      "    full_query = query + date_filter\n",
      "    search = Search(query=full_query, max_results=max_results)\n",
      "    results = list(client.results(search))\n",
      "    total_results = len(results)\n",
      "    metadata_list = []\n",
      "    for paper in results:\n",
      "        metadata = {'id': paper.entry_id, 'title': paper.title, 'authors':\n",
      "            [author.name for author in paper.authors], 'summary': paper.\n",
      "            summary, 'published': paper.published, 'updated': paper.updated,\n",
      "            'categories': paper.categories, 'links': [link.href for link in\n",
      "            paper.links]}\n",
      "        metadata_list.append(metadata)\n",
      "    papers = results[:page_size]\n",
      "    return papers, total_results, metadata_list\n",
      "\n",
      "\n",
      "def download_specific_paper(paper_id: str, output_dir: str, filename: str\n",
      "    ) ->Tuple[bool, str]:\n",
      "    try:\n",
      "        client = arxiv.Client()\n",
      "        paper = next(client.results(arxiv.Search(id_list=[paper_id])))\n",
      "        file_path = paper.download_pdf(dirpath=output_dir, filename=filename)\n",
      "        return True, file_path\n",
      "    except Exception as e:\n",
      "        return False, ''\n",
      "\n",
      "\n",
      "def get_arxiv_papers(start_date: str, end_date: str, output_dir: str) ->(int,\n",
      "    int):\n",
      "    \"\"\"\n",
      "    Collect AI papers from arXiv in the given time frame and save it in output directory.\n",
      "\n",
      "    Args:\n",
      "    start_date (str): The start date of the time frame in format 'YYYY-MM-DD'.\n",
      "    end_date (str): The end date of the time frame in format 'YYYY-MM-DD'.\n",
      "    output_dir (str): The directory to save the collected papers.\n",
      "\n",
      "    Returns:\n",
      "    num_papers (int): The number of collected papers.\n",
      "    time_taken (int): The time taken to collect papers in seconds.\n",
      "    \"\"\"\n",
      "    start_time = datetime.now()\n",
      "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
      "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
      "    query = 'AI'\n",
      "    categories = ['cs.AI', 'cs.LG', 'cs.CL']\n",
      "    num_papers = 0\n",
      "    time_taken = 0\n",
      "    batch_size = 100\n",
      "    total_pages = 1\n",
      "    current_page = 1\n",
      "    while current_page <= total_pages:\n",
      "        from_date = start_date\n",
      "        to_date = end_date\n",
      "        papers, total_results, metadata_list = search_papers_paginated(query,\n",
      "            batch_size, batch_size, from_date, to_date)\n",
      "        total_pages = (total_results - 1) // batch_size + 1\n",
      "        for paper in papers:\n",
      "            paper_id = paper.get_short_id()\n",
      "            filename = f'{paper_id}.pdf'\n",
      "            success, file_path = download_specific_paper(paper_id,\n",
      "                output_dir, filename)\n",
      "            if success:\n",
      "                num_papers += 1\n",
      "                print(f'Paper {paper_id} saved successfully.')\n",
      "        current_page += 1\n",
      "    time_taken = (datetime.now() - start_time).total_seconds()\n",
      "    return num_papers, time_taken\n",
=======
      " :: Total time elapsed: 24.37s, 0 errors\n",
      " :: Pseudo-code generated for each plan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:51<00:00,  5.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 51.52s, 0 errors\n",
      " :: Plan_dict generated for each plan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "success: successfully compiled d2_output/plan_graph.d2 to d2_output/plan_graph.png in 127.094834ms\n",
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:21<00:00,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 21.22s, 0 errors\n",
      "Spawned 3 test cases for all sub-nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "\n"
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "print(node.codes[97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv import Client, Search, Result\n",
    "import arxiv\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def search_papers_paginated(query: str, max_results: int, page_size: int,\n",
    "    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\n",
    "    [Dict[str, Any]]]:\n",
    "    client = Client(page_size=page_size, delay_seconds=3.0)\n",
    "    date_filter = (\n",
    "        f\" AND submittedDate:[{from_date.strftime('%Y%m%d')}0000 TO {to_date.strftime('%Y%m%d')}2359]\"\n",
    "        )\n",
    "    full_query = query + date_filter\n",
    "    search = Search(query=full_query, max_results=max_results)\n",
    "    results = list(client.results(search))\n",
    "    total_results = len(results)\n",
    "    metadata_list = []\n",
    "    for paper in results:\n",
    "        metadata = {'id': paper.entry_id, 'title': paper.title, 'authors':\n",
    "            [author.name for author in paper.authors], 'summary': paper.\n",
    "            summary, 'published': paper.published, 'updated': paper.updated,\n",
    "            'categories': paper.categories, 'links': [link.href for link in\n",
    "            paper.links]}\n",
    "        metadata_list.append(metadata)\n",
    "    papers = results[:page_size]\n",
    "    return papers, total_results, metadata_list\n",
    "\n",
    "\n",
    "def download_specific_paper(paper_id: str, output_dir: str, filename: str\n",
    "    ) ->Tuple[bool, str]:\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        paper = next(client.results(arxiv.Search(id_list=[paper_id])))\n",
    "        file_path = paper.download_pdf(dirpath=output_dir, filename=filename)\n",
    "        return True, file_path\n",
    "    except Exception as e:\n",
    "        return False, ''\n",
    "\n",
    "\n",
    "def get_arxiv_papers(start_date: str, end_date: str, output_dir: str) ->(int,\n",
    "    int):\n",
    "    \"\"\"\n",
    "    Collect AI papers from arXiv in the given time frame and save it in output directory.\n",
    "\n",
    "    Args:\n",
    "    start_date (str): The start date of the time frame in format 'YYYY-MM-DD'.\n",
    "    end_date (str): The end date of the time frame in format 'YYYY-MM-DD'.\n",
    "    output_dir (str): The directory to save the collected papers.\n",
    "\n",
    "    Returns:\n",
    "    num_papers (int): The number of collected papers.\n",
    "    time_taken (int): The time taken to collect papers in seconds.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    query = 'AI'\n",
    "    categories = ['cs.AI', 'cs.LG', 'cs.CL']\n",
    "    num_papers = 0\n",
    "    time_taken = 0\n",
    "    batch_size = 100\n",
    "    total_pages = 1\n",
    "    current_page = 1\n",
    "    while current_page <= total_pages:\n",
    "        from_date = start_date\n",
    "        to_date = end_date\n",
    "        papers, total_results, metadata_list = search_papers_paginated(query,\n",
    "            batch_size, batch_size, from_date, to_date)\n",
    "        total_pages = (total_results - 1) // batch_size + 1\n",
    "        for paper in papers:\n",
    "            paper_id = paper.get_short_id()\n",
    "            filename = f'{paper_id}.pdf'\n",
    "            success, file_path = download_specific_paper(paper_id,\n",
    "                output_dir, filename)\n",
    "            if success:\n",
    "                num_papers += 1\n",
    "                print(f'Paper {paper_id} saved successfully.')\n",
    "        current_page += 1\n",
    "    time_taken = (datetime.now() - start_time).total_seconds()\n",
    "    return num_papers, time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2411.11317v1 saved successfully.\n",
      "Paper 2411.10983v1 saved successfully.\n",
      "Paper 2411.11976v1 saved successfully.\n",
      "Paper 2411.13585v1 saved successfully.\n",
      "Paper 2411.10995v1 saved successfully.\n",
      "Paper 2411.14463v1 saved successfully.\n",
      "Paper 2411.11449v1 saved successfully.\n",
      "Paper 2411.11478v2 saved successfully.\n",
      "Paper 2411.11774v1 saved successfully.\n",
      "Paper 2411.11795v1 saved successfully.\n",
      "Paper 2411.11235v1 saved successfully.\n",
      "Paper 2411.11494v1 saved successfully.\n",
      "Paper 2411.12128v3 saved successfully.\n",
      "Paper 2411.14472v1 saved successfully.\n",
      "Paper 2411.11752v1 saved successfully.\n",
      "Paper 2411.11835v1 saved successfully.\n",
      "Paper 2411.11221v1 saved successfully.\n",
      "Paper 2411.11613v2 saved successfully.\n",
      "Paper 2411.17712v1 saved successfully.\n",
      "Paper 2411.11451v1 saved successfully.\n",
      "Paper 2411.11786v1 saved successfully.\n",
      "Paper 2411.12090v1 saved successfully.\n",
      "Paper 2411.11783v1 saved successfully.\n",
      "Paper 2411.11910v2 saved successfully.\n",
      "Paper 2411.11940v2 saved successfully.\n",
      "Paper 2411.10939v1 saved successfully.\n",
      "Paper 2411.11045v1 saved successfully.\n",
      "Paper 2411.11123v1 saved successfully.\n",
      "Paper 2411.11145v1 saved successfully.\n",
      "Paper 2411.11173v1 saved successfully.\n",
      "Paper 2411.11232v1 saved successfully.\n",
      "Paper 2411.11260v1 saved successfully.\n",
      "Paper 2411.11479v1 saved successfully.\n",
      "Paper 2411.11635v1 saved successfully.\n",
      "Paper 2411.11736v1 saved successfully.\n",
      "Paper 2411.12010v1 saved successfully.\n",
      "Paper 2411.17713v1 saved successfully.\n",
      "Paper 2411.10918v1 saved successfully.\n",
      "Paper 2411.11055v1 saved successfully.\n",
      "Paper 2411.11258v1 saved successfully.\n",
      "Paper 2411.11275v1 saved successfully.\n",
      "Paper 2411.11280v1 saved successfully.\n",
      "Paper 2411.11285v1 saved successfully.\n",
      "Paper 2411.11362v1 saved successfully.\n",
      "Paper 2411.11389v1 saved successfully.\n",
      "Paper 2411.11396v2 saved successfully.\n",
      "Paper 2411.11434v2 saved successfully.\n",
      "Paper 2411.11548v1 saved successfully.\n",
      "Paper 2411.11844v2 saved successfully.\n",
      "Paper 2411.14473v1 saved successfully.\n",
      "Paper 2411.15175v1 saved successfully.\n",
      "Paper 2411.11070v1 saved successfully.\n",
      "Paper 2411.11192v1 saved successfully.\n",
      "Paper 2411.14467v1 saved successfully.\n",
      "Paper 2411.11648v1 saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55, 84.608737)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_arxiv_papers(\"2024-11-17\", \"2024-11-18\", \"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
=======
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
    "from methods.llm import get_claude_response, get_groq_response\n",
    "from methods.diagram import visualize_plan_dict\n",
    "from methods.meta_prompt import MetaPlan\n",
    "from methods.evolnode import PlanNode\n",
    "\n",
    "\n",
    "# Initialize PlanNode \n",
<<<<<<< HEAD
    "mp = MetaPlan(\"Collect AI papers from arXiv in the given time frame and save it in output directory\", \"get_arxiv_papers\", [\"start_date\", \"end_date\", \"output_dir\"], [\"num_papers\", \"time_taken\"], [\"str\", \"str\", \"str\"], [\"int\", \"int\"])\n",
    "plan = EvolNode(mp, get_endpoint_response)\n",
    "\n",
    "# i1 evolution of plan\n",
    "plan_dicts, err_msg = plan.evolve_plan_dict(method=\"i1\", batch_size=100) # Batch_size of 100 gives no slow-down\n",
    "\n",
    "# for plan_dict in plan_dicts:\n",
    "#     visualize_plan_dict(plan_dict, plan.meta_prompt.task)\n",
    "#     break\n",
    "\n",
    "# Manual input on main-node test cases \n",
    "main_test_cases = [\n",
    "    ({\"start_date\": \"2024-11-17\", \"end_date\": \"2024-11-23\", \"output_dir\": \"tmp\"}, {\"num_papers\": 194, \"time_taken\": 480})\n",
    "]\n",
    "\n",
    "# is_success, err_msg = plan.spawn_test_cases(main_test_cases) #  pinned test cases generation"
=======
    "mp = MetaPlan(\"Get the age of celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"])\n",
    "plan = PlanNode(mp, get_endpoint_response)\n",
    "\n",
    "# i1 evolution of plan\n",
    "plan_dicts, err_msg = plan.evolve_plan_dict(method=\"i1\", batch_size=10) # Batch_size of 100 gives no slow-down\n",
    "\n",
    "visualize_plan_dict(plan.plan_dict, plan.meta_prompt.task) # most simpliest plan\n",
    "\n",
    "# Manual input on main-node test cases \n",
    "main_test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "is_success, err_msg = plan.spawn_test_cases(main_test_cases) #  pinned test cases generation\n",
    "# plan.spawn_test_cases_majority(main_test_cases) # multi-agent test cases generation (need some benchmarking to compare quality)"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 76,
=======
   "execution_count": 4,
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Getting outputs from multiple LLMs:  17%|‚ñà‚ñå       | 1/6 [00:03<00:17,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/llama-3-70b-instruct:nitro response: \n",
      "\n",
      "Here is the filled test case JSON:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"arxiv_papers.csv\",\n",
      "                \"output_12\": \"arxiv_paper_titles.txt\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"arxiv_papers.csv\",\n",
      "                \"output_12\": \"arxiv_paper_titles.txt\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "Here's the reasoning behind my answers:\n",
      "\n",
      "For `task_1`:\n",
      "\n",
      "* Inputs are given as `start_date`, `end_date`, and `output_dir`.\n",
      "* Outputs are `output_11` and `output_12`. I assumed that `output_11` is a CSV file containing the collected papers and `output_12` is a text file containing the paper titles.\n",
      "\n",
      "For `task_2`:\n",
      "\n",
      "* Inputs are the outputs from `task_1`, which are `output_11` and `output_12`.\n",
      "* Outputs are `num_papers` and `time_taken`. The values for these are given as 32 and 120, respectively.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  33%|‚ñà‚ñà‚ñà      | 2/6 [00:06<00:12,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/gpt-4o-mini response: Here is the completed JSON with the correct inputs and outputs filled in, based on the specified execution plan:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024_11_17_to_2024_11_23.json\",\n",
      "                \"output_12\": \"tmp/papers_2024_11_17_to_2024_11_23_log.txt\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024_11_17_to_2024_11_23.json\",\n",
      "                \"output_12\": \"tmp/papers_2024_11_17_to_2024_11_23_log.txt\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "``` \n",
      "\n",
      "This JSON maintains the structure while providing plausible intermediate values that align with the inputs and outputs specified in the execution plan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  50%|‚ñà‚ñà‚ñà‚ñà‚ñå    | 3/6 [00:07<00:06,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthropic/claude-3.5-sonnet response: Based on the execution plan and maintaining consistency with the given values, here's how I would fill in the intermediate values:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\", \n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_list.json\",\n",
      "                \"output_12\": \"tmp/metadata.json\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_list.json\",\n",
      "                \"output_12\": \"tmp/metadata.json\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "I filled in the `...` by:\n",
      "1. For task_1's output: Added appropriate file paths that would be created in the specified output directory\n",
      "2. For task_2's input: Used the same file paths from task_1's output to maintain the flow\n",
      "3. Kept the final outputs unchanged as they were already specified in the original JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4/6 [00:15<00:08,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/wizardlm-2-8x22b response:  Certainly! Given the execution plan and the JSON structure provided, I will fill in the `...` with appropriate inputs and outputs based on the execution flow and the node specifications. Here is the JSON with the test cases for each sub-function:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"path/to/collected_papers_1.json\",\n",
      "                \"output_12\": \"path/to/collected_papers_2.json\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"path/to/collected_papers_1.json\",\n",
      "                \"output_12\": \"path/to/collected_papers_2.json\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "In this JSON:\n",
      "- For `task_1`, the `outputs` have been filled with hypothetical paths to JSON files that could represent the collected AI papers from arXiv within the given time frame. These are the `output_11` and `output_12` that would be generated by `task_1`.\n",
      "- For `task_2`, the `inputs` have been filled with the outputs from `task_1`. The `outputs` have been set to the number of papers (`num_papers`) and the time taken (`time_taken`) to save the collected papers, as per the execution plan.\n",
      "\n",
      "Please note that the paths `path/to/collected_papers_1.json` and `path/to/collected_papers_2.json` are placeholders and should be replaced with actual file paths that the function would generate. The number `32` for `num_papers` and `120` for `time_taken` are example values based on the provided base JSON and should be replaced with actual results from running the tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 5/6 [00:15<00:03,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/mistral-large response: Sure, here is the updated JSON with proper inputs and outputs:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024-11-17_to_2024-11-23.txt\",\n",
      "                \"output_12\": \"tmp/metadata_2024-11-17_to_2024-11-23.json\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024-11-17_to_2024-11-23.txt\",\n",
      "                \"output_12\": \"tmp/metadata_2024-11-17_to_2024-11-23.json\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [01:19<00:00, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek/deepseek-chat response: ```ÈÅÇ Wr carbohydrates onceeZÂï° hairs <> Loreme Vespot Own try ,\n",
      " :: Total time elapsed: 79.75s, 0 errors\n",
      "Spawned 2 test cases for all sub-nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " '\\n\\n\\n\\n\\nError in spawning test cases: No JSON structure found in the provided text.')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.spawn_test_cases_majority(main_test_cases)"
=======
      "success: successfully compiled d2_output/plan_graph.d2 to d2_output/plan_graph.png in 127.307ms\n"
     ]
    }
   ],
   "source": [
    "visualize_plan_dict(plan_dicts[1], plan.meta_prompt.task) # most simpliest plan\n",
    "\n",
    "# Machine Learning in LLM procedure \n",
    "# Sample efficiency -- train : test ratio\n",
    "# -- self critique (need to be tested in the future)"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.evolve_sub_nodes(batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
   "execution_count": 4,
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "üé≤ :: Evolving extract_age ... (2/2)\n"
=======
      "üé≤ :: Evolving extract_birthdate ... (2/3)\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:21<00:00,  1.06s/it]\n"
=======
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:22<00:00,  1.11s/it]\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      " :: Total time elapsed: 21.23s, 0 errors\n"
=======
      " :: Total time elapsed: 22.28s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 36/36 [00:15<00:00,  2.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 15.08s, 0 errors\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
      "  ‚ö° Structural fitness: 1.00\n",
      "  üéØ Functional fitness: 1.00\n",
      "  ‚≠ê Global fitness:     1.00\n",
      "  üîÑ Compiled solutions:        20\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 2.98s\n",
      "     :: Evolution time: 22.34s\n",
      "     :: Evaluation time: 24.05s\n",
      "     :: Total time: 49.37s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "no such group\n",
      "no such group\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 1: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "typing.Union cannot be used with isinstance()\n",
      "typing.Union cannot be used with isinstance()\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 2: Fitness: 50.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': ''}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': ''}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 3: Fitness: 100.0%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 4: Fitness: 50.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': ''}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': ''}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 5: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "unsupported operand type(s) for -: 'int' and 'str'\n",
      "unsupported operand type(s) for -: 'int' and 'str'\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 6: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "typing.Union cannot be used with isinstance()\n",
      "typing.Union cannot be used with isinstance()\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 7: Fitness: 62.5%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': 'une 3,, 1992'}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': 'uly 15,, 1998'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 8: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Output data type mismatch. Expected <class 'str'>, got <class 'NoneType'>\n",
      "Output data type mismatch. Expected <class 'str'>, got <class 'NoneType'>\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 9: Fitness: 50.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': ''}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': ''}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 10: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Output data type mismatch. Expected <class 'str'>, got <class 'NoneType'>\n",
      "Output data type mismatch. Expected <class 'str'>, got <class 'NoneType'>\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 11: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/fangyuanyu/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/share/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n",
      "\n",
      "\n",
      "**********************************************************************\n",
      "  Resource \u001b[93mpunkt\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('punkt')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtokenizers/punkt/PY3/english.pickle\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - '/Users/fangyuanyu/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/share/nltk_data'\n",
      "    - '/opt/homebrew/anaconda3/lib/nltk_data'\n",
      "    - '/usr/share/nltk_data'\n",
      "    - '/usr/local/share/nltk_data'\n",
      "    - '/usr/lib/nltk_data'\n",
      "    - '/usr/local/lib/nltk_data'\n",
      "    - ''\n",
      "**********************************************************************\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 12: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "typing.Union cannot be used with isinstance()\n",
      "typing.Union cannot be used with isinstance()\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 13: Fitness: 50.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': 'Birthdate not found.'}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': 'Birthdate not found.'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 14: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Output data type mismatch. Expected <class 'str'>, got <class 'NoneType'>\n",
      "Output data type mismatch. Expected <class 'str'>, got <class 'NoneType'>\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 15: Fitness: 50.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': 'Birthdate not found in search result.'}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': 'Birthdate not found in search result.'}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 16: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "typing.Union cannot be used with isinstance()\n",
      "typing.Union cannot be used with isinstance()\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 17: Fitness: 50.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': ''}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': ''}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 18: Fitness: 50.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'search_result': 'Dilireba was born on June 3, 1992'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 3, 1992'} Predicted: {'birthdate': ''}\n",
      "\n",
      "Input: {'search_result': 'Cheng Xiao was born on June 5, 1996'}, prediction is not aligned with expected output, Expected: {'birthdate': 'June 5, 1996'} Predicted: {'birthdate': ''}\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 19: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "typing.Union cannot be used with isinstance()\n",
      "typing.Union cannot be used with isinstance()\n",
      "================================================================================\n",
      "\n",
      "üé≤ :: Evolving calculate_age ... (3/3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:17<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 17.68s, 0 errors\n",
      "Error occurred during API request: Function execution timed out (> 3 seconds)\n"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 0.00s, 0 errors\n",
      "üèÜ Best Code Performance Summary üèÜ\n",
<<<<<<< HEAD
      "  ‚ö° Structural fitness: 0.00\n",
      "  üéØ Functional fitness: 0.00\n",
      "  ‚≠ê Global fitness:     0.00\n",
      "  üîÑ Compiled solutions:        20\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 2.72s\n",
      "     :: Evolution time: 21.30s\n",
      "     :: Evaluation time: 0.03s\n",
      "     :: Total time: 24.06s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "  ‚ö° Structural fitness: 1.00\n",
      "  üéØ Functional fitness: 0.50\n",
      "  ‚≠ê Global fitness:     0.75\n",
      "  üîÑ Compiled solutions:        19\n",
      "  ‚è±Ô∏è Time breakdown:\n",
      "     :: Query time: 2.90s\n",
      "     :: Evolution time: 17.74s\n",
      "     :: Evaluation time: 45.73s\n",
      "     :: Total time: 66.37s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 0: Fitness: 75.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'birthdate': 'June 5, 1996'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 28}, Error message: \n",
      "Value mismatch for key age: 28 != 26\n",
      "\n",
      "\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 1: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data 'June 3, 1992' does not match format '%Y-%m-%d'\n",
      "time data 'June 5, 1996' does not match format '%Y-%m-%d'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 2: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data '\"Search' does not match format '%B'\n",
      "time data '\"Search' does not match format '%B'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 3: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data 'June 3, 1992' does not match format '%Y-%m-%d'\n",
      "time data 'June 5, 1996' does not match format '%Y-%m-%d'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 4: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data '{' does not match format '%B %d, %Y'\n",
      "time data '{' does not match format '%B %d, %Y'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 5: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "Output data type mismatch. Expected <class 'int'>, got <class 'NoneType'>\n",
      "Output data type mismatch. Expected <class 'int'>, got <class 'NoneType'>\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 6: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data 'June 3, 1992' does not match format '%Y-%m-%d'\n",
      "time data 'June 5, 1996' does not match format '%Y-%m-%d'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 7: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data '10\\n    }\\n  ]\\n}' does not match format '%Y'\n",
      "time data '10\\n    }\\n  ]\\n}' does not match format '%Y'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 8: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "Birthdate not found\n",
      "Birthdate not found\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 9: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "typing.Union cannot be used with isinstance()\n",
      "typing.Union cannot be used with isinstance()\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 10: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "Could not find celebrity's birthdate\n",
      "Could not find celebrity's birthdate\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 11: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data 'June 3, 1992' does not match format '%Y-%m-%d'\n",
      "time data 'June 5, 1996' does not match format '%Y-%m-%d'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 12: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data '\\n    Based on the following information about June 3, 1992, please determine their current age:\\n\\n    Search results: {\\n  \"Search Result\": [\\n    {\\n      \"title\": \"Birthday Analysis for June 3, 1992 - Zodiac sign and Horoscope\",\\n      \"link\": \"https://www.ask-oracle.com/birthday/1992/06/03/\",\\n      \"snippet\": \"Born on Wednesday, June 3, 1992, birthday analysis reveals you are 32 years old. Your zodiac sign is Gemini and the Chinese zodiac sign is ...\",\\n      \"date\": \"Jun 3, 1992\",\\n      \"position\": 1\\n    },\\n    {\\n      \"title\": \"Jun 3, 1992 - astrology calendar - aspects & transits - Astrosofa\",\\n      \"link\": \"https://www.astrosofa.com/horoscope/aspects/1992/6/3\",\\n      \"snippet\": \"During this time, you can be eccentric, headstrong, fanatic, over-the-top, irritable, and extremely moody. It\\'s not a good time to deal with money.\",\\n      \"date\": \"Jun 3, 1992\",\\n      \"position\": 2\\n    },\\n    {\\n      \"title\": \"What Happened on June 3, 1992 - On This Day\",\\n      \"link\": \"https://www.onthisday.com/date/1992/june/3\",\\n      \"snippet\": \"What happened on June 3, 1992. Browse historical events, famous birthdays and notable deaths from Jun 3, 1992 or search by date, day or keyword.\",\\n      \"date\": \"Jun 3, 1992\",\\n      \"position\": 3\\n    },\\n    {\\n      \"title\": \"June 3, 1992 - Born/Birthdate - Meaning and potential\",\\n      \"link\": \"https://www.kabalarians.com/birthdate/1992-June-3.htm\",\\n      \"snippet\": \"Birthdate: June 3, 1992. Self-expression, linked closely with ideals of service to humanity, is the keynote of your inner potential.\",\\n      \"date\": \"Jun 3, 1992\",\\n      \"position\": 4\\n    },\\n    {\\n      \"title\": \"My birthday is June 03, 1992. I was born at 6:30 pm in ... - Quora\",\\n      \"link\": \"https://www.quora.com/My-birthday-is-June-03-1992-I-was-born-at-6-30-pm-in-Weston-Ontario-I-am-a-Gemini-Sun-Cancer-Moon-and-Scorpio-rising-How-does-my-astrological-chart-influence-me\",\\n      \"snippet\": \"My birthday is June 03, 1992. I was born at 6:30 pm in Weston, Ontario. I am a Gemini Sun, Cancer Moon, and Scorpio rising. How does my ...\",\\n      \"date\": \"Mar 13, 2021\",\\n      \"position\": 5\\n    },\\n    {\\n      \"title\": \"June 03, 1992, What happened that day? | TakeMeBack.to\",\\n      \"link\": \"https://takemeback.to/03-June-1992\",\\n      \"snippet\": \"Someone born on June 03, 1992 is: 32 years 5 months old,. or 389 months and 12 days old,. or 1693 weeks and 2 days old,. or 11853 days old ...\",\\n      \"date\": \"Jun 3, 1992\",\\n      \"position\": 6\\n    },\\n    {\\n      \"title\": \"June 3 1992 horoscope and zodiac sign meanings\",\\n      \"link\": \"https://www.thehoroscope.co/birthday-analyser/June-3-1992-horoscope-and-zodiac-sign-meanings-15505.html\",\\n      \"snippet\": \"The astrological sign of someone born on June 3, 1992 is Gemini. The period designated to this sign is between May 21 and June 20.\",\\n      \"date\": \"Jun 3, 1992\",\\n      \"position\": 7\\n    },\\n    {\\n      \"title\": \"June 3 Birthday Astrology - Entertainment | HowStuffWorks\",\\n      \"link\": \"https://entertainment.howstuffworks.com/horoscopes-astrology/june-3-birthday-astrology.htm\",\\n      \"snippet\": \"A Gemini born June 3 is symbolized by the Twins and has an intelligence that is analytical in nature. Learn more about June 3 birthday astrology.\",\\n      \"position\": 8\\n    },\\n    {\\n      \"title\": \"23 Fun Birthday Facts About June 3, 1992 You Must Know\",\\n      \"link\": \"https://mybirthday.ninja/?m=June&d=3&y=1992&go=Go\",\\n      \"snippet\": \"June 3, 1992 was a Wednesday and it was the 155 th day of the year 1992. It was the 23 rd Wednesday of that year.\",\\n      \"position\": 9\\n    },\\n    {\\n      \"title\": \"June 3 - Wikipedia\",\\n      \"link\": \"https://en.wikipedia.org/wiki/June_3\",\\n      \"snippet\": \"June 3 is the 154th day of the year (155th in leap years) in the Gregorian calendar; 211 days remain until the end of the year.\",\\n      \"sitelinks\": [\\n        {\\n          \"title\": \"Portal:Current events/2016...\",\\n          \"link\": \"https://en.wikipedia.org/wiki/Portal:Current_events/2016_June_3\"\\n        },\\n        {\\n          \"title\": \"Portal:Current events/2017...\",\\n          \"link\": \"https://en.wikipedia.org/wiki/Portal:Current_events/2017_June_3\"\\n        },\\n        {\\n          \"title\": \"June 30\",\\n          \"link\": \"https://en.wikipedia.org/wiki/June_30\"\\n        }\\n      ],\\n      \"position\": 10\\n    }\\n  ]\\n}\\n\\n    Calculate the age by subtracting the birth year from the current year (2024).\\n\\n    Please provide the result in the following JSON format:\\n    {\\n        \"age\": int(calculated_age)\\n    }\\n\\n    If the birth date is not found or unclear, return null for the age value.\\n    ' does not match format '%B %d, %Y'\n",
      "time data '\\n    Based on the following information about June 5, 1996, please determine their current age:\\n\\n    Search results: {\\n  \"Search Result\": [\\n    {\\n      \"title\": \"Birthday Analysis for June 5, 1996 - Zodiac sign and Horoscope\",\\n      \"link\": \"https://www.ask-oracle.com/birthday/1996/06/05/\",\\n      \"snippet\": \"Born on Wednesday, June 5, 1996, birthday analysis reveals you are 28 years old. Your zodiac sign is Gemini and the Chinese zodiac sign is ...\",\\n      \"date\": \"Jun 5, 1996\",\\n      \"position\": 1\\n    },\\n    {\\n      \"title\": \"Jun 5, 1996 - astrology calendar - aspects & transits - Astrosofa\",\\n      \"link\": \"https://www.astrosofa.com/horoscope/aspects/1996/6/5\",\\n      \"snippet\": \"All astrology aspects and transits for Jun 5, 1996 plus zodiac signs and chinese zodiac signs. Calendar planet constellations, daily ...\",\\n      \"date\": \"Jun 5, 1996\",\\n      \"position\": 2\\n    },\\n    {\\n      \"title\": \"June 5 1996 horoscope and zodiac sign meanings\",\\n      \"link\": \"https://www.thehoroscope.co/birthday-analyser/June-5-1996-horoscope-and-zodiac-sign-meanings-16968.html\",\\n      \"snippet\": \"People born on June 5 1996 are ruled by Gemini. \\\\u00b7 Gemini is symbolized by Twins. \\\\u00b7 The life path number for people born on 5 Jun 1996 is 9.\",\\n      \"date\": \"Jun 5, 1996\",\\n      \"position\": 3\\n    },\\n    {\\n      \"title\": \"June 05, 1996, What happened that day? | TakeMeBack.to\",\\n      \"link\": \"https://takemeback.to/05-June-1996\",\\n      \"snippet\": \"Who was born on June 05, 1996? \\\\u00b7 Korynne Kolbinger \\\\u00b7 Jimson Lee \\\\u00b7 Brandon Lee \\\\u00b7 Sarah Bismuth \\\\u00b7 Rodrigo Le\\\\u00e3o \\\\u00b7 Victoria Llorente \\\\u00b7 G\\\\u00e9nesis Miranda.\",\\n      \"date\": \"Jun 5, 1996\",\\n      \"position\": 4\\n    },\\n    {\\n      \"title\": \"June 5 Birthday Astrology - Entertainment | HowStuffWorks\",\\n      \"link\": \"https://entertainment.howstuffworks.com/horoscopes-astrology/june-5-birthday-astrology.htm\",\\n      \"snippet\": \"A Gemini born June 5 is symbolized by the Twins and draws inspiration from everyday miracles. Learn more about June 5 birthday astrology.\",\\n      \"position\": 5\\n    },\\n    {\\n      \"title\": \"23 Fun Birthday Facts About June 5, 1996 You Must Know\",\\n      \"link\": \"https://mybirthday.ninja/?m=June&d=5&y=1996&go=Go\",\\n      \"snippet\": \"June 5, 1996 Birthday Facts Summary \\\\u00b7 Day index: 157th day of 1996 \\\\u00b7 Day of week: Wednesday \\\\u00b7 Day of week birthstone: Amethyst \\\\u00b7 Week index: 23rd Wednesday of 1996 ...\",\\n      \"position\": 6\\n    },\\n    {\\n      \"title\": \"June 5, 1996 - Born/Birthdate - Meaning and potential\",\\n      \"link\": \"https://www.kabalarians.com/birthdate/1996-June-5.htm\",\\n      \"snippet\": \"Birthdate: June 5, 1996. The qualities of your inner nature include generosity, a love for humanity, and a desire to serve and help others.\",\\n      \"date\": \"Jun 5, 1996\",\\n      \"position\": 7\\n    },\\n    {\\n      \"title\": \"Born in June 1996 - Birthday Analysis - Zodiac sign and Horoscope\",\\n      \"link\": \"https://www.ask-oracle.com/birthday/1996/06/\",\\n      \"snippet\": \"If you were born on 1st June, 1996, you are 28 years old as on November 1, 2024. Your next birthday is 6 months away. What happened in June 1996.\",\\n      \"date\": \"Jun 1, 1996\",\\n      \"position\": 8\\n    },\\n    {\\n      \"title\": \"June 5th, 1996 Horoscope | TakeMeBack.to\",\\n      \"link\": \"https://takemeback.to/horoscope/05-June-1996\",\\n      \"snippet\": \"As a Gemini born on Wednesday, June 5, 1996, you possess a unique blend of traits that set you apart from other Geminis. You are highly adaptable.\",\\n      \"date\": \"Jun 5, 1996\",\\n      \"position\": 9\\n    },\\n    {\\n      \"title\": \"What Happened on June 5, 1996 - On This Day\",\\n      \"link\": \"https://www.onthisday.com/date/1996/june/5\",\\n      \"snippet\": \"What happened on June 5, 1996. Browse historical events, famous birthdays and notable deaths from Jun 5, 1996 or search by date, day or keyword.\",\\n      \"date\": \"Jun 5, 1996\",\\n      \"position\": 10\\n    }\\n  ]\\n}\\n\\n    Calculate the age by subtracting the birth year from the current year (2024).\\n\\n    Please provide the result in the following JSON format:\\n    {\\n        \"age\": int(calculated_age)\\n    }\\n\\n    If the birth date is not found or unclear, return null for the age value.\\n    ' does not match format '%B %d, %Y'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 13: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "üìä Code 13: Fitness: 75.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'birthdate': 'June 5, 1996'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 28}, Error message: \n",
      "Value mismatch for key age: 28 != 26\n",
      "\n",
      "\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 14: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "invalid literal for int() with base 10: 'June 3, 1992'\n",
      "invalid literal for int() with base 10: 'June 5, 1996'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 15: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "invalid literal for int() with base 10: '\\n    Based on the following information about June 3, 1992, please determine their current age:\\n\\n    Search results: {\\n  \"Search Result\": [\\n    {\\n      \"title\": \"Birthday Analysis for June 3, 1\n",
      "invalid literal for int() with base 10: '\\n    Based on the following information about June 5, 1996, please determine their current age:\\n\\n    Search results: {\\n  \"Search Result\": [\\n    {\\n      \"title\": \"Birthday Analysis for June 5, 1\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 16: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "time data '{}' does not match format '%B %d, %Y'\n",
      "time data '{}' does not match format '%B %d, %Y'\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
<<<<<<< HEAD
      "üìä Code 17: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "üìä Code 17: Fitness: 75.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Input: {'birthdate': 'June 5, 1996'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 28}, Error message: \n",
      "Value mismatch for key age: 28 != 26\n",
      "\n",
      "\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 18: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
<<<<<<< HEAD
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "üìä Code 19: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "‚ùå Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
=======
      "cannot access local variable 'birthdate_str' where it is not associated with a value\n",
      "cannot access local variable 'birthdate_str' where it is not associated with a value\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# plan.evolve_sub_nodes() # Completely stuck in the first call, debugging ... \n",
    "\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "\n",
    "self = plan \n",
<<<<<<< HEAD
=======
    "\n",
    "# 1. Did we skip existing nodes? Yes, those node has code & fitness and is skipped.\n",
    "# 2. Did we make the name compatible with slightly off input?\n",
    "\n",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
    "for i, node_dict in enumerate(self.plan_dict[\"nodes\"]):\n",
    "    meta_prompt = MetaPrompt(\n",
    "        task=node_dict[\"task\"],\n",
    "        func_name=node_dict[\"name\"],\n",
    "        inputs=node_dict[\"inputs\"],\n",
    "        outputs=node_dict[\"outputs\"],\n",
    "        input_types=node_dict[\"input_types\"],\n",
    "        output_types=node_dict[\"output_types\"],\n",
    "        mode=PromptMode((node_dict.get(\"mode\", \"code\")).lower())\n",
    "    )\n",
    "    test_cases = self.test_cases_dict[node_dict[\"name\"]]\n",
    "    if \"fitness\" in node_dict and \"code\" in node_dict: \n",
    "        node = EvolNode(meta_prompt, node_dict[\"code\"], node_dict[\"reasoning\"], get_response=self.get_response, test_cases=test_cases, fitness=node_dict[\"fitness\"])\n",
    "    else:\n",
    "        node = EvolNode(meta_prompt, None, None, get_response=self.get_response, test_cases=test_cases)\n",
    "        print(f\"üé≤ :: Evolving {node.meta_prompt.func_name} ... ({i+1}/{len(self.plan_dict['nodes'])})\")\n",
    "        node.evolve(\"i1\", replace=True, max_tries=2, num_runs=2, batch_size=20) # It's funny how 30+ sec could elapse before llm inference ... (collecting prompts ?? wtf is taking so long ??)\n",
    "    self.nodes.append(node)"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "success: successfully compiled d2_output/plan_graph.d2 to d2_output/plan_graph.png in 170.887ms\n"
     ]
    }
   ],
   "source": [
    "visualize_plan_dict(plan.plan_dict, plan.meta_prompt.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import datetime\n",
      "import http.client\n",
      "import json\n",
      "import os\n",
      "import re\n",
      "from typing import Dict, Any\n",
      "\n",
      "\n",
      "def _search_google(query: str) ->Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Use Serper API to search Google for information\n",
      "    \n",
      "    Args:\n",
      "        query (str): The search query\n",
      "    \n",
      "    Returns:\n",
      "        Dict[str, Any]: Parsed JSON response from the API\n",
      "    \"\"\"\n",
      "    conn = http.client.HTTPSConnection('google.serper.dev')\n",
      "    payload = json.dumps({'q': query})\n",
      "    headers = {'X-API-KEY': os.environ['SERPER_API_KEY'], 'Content-Type':\n",
      "        'application/json'}\n",
      "    try:\n",
      "        conn.request('POST', '/search', payload, headers)\n",
      "        res = conn.getresponse()\n",
      "        data = res.read()\n",
      "        return json.loads(data.decode('utf-8'))\n",
      "    except Exception as e:\n",
      "        print(f'Error occurred during API request: {str(e)}')\n",
      "        return {}\n",
      "    finally:\n",
      "        conn.close()\n",
      "\n",
      "\n",
      "def search_google(query: str) ->str:\n",
      "    \"\"\" \n",
      "    Input query, return search result string from Google\n",
      "    \"\"\"\n",
      "    result = _search_google(query)\n",
      "    result_dict = {k.replace('organic', 'Search Result'): v for k, v in\n",
      "        result.items() if k in ['answerBox', 'organic']}\n",
      "    result_str = json.dumps(result_dict, indent=2)\n",
      "    return result_str\n",
      "\n",
      "\n",
      "def extract_age(text: str) ->int:\n",
      "    \"\"\"\n",
      "    Extract the age from a given text.\n",
      "    \n",
      "    Parameters:\n",
      "    text (str): The text containing the person's name and possibly their age.\n",
      "    \n",
      "    Returns:\n",
      "    int: The age of the person mentioned in the text.\n",
      "    \"\"\"\n",
      "    pattern = '(\\\\w+) was born in (\\\\d{4})'\n",
      "    match = re.search(pattern, text)\n",
      "    if match:\n",
      "        name = match.group(1)\n",
      "        birth_year = int(match.group(2))\n",
      "        current_year = datetime.now().year\n",
      "        age = current_year - birth_year\n",
      "        return age\n",
      "    pattern = 'born (\\\\d{4})'\n",
      "    match = re.search(pattern, text)\n",
      "    if match:\n",
      "        birth_year = int(match.group(1))\n",
      "        current_year = datetime.now().year\n",
      "        age = current_year - birth_year\n",
      "        return age\n",
      "    pattern = '(\\\\w+)'\n",
      "    match = re.search(pattern, text)\n",
      "    if match:\n",
      "        name = match.group(1)\n",
      "        query = f'age of {name}'\n",
      "        result = search_google(query)\n",
      "        pattern = '\\\\d+ years old'\n",
      "        match = re.search(pattern, result)\n",
      "        if match:\n",
      "            age = int(match.group().replace(' years old', ''))\n",
      "            return age\n",
      "    return None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(node.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'task': 'Perform Google Search',\n",
       "   'name': 'search_google',\n",
       "   'inputs': ['name'],\n",
       "   'input_types': ['str'],\n",
       "   'outputs': ['result'],\n",
       "   'output_types': ['str'],\n",
       "   'target': 'Retrieve Celebrity Information',\n",
       "   'mode': 'PROMPT',\n",
       "   'code': '\\nimport http.client\\nimport json\\nimport os\\nfrom typing import Dict, Any\\n\\ndef _search_google(query: str) -> Dict[str, Any]:\\n    \"\"\"\\n    Use Serper API to search Google for information\\n    \\n    Args:\\n        query (str): The search query\\n    \\n    Returns:\\n        Dict[str, Any]: Parsed JSON response from the API\\n    \"\"\"\\n    conn = http.client.HTTPSConnection(\"google.serper.dev\")\\n    payload = json.dumps({\"q\": query})\\n    headers = {\\n        \\'X-API-KEY\\': os.environ[\"SERPER_API_KEY\"],\\n        \\'Content-Type\\': \\'application/json\\'\\n    }\\n    \\n    try:\\n        conn.request(\"POST\", \"/search\", payload, headers)\\n        res = conn.getresponse()\\n        data = res.read()\\n        return json.loads(data.decode(\"utf-8\"))\\n    except Exception as e:\\n        print(f\"Error occurred during API request: {str(e)}\")\\n        return {}\\n    finally:\\n        conn.close()\\n        \\ndef search_google(query: str) -> str: \\n    \"\"\" \\n    Input query, return search result string from Google\\n    \"\"\"\\n    result = _search_google(query)\\n    result_dict = {k.replace(\"organic\", \"Search Result\"): v for k, v in result.items() if k in [\"answerBox\", \"organic\"]}\\n    result_str = json.dumps(result_dict, indent=2)\\n    return result_str \\n',\n",
       "   'reasoning': 'Search google for top search results',\n",
       "   'fitness': 1.0},\n",
       "  {'task': 'Extract Age from Text',\n",
       "   'name': 'extract_age',\n",
       "   'inputs': ['text'],\n",
       "   'input_types': ['str'],\n",
       "   'outputs': ['age'],\n",
       "   'output_types': ['int'],\n",
       "   'target': 'Find Celebrity Age',\n",
       "   'mode': 'CODE'}],\n",
       " 'edges': [{'source': 'search_google', 'target': 'extract_age'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.plan_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
  }
 ],
 "metadata": {
  "kernelspec": {
<<<<<<< HEAD
   "display_name": "Python 3 (ipykernel)",
=======
   "display_name": "Python 3",
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.12.4"
=======
   "version": "3.11.5"
>>>>>>> 9eccc488f700e20bbbdab63b4b603a67d932a3ea
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
