{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/evolnode.png\" width=\"800\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> Evolution Node free you from coding and debugging, let LLM evolve your code for you.</em></p>\n",
    "</div>\n",
    "<div align=\"left\">\n",
    "<p><em>Bored of manual coding, a function? EvolNode let LLM automate function design and guide the evolution of it with genetic algorithm. A node here takes a task, input, and output, it uses either code or another LLM to complete the task, ensuring aligned input and output value types and ,.names.Fitness evaluation is done by running the function with a few specified test cases, the more diverse the test case, the better the evolution.\n",
    "</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_prompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetaPrompt, PromptMode\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmethods\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meoh_evolution\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m EvolNode\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Code + Compilor Task\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# mp = MetaPrompt(\"Generate Fibonacci sequence.\", \"fibonacci\", [\"n\"], [\"sequence\"], [\"int\"], [\"list\"], PromptMode.CODE) # \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# node = EvolNode(mp, None, None)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Prompt + LLM Task\u001b[39;00m\n\u001b[1;32m     14\u001b[0m mp \u001b[38;5;241m=\u001b[39m MetaPrompt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGet the age of a celebrity.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_celeb_age\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m], PromptMode\u001b[38;5;241m.\u001b[39mPROMPT) \u001b[38;5;66;03m# \u001b[39;00m\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/eoh_evolution.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_prompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MetaPlan, extract_json_from_text\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmeta_execute\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m call_func_code, call_func_prompt\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_openai_response \u001b[38;5;28;01mas\u001b[39;00m get_response\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optional, Dict, List\n",
      "File \u001b[0;32m~/Implementation/heuristic-search/eoh/methods/llm.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m \n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01manthropic\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     10\u001b[0m oai_client \u001b[38;5;241m=\u001b[39m OpenAI()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/anthropic/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, NoneType, NotGiven, Transport, ProxiesTypes\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/anthropic/types/__init__.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage_stream_event\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessageStreamEvent \u001b[38;5;28;01mas\u001b[39;00m MessageStreamEvent\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtool_use_block_param\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToolUseBlockParam \u001b[38;5;28;01mas\u001b[39;00m ToolUseBlockParam\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage_create_params\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessageCreateParams \u001b[38;5;28;01mas\u001b[39;00m MessageCreateParams\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mraw_message_stop_event\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RawMessageStopEvent \u001b[38;5;28;01mas\u001b[39;00m RawMessageStopEvent\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mraw_message_delta_event\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RawMessageDeltaEvent \u001b[38;5;28;01mas\u001b[39;00m RawMessageDeltaEvent\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/anthropic/types/message_create_params.py:23\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessage_param\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MessageParam\n\u001b[1;32m     11\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessageCreateParamsBase\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessageCreateParamsStreaming\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m ]\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mMessageCreateParamsBase\u001b[39;49;00m\u001b[43m(\u001b[49m\u001b[43mTypedDict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mRequired\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;250;43m    \u001b[39;49m\u001b[38;5;124;43;03m\"\"\"The maximum number of tokens to generate before stopping.\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;43;03m    Note that our models may stop _before_ reaching this maximum. This parameter\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;43;03m    [models](https://docs.anthropic.com/en/docs/models-overview) for details.\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;43;03m    \"\"\"\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/typing_extensions.py:850\u001b[0m, in \u001b[0;36m_TypedDictMeta.__new__\u001b[0;34m(cls, name, bases, ns, total)\u001b[0m\n\u001b[1;32m    848\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypedDict(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mf0: t0, f1: t1, ...}); each t must be a type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TAKES_MODULE:\n\u001b[0;32m--> 850\u001b[0m     own_annotations \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_type_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtp_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mown_annotations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     own_annotations \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    856\u001b[0m         n: typing\u001b[38;5;241m.\u001b[39m_type_check(tp, msg)\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n, tp \u001b[38;5;129;01min\u001b[39;00m own_annotations\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    858\u001b[0m     }\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/typing_extensions.py:851\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    848\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTypedDict(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mf0: t0, f1: t1, ...}); each t must be a type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _TAKES_MODULE:\n\u001b[1;32m    850\u001b[0m     own_annotations \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 851\u001b[0m         n: \u001b[43mtyping\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_type_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtp_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__module__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n, tp \u001b[38;5;129;01min\u001b[39;00m own_annotations\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    853\u001b[0m     }\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    855\u001b[0m     own_annotations \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    856\u001b[0m         n: typing\u001b[38;5;241m.\u001b[39m_type_check(tp, msg)\n\u001b[1;32m    857\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m n, tp \u001b[38;5;129;01min\u001b[39;00m own_annotations\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    858\u001b[0m     }\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/typing.py:186\u001b[0m, in \u001b[0;36m_type_check\u001b[0;34m(arg, msg, is_argument, module, allow_special_forms)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_argument:\n\u001b[1;32m    184\u001b[0m         invalid_generic_forms \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (Final,)\n\u001b[0;32m--> 186\u001b[0m arg \u001b[38;5;241m=\u001b[39m \u001b[43m_type_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_special_forms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_special_forms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arg, _GenericAlias) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    188\u001b[0m         arg\u001b[38;5;241m.\u001b[39m__origin__ \u001b[38;5;129;01min\u001b[39;00m invalid_generic_forms):\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not valid as type argument\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/typing.py:164\u001b[0m, in \u001b[0;36m_type_convert\u001b[0;34m(arg, module, allow_special_forms)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mForwardRef\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_special_forms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/typing.py:857\u001b[0m, in \u001b[0;36mForwardRef.__init__\u001b[0;34m(self, arg, is_argument, module, is_class)\u001b[0m\n\u001b[1;32m    855\u001b[0m     arg_to_compile \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 857\u001b[0m     code \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcompile\u001b[39m(arg_to_compile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<string>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m:\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward reference must be an expression -- got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.eoh_evolution import EvolNode\n",
    "\n",
    "\n",
    "# Code + Compilor Task\n",
    "# mp = MetaPrompt(\"Generate Fibonacci sequence.\", \"fibonacci\", [\"n\"], [\"sequence\"], [\"int\"], [\"list\"], PromptMode.CODE) # \n",
    "# node = EvolNode(mp, None, None)\n",
    "# input_dict = {\"n\": 10}\n",
    "# reasoning, code = node.evolve([input_dict], \"i1\", replace=True) # Evolution with guaranteed structural fitness\n",
    "# node(input_dict) # Ok we need a output dictionary here as well ...\n",
    "\n",
    "\n",
    "# Prompt + LLM Task\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "node = EvolNode(mp, None, None)\n",
    "input_dict = {\"name\": \"Dilireba\"}\n",
    "reasoning, code = node.evolve([input_dict], \"i1\", replace=True)\n",
    "node(input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/user-attachments/assets/af98faeb-66d6-4278-af86-67d668d1954e\" width=\"1000\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> But how do we know what are the tasks suitable for our goal? Design of tasks topology is the fundation of planning, let's ask LLM for help on this, too! Evolution Graph autuomate planning by imagning topology of tasks which works best for your goal.</em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MetaPrompt I1: \n",
      "Goal: Help me date Dilireba, I'm a white colar worker living on salary.\n",
      "First, describe the intuition for your tactics and main steps in one sentence. The description must be inside a brace.\n",
      "Generate a JSON-style plan represented as a Directed Acyclic Graph (DAG) to achieve the goal. Use creative topology in the DAG, include parallel tasks if required.\n",
      "\n",
      "The plan should include:\n",
      "\n",
      "- **Nodes**: Each node represents a key action or step and must contain the following attributes:\n",
      "  - `task`: Description of the task.\n",
      "  - `name`: Concise name used for the task function.\n",
      "  - `input`: The resources, information, or prerequisites needed to perform the action.\n",
      "  - `output`: The immediate result or outcome of the action.\n",
      "  - `target`: The purpose or goal that the action contributes to.\n",
      "  - `mode`: The execution mode for this task (\"CODE\" or \"PROMPT\").\n",
      "\n",
      "- **Edges**: Each edge represents a dependency or relationship between nodes, indicating that one step supports or leads to another.\n",
      "  - `source`: The `id` of the source node (the preceding action).\n",
      "  - `target`: The `id` of the target node (the subsequent action).\n",
      "\n",
      "**Output Format:**\n",
      "\n",
      "Provide the output in the following JSON structure:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"task\": \"Task 1\",\n",
      "      \"name\": \"task_1\"\n",
      "      \"input\": \"Inputs required for Action 1\",\n",
      "      \"output\": \"Outputs/result of Action 1\",\n",
      "      \"target\": \"Purpose of Action 1\"\n",
      "      \"mode\": \"CODE\"\n",
      "    },\n",
      "    {\n",
      "      \"task\": \"Task 2\",\n",
      "      \"name\": \"task_2\",\n",
      "      \"input\": \"Inputs required for Action 2\",\n",
      "      \"output\": \"Outputs/result of Action 2\",\n",
      "      \"target\": \"Purpose of Action 2\",\n",
      "      \"mode\": \"PROMPT\"\n",
      "    }\n",
      "    // Add more nodes as needed\n",
      "  ],\n",
      "  \"edges\": [\n",
      "    {\n",
      "      \"source\": \"task_1\",\n",
      "      \"target\": \"task_2\"\n",
      "    }\n",
      "    // Add more edges as needed\n",
      "  ]\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPlan, extract_json_from_text\n",
    "\n",
    "mp = MetaPlan(\"Help me date Dilireba, I'm a white colar worker living on salary.\")\n",
    "prompt = mp._get_prompt_i1()\n",
    "print(\"MetaPrompt I1: \")\n",
    "print(prompt)\n",
    "\n",
    "from methods.llm import get_openai_response\n",
    "response = get_openai_response(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "# from methods.meta_prompt import parse_json_from_response\n",
    "\n",
    "tactic = re.findall(r\"\\{(.*)\\}\", response, re.DOTALL)\n",
    "plan_dict = extract_json_from_text(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<methods.eoh_evolution.EvolNode at 0x2893758d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from methods.meta_prompt import MetaPrompt, MetaPlan, extract_json_from_text\n",
    "from methods.eoh_evolution import EvolNode\n",
    "from methods.llm import get_openai_response as get_response\n",
    "from typing import Optional, List, Dict\n",
    "\n",
    "# Collect MetaPrompt from parsed Plan-Graph Response\n",
    "meta_node_prompts = []\n",
    "for node in plan_dict[\"nodes\"]:\n",
    "    node_prompt = MetaPrompt(task=node.get(\"task\"),  func_name=node.get(\"name\"), input=node.get(\"input\"), output=node.get(\"output\"), mode=node.get(\"mode\").lower())\n",
    "    meta_node_prompts.append(node_prompt)\n",
    "    \n",
    "edges = plan_dict[\"edges\"]\n",
    "\n",
    "EvolNode(meta_prompt = node_prompt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RepoRAG\n",
    "* Smooth Dependency Animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo already cloned, skipping cloning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "success: successfully compiled d2_output/evolve_graph_0.d2 to d2_output/evolve_graph_0.png in 688.717624ms\n",
      "success: successfully compiled d2_output/evolve_graph_1.d2 to d2_output/evolve_graph_1.png in 551.04575ms\n",
      "success: successfully compiled d2_output/evolve_graph_2.d2 to d2_output/evolve_graph_2.png in 530.829375ms\n",
      "success: successfully compiled d2_output/evolve_graph_3.d2 to d2_output/evolve_graph_3.png in 526.930417ms\n",
      "success: successfully compiled d2_output/evolve_graph_4.d2 to d2_output/evolve_graph_4.png in 528.646584ms\n",
      "success: successfully compiled d2_output/evolve_graph_5.d2 to d2_output/evolve_graph_5.png in 573.277125ms\n",
      "success: successfully compiled d2_output/evolve_graph_6.d2 to d2_output/evolve_graph_6.png in 524.445167ms\n",
      "success: successfully compiled d2_output/evolve_graph_7.d2 to d2_output/evolve_graph_7.png in 505.692208ms\n",
      "success: successfully compiled d2_output/evolve_graph_8.d2 to d2_output/evolve_graph_8.png in 511.190582ms\n",
      "success: successfully compiled d2_output/evolve_graph_9.d2 to d2_output/evolve_graph_9.png in 516.543292ms\n",
      "success: successfully compiled d2_output/evolve_graph_10.d2 to d2_output/evolve_graph_10.png in 540.215292ms\n",
      "success: successfully compiled d2_output/evolve_graph_11.d2 to d2_output/evolve_graph_11.png in 523.641209ms\n",
      "success: successfully compiled d2_output/evolve_graph_12.d2 to d2_output/evolve_graph_12.png in 515.868792ms\n",
      "success: successfully compiled d2_output/evolve_graph_13.d2 to d2_output/evolve_graph_13.png in 518.15675ms\n",
      "success: successfully compiled d2_output/evolve_graph_14.d2 to d2_output/evolve_graph_14.png in 524.987459ms\n",
      "success: successfully compiled d2_output/evolve_graph_15.d2 to d2_output/evolve_graph_15.png in 514.149917ms\n",
      "success: successfully compiled d2_output/evolve_graph_16.d2 to d2_output/evolve_graph_16.png in 525.933083ms\n",
      "success: successfully compiled d2_output/evolve_graph_17.d2 to d2_output/evolve_graph_17.png in 518.454916ms\n",
      "success: successfully compiled d2_output/evolve_graph_18.d2 to d2_output/evolve_graph_18.png in 560.795167ms\n",
      "success: successfully compiled d2_output/evolve_graph_19.d2 to d2_output/evolve_graph_19.png in 554.915459ms\n",
      "success: successfully compiled d2_output/evolve_graph_20.d2 to d2_output/evolve_graph_20.png in 581.815708ms\n",
      "success: successfully compiled d2_output/evolve_graph_21.d2 to d2_output/evolve_graph_21.png in 547.716084ms\n",
      "success: successfully compiled d2_output/evolve_graph_22.d2 to d2_output/evolve_graph_22.png in 537.779875ms\n",
      "success: successfully compiled d2_output/evolve_graph_23.d2 to d2_output/evolve_graph_23.png in 597.751375ms\n",
      "success: successfully compiled d2_output/evolve_graph_24.d2 to d2_output/evolve_graph_24.png in 570.974708ms\n",
      "success: successfully compiled d2_output/evolve_graph_25.d2 to d2_output/evolve_graph_25.png in 570.184293ms\n",
      "success: successfully compiled d2_output/evolve_graph_26.d2 to d2_output/evolve_graph_26.png in 566.610292ms\n",
      "success: successfully compiled d2_output/evolve_graph_27.d2 to d2_output/evolve_graph_27.png in 568.480333ms\n",
      "success: successfully compiled d2_output/evolve_graph_28.d2 to d2_output/evolve_graph_28.png in 572.443917ms\n",
      "success: successfully compiled d2_output/evolve_graph_29.d2 to d2_output/evolve_graph_29.png in 549.059583ms\n",
      "success: successfully compiled d2_output/evolve_graph_30.d2 to d2_output/evolve_graph_30.png in 582.06125ms\n",
      "success: successfully compiled d2_output/evolve_graph_31.d2 to d2_output/evolve_graph_31.png in 573.514875ms\n",
      "success: successfully compiled d2_output/evolve_graph_32.d2 to d2_output/evolve_graph_32.png in 590.582417ms\n",
      "success: successfully compiled d2_output/evolve_graph_33.d2 to d2_output/evolve_graph_33.png in 600.324542ms\n",
      "success: successfully compiled d2_output/evolve_graph_34.d2 to d2_output/evolve_graph_34.png in 575.083874ms\n",
      "success: successfully compiled d2_output/evolve_graph_35.d2 to d2_output/evolve_graph_35.png in 572.966542ms\n",
      "success: successfully compiled d2_output/evolve_graph_36.d2 to d2_output/evolve_graph_36.png in 623.435625ms\n",
      "success: successfully compiled d2_output/evolve_graph_37.d2 to d2_output/evolve_graph_37.png in 627.033959ms\n",
      "success: successfully compiled d2_output/evolve_graph_38.d2 to d2_output/evolve_graph_38.png in 626.627875ms\n",
      "success: successfully compiled d2_output/evolve_graph_39.d2 to d2_output/evolve_graph_39.png in 627.643834ms\n",
      "success: successfully compiled d2_output/evolve_graph_40.d2 to d2_output/evolve_graph_40.png in 625.883416ms\n",
      "success: successfully compiled d2_output/evolve_graph_41.d2 to d2_output/evolve_graph_41.png in 622.849292ms\n",
      "success: successfully compiled d2_output/evolve_graph_42.d2 to d2_output/evolve_graph_42.png in 663.517833ms\n",
      "success: successfully compiled d2_output/evolve_graph_43.d2 to d2_output/evolve_graph_43.png in 649.932042ms\n",
      "success: successfully compiled d2_output/evolve_graph_44.d2 to d2_output/evolve_graph_44.png in 656.906708ms\n",
      "success: successfully compiled d2_output/evolve_graph_45.d2 to d2_output/evolve_graph_45.png in 650.935375ms\n",
      "success: successfully compiled d2_output/evolve_graph_46.d2 to d2_output/evolve_graph_46.png in 657.302416ms\n",
      "success: successfully compiled d2_output/evolve_graph_47.d2 to d2_output/evolve_graph_47.png in 651.271625ms\n",
      "success: successfully compiled d2_output/evolve_graph_48.d2 to d2_output/evolve_graph_48.png in 673.121875ms\n",
      "success: successfully compiled d2_output/evolve_graph_49.d2 to d2_output/evolve_graph_49.png in 721.237375ms\n",
      "success: successfully compiled d2_output/evolve_graph_50.d2 to d2_output/evolve_graph_50.png in 728.206042ms\n",
      "success: successfully compiled d2_output/evolve_graph_51.d2 to d2_output/evolve_graph_51.png in 730.000709ms\n",
      "success: successfully compiled d2_output/evolve_graph_52.d2 to d2_output/evolve_graph_52.png in 726.282166ms\n",
      "success: successfully compiled d2_output/evolve_graph_53.d2 to d2_output/evolve_graph_53.png in 714.084041ms\n",
      "success: successfully compiled d2_output/evolve_graph_54.d2 to d2_output/evolve_graph_54.png in 721.532458ms\n",
      "success: successfully compiled d2_output/evolve_graph_55.d2 to d2_output/evolve_graph_55.png in 721.986666ms\n",
      "success: successfully compiled d2_output/evolve_graph_56.d2 to d2_output/evolve_graph_56.png in 714.533126ms\n",
      "success: successfully compiled d2_output/evolve_graph_57.d2 to d2_output/evolve_graph_57.png in 715.8715ms\n",
      "success: successfully compiled d2_output/evolve_graph_58.d2 to d2_output/evolve_graph_58.png in 757.139334ms\n",
      "success: successfully compiled d2_output/evolve_graph_59.d2 to d2_output/evolve_graph_59.png in 719.509375ms\n",
      "success: successfully compiled d2_output/evolve_graph_60.d2 to d2_output/evolve_graph_60.png in 762.038625ms\n",
      "success: successfully compiled d2_output/evolve_graph_61.d2 to d2_output/evolve_graph_61.png in 715.007918ms\n",
      "success: successfully compiled d2_output/evolve_graph_62.d2 to d2_output/evolve_graph_62.png in 732.640792ms\n",
      "success: successfully compiled d2_output/evolve_graph_63.d2 to d2_output/evolve_graph_63.png in 751.626166ms\n",
      "success: successfully compiled d2_output/evolve_graph_64.d2 to d2_output/evolve_graph_64.png in 723.542292ms\n",
      "success: successfully compiled d2_output/evolve_graph_65.d2 to d2_output/evolve_graph_65.png in 748.25575ms\n",
      "success: successfully compiled d2_output/evolve_graph_66.d2 to d2_output/evolve_graph_66.png in 747.521166ms\n",
      "success: successfully compiled d2_output/evolve_graph_67.d2 to d2_output/evolve_graph_67.png in 724.502625ms\n",
      "success: successfully compiled d2_output/evolve_graph_68.d2 to d2_output/evolve_graph_68.png in 723.209583ms\n",
      "success: successfully compiled d2_output/evolve_graph_69.d2 to d2_output/evolve_graph_69.png in 717.329125ms\n",
      "success: successfully compiled d2_output/evolve_graph_70.d2 to d2_output/evolve_graph_70.png in 719.220625ms\n",
      "success: successfully compiled d2_output/evolve_graph_71.d2 to d2_output/evolve_graph_71.png in 719.724208ms\n",
      "success: successfully compiled d2_output/evolve_graph_72.d2 to d2_output/evolve_graph_72.png in 712.919833ms\n",
      "success: successfully compiled d2_output/evolve_graph_73.d2 to d2_output/evolve_graph_73.png in 739.832916ms\n",
      "success: successfully compiled d2_output/evolve_graph_74.d2 to d2_output/evolve_graph_74.png in 713.694333ms\n",
      "success: successfully compiled d2_output/evolve_graph_75.d2 to d2_output/evolve_graph_75.png in 723.052083ms\n",
      "success: successfully compiled d2_output/evolve_graph_76.d2 to d2_output/evolve_graph_76.png in 713.130125ms\n",
      "success: successfully compiled d2_output/evolve_graph_77.d2 to d2_output/evolve_graph_77.png in 723.543501ms\n",
      "success: successfully compiled d2_output/evolve_graph_78.d2 to d2_output/evolve_graph_78.png in 721.523166ms\n",
      "success: successfully compiled d2_output/evolve_graph_79.d2 to d2_output/evolve_graph_79.png in 728.088375ms\n",
      "success: successfully compiled d2_output/evolve_graph_80.d2 to d2_output/evolve_graph_80.png in 717.992958ms\n",
      "success: successfully compiled d2_output/evolve_graph_81.d2 to d2_output/evolve_graph_81.png in 726.026333ms\n",
      "success: successfully compiled d2_output/evolve_graph_82.d2 to d2_output/evolve_graph_82.png in 743.068459ms\n",
      "success: successfully compiled d2_output/evolve_graph_83.d2 to d2_output/evolve_graph_83.png in 722.534875ms\n",
      "success: successfully compiled d2_output/evolve_graph_84.d2 to d2_output/evolve_graph_84.png in 719.859626ms\n",
      "success: successfully compiled d2_output/evolve_graph_85.d2 to d2_output/evolve_graph_85.png in 728.60975ms\n",
      "success: successfully compiled d2_output/evolve_graph_86.d2 to d2_output/evolve_graph_86.png in 736.503916ms\n",
      "success: successfully compiled d2_output/evolve_graph_87.d2 to d2_output/evolve_graph_87.png in 725.965501ms\n",
      "success: successfully compiled d2_output/evolve_graph_88.d2 to d2_output/evolve_graph_88.png in 730.041251ms\n",
      "success: successfully compiled d2_output/evolve_graph_89.d2 to d2_output/evolve_graph_89.png in 726.096249ms\n",
      "success: successfully compiled d2_output/evolve_graph_90.d2 to d2_output/evolve_graph_90.png in 735.007208ms\n",
      "success: successfully compiled d2_output/evolve_graph_91.d2 to d2_output/evolve_graph_91.png in 725.929333ms\n",
      "success: successfully compiled d2_output/evolve_graph_92.d2 to d2_output/evolve_graph_92.png in 726.458417ms\n",
      "success: successfully compiled d2_output/evolve_graph_93.d2 to d2_output/evolve_graph_93.png in 718.309833ms\n",
      "success: successfully compiled d2_output/evolve_graph_94.d2 to d2_output/evolve_graph_94.png in 726.8805ms\n",
      "success: successfully compiled d2_output/evolve_graph_95.d2 to d2_output/evolve_graph_95.png in 723.444709ms\n",
      "success: successfully compiled d2_output/evolve_graph_96.d2 to d2_output/evolve_graph_96.png in 716.166667ms\n",
      "success: successfully compiled d2_output/evolve_graph_97.d2 to d2_output/evolve_graph_97.png in 723.200541ms\n",
      "success: successfully compiled d2_output/evolve_graph_98.d2 to d2_output/evolve_graph_98.png in 743.516ms\n",
      "success: successfully compiled d2_output/evolve_graph_99.d2 to d2_output/evolve_graph_99.png in 736.355875ms\n",
      "Creating GIF: 100%|██████████| 100/100 [00:26<00:00,  3.75it/s]\n"
     ]
    }
   ],
   "source": [
    "from tools.repo import *\n",
    "\n",
    "# (I). Pick top growing repo\n",
    "# fastest_repos = get_fastest_growing_repos(days_ago=14, top_n = 50, print=False)\n",
    "# repo_url = fastest_repos[0]['html_url']\n",
    "\n",
    "# (I). Build a GIF of file-dependency of a git repo\n",
    "repo_url = \"https://github.com/xjdr-alt/entropix.git\"\n",
    "temp_repo = \"entropix_repo\"\n",
    "\n",
    "file_graph = create_gif_from_repo(repo_url, temp_repo, frame_count=60, fps = 10, output_name=\"entropix_repo_evolution\")\n",
    "\n",
    "# (II). Build function-level dependency graph of a module within the repo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Number of objects in the subgraph: 15\n"
     ]
    }
   ],
   "source": [
    "# (II). Build function-level dependency graph of a module within the repo\n",
    "from tools.repo import *\n",
    "\n",
    "temp_repo = \"hdcnn\" # Contains an interesting new DL model architecture's implementation\n",
    "\n",
    "python_files = get_python_files(temp_repo) # get all python files \n",
    "\n",
    "start_file = python_files[0] # pick the first one for debugging purpose \n",
    "\n",
    "dag = build_cross_file_dag(temp_repo, start_file)\n",
    "\n",
    "name_map = {dag[k][\"name\"]: k for k in dag} # Map name to node-id for all nodes in the DAG\n",
    "\n",
    "pick_object = list(name_map.keys())[0] # pick the first object from 'start_file'\n",
    "\n",
    "sub_dag = extract_subgraph_dag(dag, name_map[pick_object], depth=6) # Extact depedency graph starting from pick_object\n",
    "\n",
    "sub_dag = decide_opacity_of_dag(sub_dag, progress=1.0, cap_node_number=30) # limit number of nodes for easy understanding\n",
    "\n",
    "# func_graph = create_evolution_gif(sub_dag, frame_count=60, fps=10) # Sick Animation once again ...\n",
    "# visualize_dag(sub_dag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found class method with name:  HDComputing::superpose\n",
      "def superpose(self, hvs):\n",
      "    sum_hv = np.sum(hvs, axis=0)\n",
      "    return np.sign(sum_hv)\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_claude_response # <-- this is the LLM we will be using for summarization\n",
    "from tools.repo import read_node_content, get_summary_and_code\n",
    "\n",
    "node_content = read_node_content(sub_dag['node9']) # read code-snippet of a specific node\n",
    "\n",
    "summary_str, code_str = get_summary_and_code(sub_dag['node9'], get_claude_response) # get summary and code for a specific node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found function with name:  encode_sequence\n",
      "Found class method with name:  AGNewsDataset::__getitem__\n",
      "Found class method with name:  HDComputing::bind\n",
      "Found function with name:  create_token_hvs\n",
      "Found class method with name:  AGNewsDataset::__len__\n",
      "Found class method with name:  HDComputing::random_hv\n",
      "Found function with name:  build_vocab\n",
      "Found class method with name:  HDCNNClassifier::forward\n",
      "Found class method with name:  HDComputing::superpose\n",
      "Found class method with name:  HDComputing::permute\n",
      "Found class with name:  AGNewsDataset\n",
      "Found class with name:  HDComputing\n",
      "Found function with name:  main\n",
      "Found class with name:  HDCNNClassifier\n",
      "Node: AGNewsDataset (Level: 2)\n",
      "Summary: This code defines a custom PyTorch Dataset class called AGNewsDataset for processing text data. It tokenizes and encodes text into high-dimensional vectors using Hyperdimensional Computing techniques. The class prepares data for machine learning tasks, particularly for text classification, by converting text and labels into tensor format.\n",
      "Minimal Code:\n",
      "from torch.utils.data import Dataset\n",
      "from nltk.tokenize import word_tokenize\n",
      "import torch\n",
      "\n",
      "class AGNewsDataset(Dataset):\n",
      "    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\n",
      "        self.data = data\n",
      "        self.vocab = vocab\n",
      "        self.token_hvs = token_hvs\n",
      "        self.hd = hd\n",
      "        self.max_seq_len = max_seq_len\n",
      "        self.stop_words = stop_words\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        item = self.data[idx]\n",
      "        text, label = item['text'], item['label']\n",
      "        tokens = word_tokenize(text.lower())\n",
      "        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n",
      "        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\n",
      "        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n",
      "        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
      "\n",
      "def encode_sequence(tokens, token_hvs, hd):\n",
      "    # Placeholder for encode_sequence function\n",
      "    # This should implement the actual encoding logic\n",
      "    return [0] * hd.dimension  # Returning a dummy vector for illustration\n",
      "--------------------------------------------------\n",
      "Node: AGNewsDataset::__getitem__ (Level: 3)\n",
      "Summary: This code defines a custom dataset item retrieval method. It processes text data by tokenizing, cleaning, and encoding it into a high-dimensional vector using Hyperdimensional Computing. It also prepares the corresponding label. The method is likely part of a custom PyTorch Dataset class used for natural language processing tasks.\n",
      "Minimal Code:\n",
      "import torch\n",
      "from nltk.tokenize import word_tokenize\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "class CustomDataset:\n",
      "    def __init__(self, data, token_hvs, hd, max_seq_len=128):\n",
      "        self.data = data\n",
      "        self.token_hvs = token_hvs\n",
      "        self.hd = hd\n",
      "        self.max_seq_len = max_seq_len\n",
      "        self.stop_words = set(stopwords.words('english'))\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        item = self.data[idx]\n",
      "        text, label = item['text'], item['label']\n",
      "        \n",
      "        tokens = word_tokenize(text.lower())\n",
      "        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n",
      "        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\n",
      "        \n",
      "        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\n",
      "        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
      "\n",
      "def encode_sequence(tokens, token_hvs, hd):\n",
      "    # Placeholder for the encode_sequence function\n",
      "    # This should implement the Hyperdimensional Computing encoding\n",
      "    pass\n",
      "--------------------------------------------------\n",
      "Node: HDComputing::bind (Level: 3)\n",
      "Summary: The `bind` function performs element-wise multiplication of two hypervectors (HVs). This operation is a fundamental operation in Hyperdimensional Computing, used to bind or associate two concepts represented by high-dimensional vectors.\n",
      "Minimal Code:\n",
      "def bind(self, hv1, hv2):\n",
      "    return hv1 * hv2\n",
      "--------------------------------------------------\n",
      "Node: HDComputing (Level: 2)\n",
      "Summary: This code defines a class `HDComputing` that implements core operations for Hyperdimensional Computing. It includes methods for generating random high-dimensional vectors, superposing (combining) multiple vectors, binding (associating) two vectors, and permuting (shifting) a vector. These operations are fundamental in representing and manipulating information in high-dimensional spaces, which is the essence of Hyperdimensional Computing.\n",
      "Minimal Code:\n",
      "import numpy as np\n",
      "\n",
      "class HDComputing:\n",
      "    def __init__(self, dim, seed=None):\n",
      "        self.dim = dim\n",
      "        self.random_state = np.random.RandomState(seed)\n",
      "\n",
      "    def random_hv(self):\n",
      "        return self.random_state.choice([-1, 1], size=self.dim)\n",
      "\n",
      "    def superpose(self, hvs):\n",
      "        return np.sign(np.sum(hvs, axis=0))\n",
      "\n",
      "    def bind(self, hv1, hv2):\n",
      "        return hv1 * hv2\n",
      "\n",
      "    def permute(self, hv, shifts=1):\n",
      "        return np.roll(hv, shifts)\n",
      "--------------------------------------------------\n",
      "Node: create_token_hvs (Level: 3)\n",
      "Summary: This function, `create_token_hvs`, creates a dictionary that maps each token in a given vocabulary to a randomly generated high-dimensional vector (HV). It uses a hyperdimensional computing library (represented by the `hd` parameter) to generate these random vectors. The function takes three parameters: the vocabulary, the dimension of the vectors, and the hyperdimensional computing object.\n",
      "Minimal Code:\n",
      "def create_token_hvs(vocab, dim, hd):\n",
      "    return {token: hd.random_hv() for token in vocab}\n",
      "--------------------------------------------------\n",
      "Node: hdcnn.py (Level: 1)\n",
      "Summary: This code implements a text classification system using Hyperdimensional Computing (HDC) and a neural network for the AG News dataset. It processes text data into high-dimensional vectors, trains a simple neural network classifier, and compares its performance with a Naive Bayes baseline. The system includes custom dataset handling, vocabulary building, sequence encoding, and a neural network model. It also implements early stopping for training optimization.\n",
      "Minimal Code:\n",
      "\n",
      "--------------------------------------------------\n",
      "Node: AGNewsDataset::__len__ (Level: 3)\n",
      "Summary: This code defines a `__len__` method for a class, which returns the length of the `data` attribute of the class instance. This is typically used to make an object \"sized,\" allowing it to be used with Python's built-in `len()` function.\n",
      "Minimal Code:\n",
      "class SomeClass:\n",
      "    def __init__(self, data):\n",
      "        self.data = data\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.data)\n",
      "--------------------------------------------------\n",
      "Node: HDComputing::random_hv (Level: 3)\n",
      "Summary: This function generates a random high-dimensional vector (HV) of binary values (-1 or 1) with a specified dimension. It uses a random state object to ensure reproducibility and consistency in the generated vectors.\n",
      "Minimal Code:\n",
      "import numpy as np\n",
      "\n",
      "class HDComputing:\n",
      "    def __init__(self, dim, seed=None):\n",
      "        self.dim = dim\n",
      "        self.random_state = np.random.RandomState(seed)\n",
      "\n",
      "    def random_hv(self):\n",
      "        return self.random_state.choice([-1, 1], size=self.dim)\n",
      "\n",
      "# Usage example\n",
      "hd = HDComputing(dim=10000, seed=42)\n",
      "random_vector = hd.random_hv()\n",
      "--------------------------------------------------\n",
      "Node: main (Level: 2)\n",
      "Summary: This code implements a text classification task using Hyperdimensional Computing (HDC) and a neural network. It processes the AG News dataset, creates a vocabulary, encodes text into high-dimensional vectors, and trains a classifier. The program includes data loading, preprocessing, model training with early stopping, and evaluation. It also implements a Naive Bayes baseline for comparison.\n",
      "Minimal Code:\n",
      "\n",
      "--------------------------------------------------\n",
      "Node: HDCNNClassifier (Level: 2)\n",
      "Summary: This code defines a simple neural network classifier called HDCNNClassifier. It's designed to work with high-dimensional input vectors (likely from Hyperdimensional Computing) and classify them into multiple classes. The network consists of two fully connected layers with ReLU activation and dropout for regularization between them.\n",
      "Minimal Code:\n",
      "import torch.nn as nn\n",
      "\n",
      "class HDCNNClassifier(nn.Module):\n",
      "    def __init__(self, input_dim, num_classes):\n",
      "        super().__init__()\n",
      "        self.model = nn.Sequential(\n",
      "            nn.Linear(input_dim, 512),\n",
      "            nn.ReLU(),\n",
      "            nn.Dropout(0.6),\n",
      "            nn.Linear(512, num_classes)\n",
      "        )\n",
      "\n",
      "    def forward(self, x):\n",
      "        return self.model(x)\n",
      "--------------------------------------------------\n",
      "Node: build_vocab (Level: 3)\n",
      "Summary: This function builds a vocabulary from a dataset of text items. It tokenizes each text, removes non-alphabetic tokens and stop words, counts token frequencies, and creates a vocabulary dictionary with the most common tokens. It also includes special tokens like '[PAD]', '[UNK]', '[CLS]', and '[SEP]'.\n",
      "Minimal Code:\n",
      "from collections import Counter\n",
      "from nltk.tokenize import word_tokenize\n",
      "\n",
      "def build_vocab(dataset, max_vocab_size, stop_words):\n",
      "    counter = Counter()\n",
      "    for item in dataset:\n",
      "        tokens = word_tokenize(item['text'].lower())\n",
      "        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\n",
      "        counter.update(tokens)\n",
      "    \n",
      "    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]']\n",
      "    vocab_tokens = special_tokens + [t for t, _ in counter.most_common(max_vocab_size)]\n",
      "    return {token: idx for idx, token in enumerate(vocab_tokens)}\n",
      "--------------------------------------------------\n",
      "Node: encode_sequence (Level: 4)\n",
      "Summary: This function encodes a sequence of tokens into a single high-dimensional vector (HV) using Hyperdimensional Computing techniques. It iterates through the tokens, retrieves their corresponding HVs, applies position-based permutation, and combines them using element-wise addition. The final result is binarized using the sign function.\n",
      "Minimal Code:\n",
      "import numpy as np\n",
      "\n",
      "def encode_sequence(tokens, token_hvs, hd):\n",
      "    sequence_hv = np.zeros(hd.dim)\n",
      "    for i, token in enumerate(tokens):\n",
      "        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\n",
      "        permuted_token_hv = hd.permute(token_hv, shifts=i)\n",
      "        sequence_hv += permuted_token_hv\n",
      "    return np.sign(sequence_hv)\n",
      "--------------------------------------------------\n",
      "Node: HDCNNClassifier::forward (Level: 3)\n",
      "Summary: This code defines a forward pass of a simple neural network layer. It applies a fully connected (dense) layer, followed by an activation function, dropout for regularization, and another fully connected layer. This structure is typical for a basic feedforward neural network or a part of a more complex network.\n",
      "Minimal Code:\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "class SimpleNetwork(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, output_size):\n",
      "        super(SimpleNetwork, self).__init__()\n",
      "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
      "        self.activation = nn.ReLU()\n",
      "        self.dropout = nn.Dropout(0.5)\n",
      "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
      "\n",
      "    def forward(self, x):\n",
      "        out = self.fc1(x)\n",
      "        out = self.activation(out)\n",
      "        out = self.dropout(out)\n",
      "        out = self.fc2(out)\n",
      "        return out\n",
      "\n",
      "# Example usage\n",
      "model = SimpleNetwork(input_size=10, hidden_size=20, output_size=5)\n",
      "input_tensor = torch.randn(1, 10)\n",
      "output = model(input_tensor)\n",
      "print(output.shape)  # Should print torch.Size([1, 5])\n",
      "--------------------------------------------------\n",
      "Node: HDComputing::superpose (Level: 3)\n",
      "Summary: The `superpose` function combines multiple high-dimensional vectors (HVs) into a single representative HV. It does this by summing the vectors element-wise and then applying the sign function to binarize the result. This method is commonly used in Hyperdimensional Computing to aggregate information from multiple vectors.\n",
      "Minimal Code:\n",
      "import numpy as np\n",
      "\n",
      "def superpose(hvs):\n",
      "    return np.sign(np.sum(hvs, axis=0))\n",
      "--------------------------------------------------\n",
      "Node: HDComputing::permute (Level: 3)\n",
      "Summary: The `permute` function is a simple method that performs a circular shift on a given high-dimensional vector (HV). It uses NumPy's `roll` function to shift the elements of the vector by a specified number of positions. This operation is likely used as part of the position-based permutation in the encoding process of Hyperdimensional Computing.\n",
      "Minimal Code:\n",
      "import numpy as np\n",
      "\n",
      "def permute(hv, shifts=1):\n",
      "    return np.roll(hv, shifts)\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Bottom-up Summary + Minimal Code ()\n",
    "from tools.repo import bottom_up_summarization\n",
    "\n",
    "# Use the pipeline\n",
    "summarized_dag = bottom_up_summarization(sub_dag, get_claude_response)\n",
    "\n",
    "# Print summaries and minimal code for each node\n",
    "for node_id, node in summarized_dag.items():\n",
    "    print(f\"Node: {node['name']} (Level: {node['level']})\")\n",
    "    print(f\"Summary: {node['summary']}\")\n",
    "    print(f\"Minimal Code:\\n{node['minimal_code']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'node12': {'name': 'AGNewsDataset',\n",
       "  'type': 'class',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 4,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': ['node3', 'node15', 'node14'],\n",
       "  'opacity': 0.9333333333333333,\n",
       "  'level': 2,\n",
       "  'summary': 'This code defines a custom PyTorch Dataset class called AGNewsDataset for processing text data. It tokenizes and encodes text into high-dimensional vectors using Hyperdimensional Computing techniques. The class prepares data for machine learning tasks, particularly for text classification, by converting text and labels into tensor format.',\n",
       "  'minimal_code': \"from torch.utils.data import Dataset\\nfrom nltk.tokenize import word_tokenize\\nimport torch\\n\\nclass AGNewsDataset(Dataset):\\n    def __init__(self, data, vocab, token_hvs, hd, max_seq_len, stop_words):\\n        self.data = data\\n        self.vocab = vocab\\n        self.token_hvs = token_hvs\\n        self.hd = hd\\n        self.max_seq_len = max_seq_len\\n        self.stop_words = stop_words\\n\\n    def __len__(self):\\n        return len(self.data)\\n\\n    def __getitem__(self, idx):\\n        item = self.data[idx]\\n        text, label = item['text'], item['label']\\n        tokens = word_tokenize(text.lower())\\n        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\\n        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\\n\\ndef encode_sequence(tokens, token_hvs, hd):\\n    # Placeholder for encode_sequence function\\n    # This should implement the actual encoding logic\\n    return [0] * hd.dimension  # Returning a dummy vector for illustration\"},\n",
       " 'node15': {'name': 'AGNewsDataset::__getitem__',\n",
       "  'type': 'method',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 2,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': ['node3'],\n",
       "  'opacity': 0.8,\n",
       "  'level': 3,\n",
       "  'summary': 'This code defines a custom dataset item retrieval method. It processes text data by tokenizing, cleaning, and encoding it into a high-dimensional vector using Hyperdimensional Computing. It also prepares the corresponding label. The method is likely part of a custom PyTorch Dataset class used for natural language processing tasks.',\n",
       "  'minimal_code': \"import torch\\nfrom nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\n\\nclass CustomDataset:\\n    def __init__(self, data, token_hvs, hd, max_seq_len=128):\\n        self.data = data\\n        self.token_hvs = token_hvs\\n        self.hd = hd\\n        self.max_seq_len = max_seq_len\\n        self.stop_words = set(stopwords.words('english'))\\n\\n    def __getitem__(self, idx):\\n        item = self.data[idx]\\n        text, label = item['text'], item['label']\\n        \\n        tokens = word_tokenize(text.lower())\\n        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\\n        tokens = ['[CLS]'] + tokens[:self.max_seq_len] + ['[SEP]']\\n        \\n        seq_hv = encode_sequence(tokens, self.token_hvs, self.hd)\\n        return torch.tensor(seq_hv, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\\n\\ndef encode_sequence(tokens, token_hvs, hd):\\n    # Placeholder for the encode_sequence function\\n    # This should implement the Hyperdimensional Computing encoding\\n    pass\"},\n",
       " 'node10': {'name': 'HDComputing::bind',\n",
       "  'type': 'method',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': 'The `bind` function performs element-wise multiplication of two hypervectors (HVs). This operation is a fundamental operation in Hyperdimensional Computing, used to bind or associate two concepts represented by high-dimensional vectors.',\n",
       "  'minimal_code': 'def bind(self, hv1, hv2):\\n    return hv1 * hv2'},\n",
       " 'node6': {'name': 'HDComputing',\n",
       "  'type': 'class',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 5,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': ['node10', 'node9', 'node11', 'node8'],\n",
       "  'opacity': 1.0,\n",
       "  'level': 2,\n",
       "  'summary': 'This code defines a class `HDComputing` that implements core operations for Hyperdimensional Computing. It includes methods for generating random high-dimensional vectors, superposing (combining) multiple vectors, binding (associating) two vectors, and permuting (shifting) a vector. These operations are fundamental in representing and manipulating information in high-dimensional spaces, which is the essence of Hyperdimensional Computing.',\n",
       "  'minimal_code': 'import numpy as np\\n\\nclass HDComputing:\\n    def __init__(self, dim, seed=None):\\n        self.dim = dim\\n        self.random_state = np.random.RandomState(seed)\\n\\n    def random_hv(self):\\n        return self.random_state.choice([-1, 1], size=self.dim)\\n\\n    def superpose(self, hvs):\\n        return np.sign(np.sum(hvs, axis=0))\\n\\n    def bind(self, hv1, hv2):\\n        return hv1 * hv2\\n\\n    def permute(self, hv, shifts=1):\\n        return np.roll(hv, shifts)'},\n",
       " 'node2': {'name': 'create_token_hvs',\n",
       "  'type': 'function',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': 'This function, `create_token_hvs`, creates a dictionary that maps each token in a given vocabulary to a randomly generated high-dimensional vector (HV). It uses a hyperdimensional computing library (represented by the `hd` parameter) to generate these random vectors. The function takes three parameters: the vocabulary, the dimension of the vectors, and the hyperdimensional computing object.',\n",
       "  'minimal_code': 'def create_token_hvs(vocab, dim, hd):\\n    return {token: hd.random_hv() for token in vocab}'},\n",
       " 'node1': {'name': 'hdcnn.py',\n",
       "  'type': 'file',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 15,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': ['node15',\n",
       "   'node12',\n",
       "   'node10',\n",
       "   'node6',\n",
       "   'node2',\n",
       "   'node14',\n",
       "   'node8',\n",
       "   'node5',\n",
       "   'node16',\n",
       "   'node4',\n",
       "   'node3',\n",
       "   'node18',\n",
       "   'node9',\n",
       "   'node11'],\n",
       "  'opacity': 1.0,\n",
       "  'level': 1,\n",
       "  'summary': 'This code implements a text classification system using Hyperdimensional Computing (HDC) and a neural network for the AG News dataset. It processes text data into high-dimensional vectors, trains a simple neural network classifier, and compares its performance with a Naive Bayes baseline. The system includes custom dataset handling, vocabulary building, sequence encoding, and a neural network model. It also implements early stopping for training optimization.',\n",
       "  'minimal_code': ''},\n",
       " 'node14': {'name': 'AGNewsDataset::__len__',\n",
       "  'type': 'method',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': 'This code defines a `__len__` method for a class, which returns the length of the `data` attribute of the class instance. This is typically used to make an object \"sized,\" allowing it to be used with Python\\'s built-in `len()` function.',\n",
       "  'minimal_code': 'class SomeClass:\\n    def __init__(self, data):\\n        self.data = data\\n\\n    def __len__(self):\\n        return len(self.data)'},\n",
       " 'node8': {'name': 'HDComputing::random_hv',\n",
       "  'type': 'method',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': 'This function generates a random high-dimensional vector (HV) of binary values (-1 or 1) with a specified dimension. It uses a random state object to ensure reproducibility and consistency in the generated vectors.',\n",
       "  'minimal_code': 'import numpy as np\\n\\nclass HDComputing:\\n    def __init__(self, dim, seed=None):\\n        self.dim = dim\\n        self.random_state = np.random.RandomState(seed)\\n\\n    def random_hv(self):\\n        return self.random_state.choice([-1, 1], size=self.dim)\\n\\n# Usage example\\nhd = HDComputing(dim=10000, seed=42)\\nrandom_vector = hd.random_hv()'},\n",
       " 'node5': {'name': 'main',\n",
       "  'type': 'function',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 3,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': ['node2', 'node4'],\n",
       "  'opacity': 0.8666666666666667,\n",
       "  'level': 2,\n",
       "  'summary': 'This code implements a text classification task using Hyperdimensional Computing (HDC) and a neural network. It processes the AG News dataset, creates a vocabulary, encodes text into high-dimensional vectors, and trains a classifier. The program includes data loading, preprocessing, model training with early stopping, and evaluation. It also implements a Naive Bayes baseline for comparison.',\n",
       "  'minimal_code': ''},\n",
       " 'node16': {'name': 'HDCNNClassifier',\n",
       "  'type': 'class',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 2,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': ['node18'],\n",
       "  'opacity': 0.8,\n",
       "  'level': 2,\n",
       "  'summary': \"This code defines a simple neural network classifier called HDCNNClassifier. It's designed to work with high-dimensional input vectors (likely from Hyperdimensional Computing) and classify them into multiple classes. The network consists of two fully connected layers with ReLU activation and dropout for regularization between them.\",\n",
       "  'minimal_code': 'import torch.nn as nn\\n\\nclass HDCNNClassifier(nn.Module):\\n    def __init__(self, input_dim, num_classes):\\n        super().__init__()\\n        self.model = nn.Sequential(\\n            nn.Linear(input_dim, 512),\\n            nn.ReLU(),\\n            nn.Dropout(0.6),\\n            nn.Linear(512, num_classes)\\n        )\\n\\n    def forward(self, x):\\n        return self.model(x)'},\n",
       " 'node4': {'name': 'build_vocab',\n",
       "  'type': 'function',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': \"This function builds a vocabulary from a dataset of text items. It tokenizes each text, removes non-alphabetic tokens and stop words, counts token frequencies, and creates a vocabulary dictionary with the most common tokens. It also includes special tokens like '[PAD]', '[UNK]', '[CLS]', and '[SEP]'.\",\n",
       "  'minimal_code': \"from collections import Counter\\nfrom nltk.tokenize import word_tokenize\\n\\ndef build_vocab(dataset, max_vocab_size, stop_words):\\n    counter = Counter()\\n    for item in dataset:\\n        tokens = word_tokenize(item['text'].lower())\\n        tokens = [t for t in tokens if t.isalpha() and t not in stop_words]\\n        counter.update(tokens)\\n    \\n    special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]']\\n    vocab_tokens = special_tokens + [t for t, _ in counter.most_common(max_vocab_size)]\\n    return {token: idx for idx, token in enumerate(vocab_tokens)}\"},\n",
       " 'node3': {'name': 'encode_sequence',\n",
       "  'type': 'function',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 4,\n",
       "  'summary': 'This function encodes a sequence of tokens into a single high-dimensional vector (HV) using Hyperdimensional Computing techniques. It iterates through the tokens, retrieves their corresponding HVs, applies position-based permutation, and combines them using element-wise addition. The final result is binarized using the sign function.',\n",
       "  'minimal_code': \"import numpy as np\\n\\ndef encode_sequence(tokens, token_hvs, hd):\\n    sequence_hv = np.zeros(hd.dim)\\n    for i, token in enumerate(tokens):\\n        token_hv = token_hvs.get(token, token_hvs['[UNK]'])\\n        permuted_token_hv = hd.permute(token_hv, shifts=i)\\n        sequence_hv += permuted_token_hv\\n    return np.sign(sequence_hv)\"},\n",
       " 'node18': {'name': 'HDCNNClassifier::forward',\n",
       "  'type': 'method',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': 'This code defines a forward pass of a simple neural network layer. It applies a fully connected (dense) layer, followed by an activation function, dropout for regularization, and another fully connected layer. This structure is typical for a basic feedforward neural network or a part of a more complex network.',\n",
       "  'minimal_code': 'import torch\\nimport torch.nn as nn\\n\\nclass SimpleNetwork(nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super(SimpleNetwork, self).__init__()\\n        self.fc1 = nn.Linear(input_size, hidden_size)\\n        self.activation = nn.ReLU()\\n        self.dropout = nn.Dropout(0.5)\\n        self.fc2 = nn.Linear(hidden_size, output_size)\\n\\n    def forward(self, x):\\n        out = self.fc1(x)\\n        out = self.activation(out)\\n        out = self.dropout(out)\\n        out = self.fc2(out)\\n        return out\\n\\n# Example usage\\nmodel = SimpleNetwork(input_size=10, hidden_size=20, output_size=5)\\ninput_tensor = torch.randn(1, 10)\\noutput = model(input_tensor)\\nprint(output.shape)  # Should print torch.Size([1, 5])'},\n",
       " 'node9': {'name': 'HDComputing::superpose',\n",
       "  'type': 'method',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': 'The `superpose` function combines multiple high-dimensional vectors (HVs) into a single representative HV. It does this by summing the vectors element-wise and then applying the sign function to binarize the result. This method is commonly used in Hyperdimensional Computing to aggregate information from multiple vectors.',\n",
       "  'minimal_code': 'import numpy as np\\n\\ndef superpose(hvs):\\n    return np.sign(np.sum(hvs, axis=0))'},\n",
       " 'node11': {'name': 'HDComputing::permute',\n",
       "  'type': 'method',\n",
       "  'file': 'hdcnn.py',\n",
       "  'importance': 1,\n",
       "  'file_path': 'hdcnn/hdcnn.py',\n",
       "  'edges': [],\n",
       "  'opacity': 0.7333333333333334,\n",
       "  'level': 3,\n",
       "  'summary': \"The `permute` function is a simple method that performs a circular shift on a given high-dimensional vector (HV). It uses NumPy's `roll` function to shift the elements of the vector by a specified number of positions. This operation is likely used as part of the position-based permutation in the encoding process of Hyperdimensional Computing.\",\n",
       "  'minimal_code': 'import numpy as np\\n\\ndef permute(hv, shifts=1):\\n    return np.roll(hv, shifts)'}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_dag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BreakDown on Task (When something is hard, we break it down)\n",
    "\n",
    "# 1. Construct step-wise PNG\n",
    "#    - transparency changing function\n",
    "#    - growing strategy (code-based subnodes as importance score)\n",
    "# 2. Animate into GIF (not perfect due to scale disconnection)\n",
    "\n",
    "\n",
    "# 3. Agent to understand codebase with repo-DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
