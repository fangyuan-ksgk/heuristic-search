{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div align=\"left\">\n",
    "  <img src=\"img/abstract.png\" width=\"400\" alt=\"Funny little diagram\">\n",
    "  <p><em> Evolve nodes, evolve plans, and learn from the best performing ones.</em></p>\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "</em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Node Initialization (Refactoring ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_async_vllm_endpoint\n",
    "import os \n",
    "\n",
    "# Unlimited LLM endpoints\n",
    "endpoint_id = \"vllm-8sz1f7zg7oy0ui\"\n",
    "api_key = os.environ[\"RUNPOD_API_KEY\"]\n",
    "get_endpoint_response = get_async_vllm_endpoint(endpoint_id, api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "# Code + Compilor Task\n",
    "# mp = MetaPrompt(\"Search for age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.CODE)\n",
    "# Prompt + LLM Task\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "test_inputs = [c[0] for c in test_cases]\n",
    "\n",
    "node = EvolNode(mp, None, None, get_response=get_endpoint_response, test_cases=test_cases) # setting manual test cases\n",
    "\n",
    "node.evolve(\"i1\", replace=True, batch_size=20, num_runs=2, print_summary=True) # Scale up batch size\n",
    "\n",
    "\n",
    "input_dict = {\"name\": \"Dilireba\"}\n",
    "node.get_response = get_groq_response # fast sequential inference \n",
    "output_dict = node(input_dict) # use node as a function\n",
    "print(\"Output dict: \", output_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shah.mahir/opt/anaconda3/lib/python3.12/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "2024-11-28 17:26:31.390863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load vllm class, check CUDA support and GPU RAM size\n",
      "```json\n",
      "{\n",
      "\"nodes\": [\n",
      "    {\n",
      "        \"task\": \"Download multiple files in parallel with custom retry logic, progress tracking, and error handling using Session objects\",\n",
      "        \"name\": \"parallel_download_with_retry\",\n",
      "        \"inputs\": [\"urls\", \"output_dir\", \"max_retries\", \"timeout\", \"max_workers\"],\n",
      "        \"input_types\": [\"List[str]\", \"str\", \"int\", \"int\", \"int\"], \n",
      "        \"outputs\": [\"successful_downloads\", \"failed_downloads\", \"download_stats\"],\n",
      "        \"output_types\": [\"List[str]\", \"Dict[str,str]\", \"Dict[str,Any]\"],\n",
      "        \"target\": \"Robust parallel file downloading with retries and detailed stats\",\n",
      "        \"mode\": \"CODE\",\n",
      "        \"relevant_docs\": \"Uses requests.Session() and concurrent.futures for parallel downloads. Key functions:\\n- session.mount() for custom retry adapters\\n- session.get(url, stream=True) for streaming downloads\\n- response.iter_content(chunk_size) for chunked downloads\\nExample usage:\\nsession = requests.Session()\\nretries = Retry(total=3, backoff_factor=0.1)\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nImport requirements:\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter \\nfrom urllib3.util.retry import Retry\\nfrom concurrent.futures import ThreadPoolExecutor\"\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Stream and process large files with custom header management and authentication while monitoring memory usage\", \n",
      "        \"name\": \"streaming_file_processor\",\n",
      "        \"inputs\": [\"url\", \"chunk_size\", \"auth_token\", \"custom_headers\", \"processor_func\"],\n",
      "        \"input_types\": [\"str\", \"int\", \"str\", \"Dict[str,str]\", \"Callable\"],\n",
      "        \"outputs\": [\"processed_chunks\", \"total_bytes\", \"processing_stats\"],\n",
      "        \"output_types\": [\"List[Any]\", \"int\", \"Dict[str,float]\"],\n",
      "        \"target\": \"Memory-efficient processing of large files with authentication\",\n",
      "        \"mode\": \"CODE\",\n",
      "        \"relevant_docs\": \"Uses requests streaming functionality with custom processing:\\n- requests.get(stream=True) for streaming response\\n- response.iter_content() for chunk iteration\\n- requests.auth for authentication handling\\nExample usage:\\nheaders = {'Authorization': f'Bearer {token}'}\\nwith requests.get(url, stream=True, headers=headers) as r:\\n    for chunk in r.iter_content(chunk_size=8192):\\n        process_chunk(chunk)\\nImport requirements:\\nimport requests\\nfrom requests.auth import AuthBase\"\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Create a custom transport adapter for advanced SSL configuration including certificate pinning and version control\",\n",
      "        \"name\": \"custom_ssl_adapter\",\n",
      "        \"inputs\": [\"cert_path\", \"ssl_version\", \"verify_fingerprint\", \"hostname\"],\n",
      "        \"input_types\": [\"str\", \"str\", \"str\", \"str\"],\n",
      "        \"outputs\": [\"adapter\", \"verification_info\"],\n",
      "        \"output_types\": [\"HTTPAdapter\", \"Dict[str,bool]\"],\n",
      "        \"target\": \"Enhanced SSL security with certificate pinning\",\n",
      "        \"mode\": \"CODE\", \n",
      "        \"relevant_docs\": \"Example directly from documentation with modifications:\\nimport ssl\\nfrom urllib3.poolmanager import PoolManager\\nfrom requests.adapters import HTTPAdapter\\n\\nclass CustomSSLAdapter(HTTPAdapter):\\n    def __init__(self, ssl_version=None, **kwargs):\\n        self.ssl_version = ssl_version\\n        super().__init__(**kwargs)\\n\\n    def init_poolmanager(self, connections, maxsize, block=False):\\n        self.poolmanager = PoolManager(\\n            num_pools=connections,\\n            maxsize=maxsize,\\n            block=block,\\n            ssl_version=self.ssl_version)\\n\\nUsage:\\ns = requests.Session()\\nadapter = CustomSSLAdapter(ssl_version=ssl.PROTOCOL_TLSv1_2)\\ns.mount('https://', adapter)\"\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Implement automatic request retries with exponential backoff and custom status code handling\",\n",
      "        \"name\": \"retry_with_backoff\",\n",
      "        \"inputs\": [\"url\", \"max_retries\", \"backoff_factor\", \"status_forcelist\", \"allowed_methods\"],\n",
      "        \"input_types\": [\"str\", \"int\", \"float\", \"List[int]\", \"Set[str]\"],\n",
      "        \"outputs\": [\"response\", \"retry_stats\"],\n",
      "        \"output_types\": [\"Response\", \"Dict[str,int]\"],\n",
      "        \"target\": \"Reliable request handling with smart retries\",\n",
      "        \"mode\": \"CODE\",\n",
      "        \"relevant_docs\": \"Direct example from documentation:\\nfrom urllib3.util import Retry\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter\\n\\nretries = Retry(\\n    total=3,\\n    backoff_factor=0.1,\\n    status_forcelist=[502, 503, 504],\\n    allowed_methods={'GET', 'POST'}\\n)\\n\\nUsage:\\nsession = Session()\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nresponse = session.get(url)\\n\\nThe backoff_factor creates exponentially increasing delays:\\n{backoff_factor} * (2 ** ({retry} - 1)) seconds\"\n",
      "    }\n",
      "]\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:19<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19d81b400>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:38<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19d818040>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:58<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19a86c340>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [01:17<?, ?case/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 14 column 14 (char 414)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 3) <traceback object at 0x19d81af40>\n",
      "--- Generated 0 test cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:15<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "JsonDecodeError : \n",
      "Expecting ',' delimiter: line 11 column 18 (char 366)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 4) <traceback object at 0x19d5a7800>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases: 5case [00:30,  6.16s/case]                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "unhashable type: 'dict' <traceback object at 0x19d637480>\n",
      "--- Generated 5 test cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases:   0%|                            | 0/3 [00:18<?, ?case/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "'adapter' <traceback object at 0x19d621c00>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases: 100%|████████████████████| 3/3 [00:34<00:00, 11.40s/case]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generated 3 test cases\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test cases: 5case [00:19,  3.83s/case]                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST CASE PARSING ERROR\n",
      "unhashable type: 'list' <traceback object at 0x19d621c80>\n",
      "--- Generated 5 test cases\n",
      "Uses requests.Session() and concurrent.futures for parallel downloads. Key functions:\n",
      "- session.mount() for custom retry adapters\n",
      "- session.get(url, stream=True) for streaming downloads\n",
      "- response.iter_content(chunk_size) for chunked downloads\n",
      "Example usage:\n",
      "session = requests.Session()\n",
      "retries = Retry(total=3, backoff_factor=0.1)\n",
      "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
      "Import requirements:\n",
      "from requests import Session\n",
      "from requests.adapters import HTTPAdapter \n",
      "from urllib3.util.retry import Retry\n",
      "from concurrent.futures import ThreadPoolExecutor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Download multiple files in parallel with custom retry logic, progress tracking, and error handling using Session objects\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named parallel_download_with_retry. This function should accept 5 input(s): 'urls', 'output_dir', 'max_retries', 'timeout', 'max_workers' with types List[str], str, int, int, int. The function should return 3 output(s): 'successful_downloads', 'failed_downloads', 'download_stats' with types List[str], Dict[str,str], Dict[str,Any].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: get_author_network, this fuction accept 3 input(s): 'author_name', 'max_papers', 'depth' with types str, int, int. The function return 3 output(s): 'coauthor_network', 'paper_counts', 'primary_categories' with types Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]. This function is used for Search for papers by a specific author and get their collaboration network from co-authors\\nIntuition: Search arXiv for papers by the specified author, build a network of co-authors by traversing collaborator papers up to given depth, and track paper counts and categories for each author\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Uses requests.Session() and concurrent.futures for parallel downloads. Key functions:\\n- session.mount() for custom retry adapters\\n- session.get(url, stream=True) for streaming downloads\\n- response.iter_content(chunk_size) for chunked downloads\\nExample usage:\\nsession = requests.Session()\\nretries = Retry(total=3, backoff_factor=0.1)\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nImport requirements:\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter \\nfrom urllib3.util.retry import Retry\\nfrom concurrent.futures import ThreadPoolExecutor\"]\n",
      "🏆 Best Code Performance Summary 🏆\n",
      "  ⚡ Structural fitness: 0.00\n",
      "  🎯 Functional fitness: 0.00\n",
      "  ⭐ Global fitness:     0.00\n",
      "  🔄 Compiled solutions:        1\n",
      "  ⏱️ Time breakdown:\n",
      "     :: Query time: 10.36s\n",
      "     :: Evolution time: 15.97s\n",
      "     :: Evaluation time: 0.00s\n",
      "     :: Total time: 26.33s\n",
      "\n",
      "Uses requests streaming functionality with custom processing:\n",
      "- requests.get(stream=True) for streaming response\n",
      "- response.iter_content() for chunk iteration\n",
      "- requests.auth for authentication handling\n",
      "Example usage:\n",
      "headers = {'Authorization': f'Bearer {token}'}\n",
      "with requests.get(url, stream=True, headers=headers) as r:\n",
      "    for chunk in r.iter_content(chunk_size=8192):\n",
      "        process_chunk(chunk)\n",
      "Import requirements:\n",
      "import requests\n",
      "from requests.auth import AuthBase\n",
      "[\"Stream and process large files with custom header management and authentication while monitoring memory usage\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named streaming_file_processor. This function should accept 5 input(s): 'url', 'chunk_size', 'auth_token', 'custom_headers', 'processor_func' with types str, int, str, Dict[str,str], Callable. The function should return 3 output(s): 'processed_chunks', 'total_bytes', 'processing_stats' with types List[Any], int, Dict[str,float].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: get_author_network, this fuction accept 3 input(s): 'author_name', 'max_papers', 'depth' with types str, int, int. The function return 3 output(s): 'coauthor_network', 'paper_counts', 'primary_categories' with types Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]. This function is used for Search for papers by a specific author and get their collaboration network from co-authors\\nIntuition: Search arXiv for papers by the specified author, build a network of co-authors by traversing collaborator papers up to given depth, and track paper counts and categories for each author\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Uses requests streaming functionality with custom processing:\\n- requests.get(stream=True) for streaming response\\n- response.iter_content() for chunk iteration\\n- requests.auth for authentication handling\\nExample usage:\\nheaders = {'Authorization': f'Bearer {token}'}\\nwith requests.get(url, stream=True, headers=headers) as r:\\n    for chunk in r.iter_content(chunk_size=8192):\\n        process_chunk(chunk)\\nImport requirements:\\nimport requests\\nfrom requests.auth import AuthBase\"]\n",
      "🏆 Best Code Performance Summary 🏆\n",
      "  ⚡ Structural fitness: 0.00\n",
      "  🎯 Functional fitness: 0.00\n",
      "  ⭐ Global fitness:     0.00\n",
      "  🔄 Compiled solutions:        1\n",
      "  ⏱️ Time breakdown:\n",
      "     :: Query time: 3.27s\n",
      "     :: Evolution time: 12.16s\n",
      "     :: Evaluation time: 0.01s\n",
      "     :: Total time: 15.44s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "Input data type mismatch for parameter 'auth_token'. Expected <class 'str'>, got <class 'NoneType'>\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "Input data type mismatch for parameter 'processor_func'. Expected typing.Callable, got <class 'str'>\n",
      "================================================================================\n",
      "\n",
      "Example directly from documentation with modifications:\n",
      "import ssl\n",
      "from urllib3.poolmanager import PoolManager\n",
      "from requests.adapters import HTTPAdapter\n",
      "\n",
      "class CustomSSLAdapter(HTTPAdapter):\n",
      "    def __init__(self, ssl_version=None, **kwargs):\n",
      "        self.ssl_version = ssl_version\n",
      "        super().__init__(**kwargs)\n",
      "\n",
      "    def init_poolmanager(self, connections, maxsize, block=False):\n",
      "        self.poolmanager = PoolManager(\n",
      "            num_pools=connections,\n",
      "            maxsize=maxsize,\n",
      "            block=block,\n",
      "            ssl_version=self.ssl_version)\n",
      "\n",
      "Usage:\n",
      "s = requests.Session()\n",
      "adapter = CustomSSLAdapter(ssl_version=ssl.PROTOCOL_TLSv1_2)\n",
      "s.mount('https://', adapter)\n",
      "[\"Create a custom transport adapter for advanced SSL configuration including certificate pinning and version control\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named custom_ssl_adapter. This function should accept 4 input(s): 'cert_path', 'ssl_version', 'verify_fingerprint', 'hostname' with types str, str, str, str. The function should return 2 output(s): 'adapter', 'verification_info' with types HTTPAdapter, Dict[str,bool].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: get_author_network, this fuction accept 3 input(s): 'author_name', 'max_papers', 'depth' with types str, int, int. The function return 3 output(s): 'coauthor_network', 'paper_counts', 'primary_categories' with types Dict[str, List[str]], Dict[str, int], Dict[str, List[str]]. This function is used for Search for papers by a specific author and get their collaboration network from co-authors\\nIntuition: Search arXiv for papers by the specified author, build a network of co-authors by traversing collaborator papers up to given depth, and track paper counts and categories for each author\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Example directly from documentation with modifications:\\nimport ssl\\nfrom urllib3.poolmanager import PoolManager\\nfrom requests.adapters import HTTPAdapter\\n\\nclass CustomSSLAdapter(HTTPAdapter):\\n    def __init__(self, ssl_version=None, **kwargs):\\n        self.ssl_version = ssl_version\\n        super().__init__(**kwargs)\\n\\n    def init_poolmanager(self, connections, maxsize, block=False):\\n        self.poolmanager = PoolManager(\\n            num_pools=connections,\\n            maxsize=maxsize,\\n            block=block,\\n            ssl_version=self.ssl_version)\\n\\nUsage:\\ns = requests.Session()\\nadapter = CustomSSLAdapter(ssl_version=ssl.PROTOCOL_TLSv1_2)\\ns.mount('https://', adapter)\"]\n",
      "🏆 Best Code Performance Summary 🏆\n",
      "  ⚡ Structural fitness: 0.00\n",
      "  🎯 Functional fitness: 0.00\n",
      "  ⭐ Global fitness:     0.00\n",
      "  🔄 Compiled solutions:        1\n",
      "  ⏱️ Time breakdown:\n",
      "     :: Query time: 3.22s\n",
      "     :: Evolution time: 17.71s\n",
      "     :: Evaluation time: 0.10s\n",
      "     :: Total time: 21.04s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "[Errno 2] No such file or directory\n",
      "[Errno 2] No such file or directory\n",
      "[Errno 2] No such file or directory\n",
      "================================================================================\n",
      "\n",
      "Direct example from documentation:\n",
      "from urllib3.util import Retry\n",
      "from requests import Session\n",
      "from requests.adapters import HTTPAdapter\n",
      "\n",
      "retries = Retry(\n",
      "    total=3,\n",
      "    backoff_factor=0.1,\n",
      "    status_forcelist=[502, 503, 504],\n",
      "    allowed_methods={'GET', 'POST'}\n",
      ")\n",
      "\n",
      "Usage:\n",
      "session = Session()\n",
      "session.mount('https://', HTTPAdapter(max_retries=retries))\n",
      "response = session.get(url)\n",
      "\n",
      "The backoff_factor creates exponentially increasing delays:\n",
      "{backoff_factor} * (2 ** ({retry} - 1)) seconds\n",
      "[\"Implement automatic request retries with exponential backoff and custom status code handling\\nFirst, describe your new algorithm and main steps in one sentence. The description must be inside a brace. Next, implement it in Python as a function named retry_with_backoff. This function should accept 5 input(s): 'url', 'max_retries', 'backoff_factor', 'status_forcelist', 'allowed_methods' with types str, int, float, List[int], Set[str]. The function should return 2 output(s): 'response', 'retry_stats' with types Response, Dict[str,int].Make sure to include type hints in your function signature.Available functions for use:\\nFunction name: download_large_file, this fuction accept 4 input(s): 'url', 'output_path', 'chunk_size', 'timeout' with types str, str, int, int. The function return 4 output(s): 'success', 'file_size', 'download_time', 'encoding' with types bool, int, float, str. This function is used for Download a large file in chunks while displaying progress, validating response, and handling various content encodings\\nIntuition: Downloads a large file in chunks by sending HTTP requests with range headers, validates response headers and content encoding, displays progress bar, and handles different compression methods while measuring download time\\nFitness: 1.00\\nFunction name: batch_arxiv_download, this fuction accept 5 input(s): 'query', 'max_papers', 'output_dir', 'categories', 'date_order' with types str, int, str, List[str], bool. The function return 3 output(s): 'downloaded_papers', 'metadata_list', 'failed_downloads' with types List[str], List[Dict], List[str]. This function is used for Search and download multiple arXiv papers based on a query, with automatic PDF download and metadata extraction\\nIntuition: None\\nFitness: 1.00\\nFunction name: search_papers_paginated, this fuction accept 6 input(s): 'query', 'max_results', 'page_size', 'sort_by', 'from_date', 'to_date' with types str, int, int, SortCriterion, datetime, datetime. The function return 3 output(s): 'papers', 'total_results', 'metadata_list' with types List[Result], int, List[Dict[str,Any]]. This function is used for Search arXiv for papers matching a query and time period, sort by relevance, and return paginated results with full metadata\\nIntuition: Search arXiv papers by query with date filtering, sort results by relevance/date, paginate the output, and return both papers and full metadata using the arxiv API client with proper rate limiting\\nFitness: 0.00\\nFunction name: download_specific_paper, this fuction accept 3 input(s): 'paper_id', 'output_dir', 'filename' with types str, str, str. The function return 2 output(s): 'success', 'file_path' with types bool, str. This function is used for Example from docs: Download paper by ID with custom filename\\nIntuition: This algorithm downloads a specific research paper from arXiv using its ID and saves it with a custom filename by utilizing the arXiv API client to fetch paper metadata and download the PDF to the specified output directory.\\nFitness: 1.00\\nFunction name: search_google, this fuction accept 1 input(s): 'query' with types str. The function return 1 output(s): 'result' with types str. This function is used for Search Google for result\\nIntuition: Search google for top search results\\nFitness: 1.00If you intend to use this function, put the function calls into your generated function (assume the functions are already implemented). Do not use it in a separate code block with your generated function.\\n\\nIdea: Direct example from documentation:\\nfrom urllib3.util import Retry\\nfrom requests import Session\\nfrom requests.adapters import HTTPAdapter\\n\\nretries = Retry(\\n    total=3,\\n    backoff_factor=0.1,\\n    status_forcelist=[502, 503, 504],\\n    allowed_methods={'GET', 'POST'}\\n)\\n\\nUsage:\\nsession = Session()\\nsession.mount('https://', HTTPAdapter(max_retries=retries))\\nresponse = session.get(url)\\n\\nThe backoff_factor creates exponentially increasing delays:\\n{backoff_factor} * (2 ** ({retry} - 1)) seconds\"]\n",
      "🏆 Best Code Performance Summary 🏆\n",
      "  ⚡ Structural fitness: 0.00\n",
      "  🎯 Functional fitness: 0.00\n",
      "  ⭐ Global fitness:     0.00\n",
      "  🔄 Compiled solutions:        1\n",
      "  ⏱️ Time breakdown:\n",
      "     :: Query time: 3.11s\n",
      "     :: Evolution time: 8.95s\n",
      "     :: Evaluation time: 0.00s\n",
      "     :: Total time: 12.06s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "Input data type mismatch for parameter 'allowed_methods'. Expected typing.Set[str], got <class 'list'>\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from methods.evolnode import nodes_from_api\n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "\n",
    "nodes = nodes_from_api(\"https://requests.readthedocs.io/en/latest/user/advanced/\", get_response=get_claude_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import brotli\n",
      "from concurrent.futures import ThreadPoolExecutor\n",
      "import gzip\n",
      "from pathlib import Path\n",
      "import requests\n",
      "from requests import Session\n",
      "from requests.adapters import HTTPAdapter\n",
      "import time\n",
      "from tqdm import tqdm\n",
      "from typing import Tuple\n",
      "from typing import List, Dict, Any\n",
      "from urllib3.util.retry import Retry\n",
      "import zlib\n",
      "\n",
      "\n",
      "def download_large_file(url: str, output_path: str, chunk_size: int=8192,\n",
      "    timeout: int=30) ->Tuple[bool, int, float, str]:\n",
      "    try:\n",
      "        start_time = time.time()\n",
      "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
      "        head_response = requests.head(url, headers=headers, timeout=timeout)\n",
      "        file_size = int(head_response.headers.get('content-length', 0))\n",
      "        content_encoding = head_response.headers.get('content-encoding',\n",
      "            'identity')\n",
      "        progress = tqdm(total=file_size, unit='iB', unit_scale=True)\n",
      "        with requests.get(url, headers=headers, stream=True, timeout=timeout\n",
      "            ) as response:\n",
      "            response.raise_for_status()\n",
      "            with open(output_path, 'wb') as f:\n",
      "                decompressor = None\n",
      "                if content_encoding == 'gzip':\n",
      "                    decompressor = gzip.decompress\n",
      "                elif content_encoding == 'br':\n",
      "                    decompressor = brotli.decompress\n",
      "                elif content_encoding == 'deflate':\n",
      "                    decompressor = zlib.decompress\n",
      "                for chunk in response.iter_content(chunk_size=chunk_size):\n",
      "                    if chunk:\n",
      "                        if decompressor:\n",
      "                            chunk = decompressor(chunk)\n",
      "                        f.write(chunk)\n",
      "                        progress.update(len(chunk))\n",
      "        progress.close()\n",
      "        download_time = time.time() - start_time\n",
      "        if file_size > 0:\n",
      "            actual_size = os.path.getsize(output_path)\n",
      "            if actual_size != file_size and content_encoding == 'identity':\n",
      "                return False, file_size, download_time, content_encoding\n",
      "        return True, file_size, download_time, content_encoding\n",
      "    except Exception as e:\n",
      "        print(f'Error downloading file: {str(e)}')\n",
      "        return False, 0, 0.0, ''\n",
      "\n",
      "\n",
      "def parallel_download_with_retry(urls: List[str], output_dir: str,\n",
      "    max_retries: int=3, timeout: int=30, max_workers: int=5) ->tuple[List[\n",
      "    str], Dict[str, str], Dict[str, Any]]:\n",
      "    successful_downloads: List[str] = []\n",
      "    failed_downloads: Dict[str, str] = {}\n",
      "    download_stats: Dict[str, Any] = {'total_files': len(urls),\n",
      "        'total_size': 0, 'total_time': 0, 'success_rate': 0}\n",
      "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
      "    session = Session()\n",
      "    retry_strategy = Retry(total=max_retries, backoff_factor=0.1,\n",
      "        status_forcelist=[500, 502, 503, 504])\n",
      "    adapter = HTTPAdapter(max_retries=retry_strategy)\n",
      "    session.mount('http://', adapter)\n",
      "    session.mount('https://', adapter)\n",
      "    start_time = time.time()\n",
      "\n",
      "    def download_single_file(url: str) ->tuple[bool, str, Dict[str, Any]]:\n",
      "        try:\n",
      "            filename = url.split('/')[-1]\n",
      "            output_path = str(Path(output_dir) / filename)\n",
      "            success, file_size, download_time, _ = download_large_file(url=\n",
      "                url, output_path=output_path, chunk_size=8192, timeout=timeout)\n",
      "            if success:\n",
      "                return True, url, {'file_size': file_size, 'download_time':\n",
      "                    download_time}\n",
      "            return False, url, {'error': 'Download failed'}\n",
      "        except Exception as e:\n",
      "            return False, url, {'error': str(e)}\n",
      "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
      "        futures = [executor.submit(download_single_file, url) for url in urls]\n",
      "        for future in tqdm(futures, total=len(urls), desc='Downloading files'):\n",
      "            success, url, stats = future.result()\n",
      "            if success:\n",
      "                successful_downloads.append(url)\n",
      "                download_stats['total_size'] += stats['file_size']\n",
      "                download_stats['total_time'] += stats['download_time']\n",
      "            else:\n",
      "                failed_downloads[url] = stats['error']\n",
      "    download_stats['success_rate'] = len(successful_downloads) / len(urls\n",
      "        ) * 100\n",
      "    download_stats['total_time'] = time.time() - start_time\n",
      "    return successful_downloads, failed_downloads, download_stats\n"
     ]
    }
   ],
   "source": [
    "print(nodes[0][0].evol.codes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"img/Project-Nirvana-evolve.gif\" width=\"500\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> Evolve a population of nodes. </em></p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 20/20 [00:16<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 16.71s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 34/34 [00:28<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 28.46s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 0it [00:00, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 0.00s, 0 errors\n",
      "🏆 Best Code Performance Summary 🏆\n",
      "  ⚡ Structural fitness: 0.33\n",
      "  🎯 Functional fitness: 1.00\n",
      "  ⭐ Global fitness:     0.67\n",
      "  🔄 Batch size:        17\n",
      "  ⏱️ Time taken: 85.64 seconds\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 0: Fitness: 16.7%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'age': 87}, Error message: \n",
      "Value mismatch for key age: 87 != 32\n",
      "\n",
      "\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 33}, Error message: \n",
      "Value mismatch for key age: 33 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 1: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- No JSON structure found in the provided text.\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 2: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 3 (char 2)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 3: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 31}, Error message: \n",
      "Value mismatch for key age: 31 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 4: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 5: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 25}, Error message: \n",
      "Value mismatch for key age: 25 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 6: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 22}, Error message: \n",
      "Value mismatch for key age: 22 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 7: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 8: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting property name enclosed in double quotes: line 1 column 2 (char 1)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 9: Fitness: 8.3%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': None}, Error message: Value None can't be converted into integer\n",
      "Value mismatch for key age: None != 26\n",
      "\n",
      "\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 10: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'ChengXiao'}, prediction is not aligned with expected output, Expected: {'age': 26} Predicted: {'age': 23}, Error message: \n",
      "Value mismatch for key age: 23 != 26\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 11: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting ',' delimiter: line 5 column 4 (char 51)AstLiteralError : \n",
      "'{' was never closed (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 12: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 13: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 14: Fitness: 41.7%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Input: {'name': 'Dilireba'}, prediction is not aligned with expected output, Expected: {'age': 32} Predicted: {'age': 31}, Error message: \n",
      "Value mismatch for key age: 31 != 32\n",
      "\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 15: Fitness: 33.3%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Failed to parse LLM response -- JsonDecodeError : \n",
      "Expecting value: line 1 column 1 (char 0)AstLiteralError : \n",
      "invalid syntax (<unknown>, line 1)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 16: Fitness: 66.7%\n",
      "================================================================================\n",
      "\n",
      "Based on the provided information, I can analyze the effectiveness of the current evolution strategy:\n",
      "\n",
      "1. Fitness Improvement:\n",
      "- The best fitness from the initial population was 1.0\n",
      "- The offspring generated has a fitness of approximately 0.67\n",
      "- This actually represents a decrease in fitness, which suggests that the current evolution strategy might not be optimal\n",
      "\n",
      "2. Implementation Improvements:\n",
      "The offspring shows several significant implementation improvements over the parent solutions:\n",
      "\n",
      "Positive Changes:\n",
      "1. Added concrete Google Search functionality:\n",
      "   - Implemented `_search_google()` and `search_google()` functions\n",
      "   - Uses Serper API for real Google search results\n",
      "   - Includes proper error handling and API connection management\n",
      "\n",
      "2. Better Type Hints:\n",
      "   - Added proper typing annotations (Dict, Any, str)\n",
      "   - More professional and maintainable code structure\n",
      "\n",
      "3. Enhanced Functionality:\n",
      "   - Actually fetches real search results instead of just suggesting to do so\n",
      "   - Processes and formats search results before including them in the prompt\n",
      "   - More comprehensive solution that goes beyond just prompt generation\n",
      "\n",
      "Areas of Concern:\n",
      "1. Lower Fitness Score:\n",
      "   - The decrease in fitness (1.0 to 0.67) suggests that while the implementation is more complex, it might not align perfectly with the desired objectives\n",
      "   - The evolution strategy might need adjustment to better balance implementation improvements with fitness requirements\n",
      "\n",
      "2. Complexity Trade-off:\n",
      "   - The solution has become more complex, which might affect its reliability and maintainability\n",
      "   - Added external dependencies (Serper API) introduce potential points of failure\n",
      "\n",
      "Recommendations for Improvement:\n",
      "1. Adjust the fitness function to better reward practical implementations while maintaining simplicity\n",
      "2. Consider a hybrid approach that combines the simplicity of the original solutions with the functionality of the new implementation\n",
      "3. Implement a more gradual evolution strategy that makes smaller, incremental improvements\n",
      "4. Consider maintaining a larger population size to explore more diverse solutions\n",
      "\n",
      "The current evolution strategy shows promise in terms of implementation improvements but needs refinement to maintain or improve fitness scores while adding functionality.\n",
      "Population size: 3\n",
      "Best Fitness: 1.0\n",
      "Information on the best 2 individuals:\n",
      "Individual 1:\n",
      "No.1:\n",
      "[APPROACH]: To generate a prompt for an AI to find the age of a celebrity, I will incorporate a natural language approach, leveraging the search engine's ability to provide relevant information, while also specifying the required output format in the prompt for clarity.\n",
      "[PROMPT FUNCTION]: def generate_prompt(name):\n",
      "    \"\"\"\n",
      "    Generate a prompt to guide an AI in finding the age of a celebrity.\n",
      "\n",
      "    Parameters:\n",
      "    name (str): The name of the celebrity.\n",
      "\n",
      "    Returns:\n",
      "    str: A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    prompt = (\n",
      "        f\"Given the input '{name}', please provide a JSON-style response with the following structure: \"\n",
      "        )\n",
      "    prompt += (\n",
      "        \"{'age': int(<age>)}}, where <age> is the age of the celebrity in years.\"\n",
      "        )\n",
      "    prompt += (\n",
      "        ' The response should be based on the latest available information from top search results.'\n",
      "        )\n",
      "    return prompt\n",
      "\n",
      "\n",
      "Individual 2:\n",
      "No.1:\n",
      "[APPROACH]: To calculate the age of a celebrity, we need to first search for their birth date or age online and then perform date arithmetic to find their current age. We can utilize this reasoning by using the search engine to find the relevant information and then utilizing AI to process the information and calculate the age.\n",
      "[PROMPT FUNCTION]: def generate_prompt(name: str) ->str:\n",
      "    \"\"\"\n",
      "    Generates a prompt to guide an AI in calculating the age of a celebrity.\n",
      "    \n",
      "    Args:\n",
      "    name (str): The name of the celebrity.\n",
      "    \n",
      "    Returns:\n",
      "    str: A string containing the final prompt for the AI.\n",
      "    \"\"\"\n",
      "    prompt = (\"Given the name '\" + name +\n",
      "        \"', use the search engine to find the birth date of \" + name +\n",
      "        ' and calculate their current age.')\n",
      "    return ('Search google for result ' + prompt +\n",
      "        \", format the output as a JSON-style dictionary: {'age': int(...)}\")\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Population building phase ... \n",
    "from methods.llm import get_groq_response, get_claude_response\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "from methods.population import Evolution\n",
    "\n",
    "mp = MetaPrompt(\"Get the age of a celebrity.\", \"get_celeb_age\", [\"name\"], [\"age\"], [\"str\"], [\"int\"], PromptMode.PROMPT) # \n",
    "\n",
    "test_cases = [\n",
    "    ({\"name\": \"Dilireba\"}, {\"age\": 32}),\n",
    "    ({\"name\": \"ChengXiao\"}, {\"age\": 26})\n",
    "]\n",
    "\n",
    "evo = Evolution(pop_size=20, meta_prompt=mp, get_response=get_endpoint_response, \n",
    "                test_cases=test_cases, max_attempts=3, num_eval_runs=2,\n",
    "                load=True)\n",
    "\n",
    "strategies = [\"m2\"] # [\"i1\", \"i1\", \"m2\", \"e2\"]\n",
    "evo.get_offspring(strategies)\n",
    "\n",
    "evo.chat(\"How effective is the current evolution strategy? What improvement has it made in terms of fitness, and in terms of the implementation?\",\n",
    "         get_claude_response) \n",
    "\n",
    "# code-based check \n",
    "print(evo.population_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "</div>\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://github.com/user-attachments/assets/af98faeb-66d6-4278-af86-67d668d1954e\" width=\"900\" alt=\"Fourier reconstruction convergence\">\n",
    "  <p><em> Plan, and evolve the plans. </em></p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-29 20:23:27.466926: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     :: Query time: 18.37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|█████████████████| 100/100 [01:23<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 83.77s, 0 errors\n",
      "ERROR PARSING CODE\n",
      "ERROR PARSING CODE\n",
      "     :: Evolution time: 84.80s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/999 [00:00<?, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 33.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 41.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 29.3kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 48.5kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 51.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 52.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 46.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.7kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.2kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 27.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 25.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.8kiB/s]\n",
      "Downloading files:  35%|████████▎               | 19/55 [00:00<00:00, 87.41it/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.4kiB/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 46.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.6kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 52.7kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 45.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 12.7kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading files:  51%|████████████▏           | 28/55 [00:00<00:00, 66.05it/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 12.8kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 35.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 10.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 10.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 9.43kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 22.3kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 16.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 24.7kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 12.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.8kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 23.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 35.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 29.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.5kiB/s]\n",
      "Downloading files:  80%|███████████████████▏    | 44/55 [00:00<00:00, 67.03it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 33.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 41.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 51.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 64.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 53.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 70.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 62.7kiB/s]\n",
      "Downloading files: 100%|████████████████████████| 55/55 [00:00<00:00, 74.69it/s]\n",
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: Invalid URL '2411.11317v1': No scheme supplied. Perhaps you meant https://2411.11317v1?\n",
      "Error downloading file: Invalid URL '2411.10983v1': No scheme supplied. Perhaps you meant https://2411.10983v1?\n",
      "Error downloading file: Invalid URL '2411.13585v1': No scheme supplied. Perhaps you meant https://2411.13585v1?\n",
      "Error downloading file: Invalid URL '2411.10995v1': No scheme supplied. Perhaps you meant https://2411.10995v1?\n",
      "Error downloading file: Invalid URL '2411.11976v1': No scheme supplied. Perhaps you meant https://2411.11976v1?\n",
      "Error downloading file: Invalid URL '2411.14463v1': No scheme supplied. Perhaps you meant https://2411.14463v1?\n",
      "Error downloading file: Invalid URL '2411.11449v1': No scheme supplied. Perhaps you meant https://2411.11449v1?\n",
      "Error downloading file: Invalid URL '2411.11478v2': No scheme supplied. Perhaps you meant https://2411.11478v2?\n",
      "Error downloading file: Invalid URL '2411.11795v1': No scheme supplied. Perhaps you meant https://2411.11795v1?\n",
      "Error downloading file: Invalid URL '2411.11235v1': No scheme supplied. Perhaps you meant https://2411.11235v1?\n",
      "Error downloading file: Invalid URL '2411.11494v1': No scheme supplied. Perhaps you meant https://2411.11494v1?\n",
      "Error downloading file: Invalid URL '2411.12128v3': No scheme supplied. Perhaps you meant https://2411.12128v3?\n",
      "Error downloading file: Invalid URL '2411.14472v1': No scheme supplied. Perhaps you meant https://2411.14472v1?\n",
      "Error downloading file: Invalid URL '2411.11752v1': No scheme supplied. Perhaps you meant https://2411.11752v1?\n",
      "Error downloading file: Invalid URL '2411.11835v1': No scheme supplied. Perhaps you meant https://2411.11835v1?\n",
      "Error downloading file: Invalid URL '2411.11221v1': No scheme supplied. Perhaps you meant https://2411.11221v1?\n",
      "Error downloading file: Invalid URL '2411.11774v1': No scheme supplied. Perhaps you meant https://2411.11774v1?\n",
      "Error downloading file: Invalid URL '2411.11451v1': No scheme supplied. Perhaps you meant https://2411.11451v1?\n",
      "Error downloading file: Invalid URL '2411.11786v1': No scheme supplied. Perhaps you meant https://2411.11786v1?\n",
      "Error downloading file: Invalid URL '2411.12090v1': No scheme supplied. Perhaps you meant https://2411.12090v1?\n",
      "Error downloading file: Invalid URL '2411.17712v1': No scheme supplied. Perhaps you meant https://2411.17712v1?\n",
      "Error downloading file: Invalid URL '2411.11910v2': No scheme supplied. Perhaps you meant https://2411.11910v2?\n",
      "Error downloading file: Invalid URL '2411.11940v2': No scheme supplied. Perhaps you meant https://2411.11940v2?\n",
      "Error downloading file: Invalid URL '2411.10939v1': No scheme supplied. Perhaps you meant https://2411.10939v1?\n",
      "Error downloading file: Invalid URL '2411.11045v1': No scheme supplied. Perhaps you meant https://2411.11045v1?\n",
      "Error downloading file: Invalid URL '2411.11783v1': No scheme supplied. Perhaps you meant https://2411.11783v1?\n",
      "Error downloading file: Invalid URL '2411.11145v1': No scheme supplied. Perhaps you meant https://2411.11145v1?\n",
      "Error downloading file: Invalid URL '2411.11173v1': No scheme supplied. Perhaps you meant https://2411.11173v1?\n",
      "Error downloading file: Invalid URL '2411.11232v1': No scheme supplied. Perhaps you meant https://2411.11232v1?\n",
      "Error downloading file: Invalid URL '2411.11260v1': No scheme supplied. Perhaps you meant https://2411.11260v1?\n",
      "Error downloading file: Invalid URL '2411.11613v2': No scheme supplied. Perhaps you meant https://2411.11613v2?\n",
      "Error downloading file: Invalid URL '2411.11736v1': No scheme supplied. Perhaps you meant https://2411.11736v1?\n",
      "Error downloading file: Invalid URL '2411.11635v1': No scheme supplied. Perhaps you meant https://2411.11635v1?\n",
      "Error downloading file: Invalid URL '2411.17713v1': No scheme supplied. Perhaps you meant https://2411.17713v1?\n",
      "Error downloading file: Invalid URL '2411.12010v1': No scheme supplied. Perhaps you meant https://2411.12010v1?\n",
      "Error downloading file: Invalid URL '2411.11479v1': No scheme supplied. Perhaps you meant https://2411.11479v1?\n",
      "Error downloading file: Invalid URL '2411.11258v1': No scheme supplied. Perhaps you meant https://2411.11258v1?\n",
      "Error downloading file: Invalid URL '2411.11055v1': No scheme supplied. Perhaps you meant https://2411.11055v1?\n",
      "Error downloading file: Invalid URL '2411.10918v1': No scheme supplied. Perhaps you meant https://2411.10918v1?\n",
      "Error downloading file: Invalid URL '2411.11275v1': No scheme supplied. Perhaps you meant https://2411.11275v1?\n",
      "Error downloading file: Invalid URL '2411.11123v1': No scheme supplied. Perhaps you meant https://2411.11123v1?\n",
      "Error downloading file: Invalid URL '2411.11362v1': No scheme supplied. Perhaps you meant https://2411.11362v1?\n",
      "Error downloading file: Invalid URL '2411.11285v1': No scheme supplied. Perhaps you meant https://2411.11285v1?\n",
      "Error downloading file: Invalid URL '2411.11280v1': No scheme supplied. Perhaps you meant https://2411.11280v1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading files: 100%|████████████████████| 55/55 [00:00<00:00, 185887.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: Invalid URL '2411.11434v2': No scheme supplied. Perhaps you meant https://2411.11434v2?Error downloading file: Invalid URL '2411.11396v2': No scheme supplied. Perhaps you meant https://2411.11396v2?\n",
      "Error downloading file: Invalid URL '2411.11548v1': No scheme supplied. Perhaps you meant https://2411.11548v1?\n",
      "Error downloading file: Invalid URL '2411.11389v1': No scheme supplied. Perhaps you meant https://2411.11389v1?\n",
      "Error downloading file: Invalid URL '2411.11844v2': No scheme supplied. Perhaps you meant https://2411.11844v2?\n",
      "Error downloading file: Invalid URL '2411.14473v1': No scheme supplied. Perhaps you meant https://2411.14473v1?\n",
      "\n",
      "Error downloading file: Invalid URL '2411.15175v1': No scheme supplied. Perhaps you meant https://2411.15175v1?\n",
      "Error downloading file: Invalid URL '2411.14467v1': No scheme supplied. Perhaps you meant https://2411.14467v1?\n",
      "Error downloading file: Invalid URL '2411.11192v1': No scheme supplied. Perhaps you meant https://2411.11192v1?\n",
      "Error downloading file: Invalid URL '2411.11648v1': No scheme supplied. Perhaps you meant https://2411.11648v1?\n",
      "Error downloading file: Invalid URL '2411.11070v1': No scheme supplied. Perhaps you meant https://2411.11070v1?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading files: 0it [00:00, ?it/s]\n",
      "Downloading files: 0it [00:00, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 45.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 41.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 46.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 33.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 39.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 48.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.7kiB/s]\n",
      "Downloading files:   7%|█▊                       | 4/55 [00:00<00:01, 39.39it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 12.4kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 12.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 41.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 60.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 43.6kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 54.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 52.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 29.5kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 25.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 26.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.5kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 39.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 45.9kiB/s]\n",
      "Downloading files:  40%|█████████▌              | 22/55 [00:00<00:00, 63.76it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 30.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 43.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 29.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 45.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.7kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.6kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 28.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 23.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.5kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 28.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 44.9kiB/s]\n",
      "Downloading files:  58%|█████████████▉          | 32/55 [00:00<00:00, 70.71it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 26.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 26.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.3kiB/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 43.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 52.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 41.6kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 50.6kiB/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 48.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 51.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 12.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 43.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 48.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 51.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 55.3kiB/s]\n",
      "Downloading files: 100%|████████████████████████| 55/55 [00:00<00:00, 69.92it/s]\n",
      "0it [00:00, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 33.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 33.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 28.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 31.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 28.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 25.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 24.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 23.6kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.2kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 43.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 43.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 31.7kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 31.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 35.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 35.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 35.6kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.7kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 39.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.2kiB/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.9kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 11.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 11.4kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 24.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 5.37kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 8.57kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 43.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 7.07kiB/s]\n",
      "Downloading files:  40%|█████████▌              | 22/55 [00:00<00:00, 37.42it/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 33.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.3kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.8kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 35.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 24.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.8kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.5kiB/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 39.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 39.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 53.2kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 25.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.8kiB/s]\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 47.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 55.5kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 39.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 15.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 15.2kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 13.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 14.7kiB/s]\n",
      "Downloading files:  96%|███████████████████████▏| 53/55 [00:00<00:00, 60.28it/s]\n",
      "0it [00:00, ?it/s]\n",
      "Downloading papers: 100%|███████████████████████| 55/55 [01:07<00:00,  1.23s/it]\n",
      "Downloading files: 0it [00:00, ?it/s]\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 56.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 50.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 48.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 31.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 28.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 13.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 12.1kiB/s]\n",
      "Downloading files:  18%|████▎                   | 10/55 [00:00<00:00, 46.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 30.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 39.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 44.1kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.0kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 28.3kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 29.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 29.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 30.5kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 31.3kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 30.3kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 27.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 23.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 34.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 41.3kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 0.00/1.85k [00:00<?, ?iB/s]\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 33.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.9kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 28.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 30.3kiB/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                              | 0.00/1.85k [00:00<?, ?iB/s]\u001b[A\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 45.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 44.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 13.0kiB/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 11.2kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 10.4kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 10.5kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 27.1kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 22.9kiB/s]\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 32.5kiB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 36.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 45.6kiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 42.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 49.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 65.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 66.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 70.0kiB/s]\n",
      "Downloading files: 100%|████████████████████████| 55/55 [00:00<00:00, 64.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n",
      "Error downloading file: name 'os' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/shah.mahir/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/shah.mahir/opt/anaconda3/lib/python3.12/multiprocessing/spawn.py\", line 130, in _main\n",
      "    preparation_data = reduction.pickle.load(from_parent)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "EOFError: Ran out of input\n",
      "Downloading files:   0%|                                 | 0/55 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 117kiB/s]\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 145kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 94.5kiB/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 54.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 46.7kiB/s]\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 104kiB/s]\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 114kiB/s]\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 102kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 84.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 86.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 83.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 92.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 91.1kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 65.5kiB/s]\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 104kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 75.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 99.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 96.7kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 64.6kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 98.3kiB/s]\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 108kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 99.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 13.7kiB/s]\n",
      "Downloading files:  36%|████████▋               | 20/55 [00:00<00:00, 60.39it/s]\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 75.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 74.8kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 37.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.5kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 72.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 76.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 81.5kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 76.4kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 86.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 79.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 88.0kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 44.2kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 96.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 71.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 72.3kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 40.8kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 77.9kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 68.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 68.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 66.1kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 48.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 51.1kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 83.0kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 55.7kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 57.9kiB/s]\n",
      "\n",
      "\u001b[A\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 67.3kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 54.0kiB/s]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 38.1kiB/s]\n",
      "100%|██████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 109kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 48.2kiB/s]\n",
      "100%|█████████████████████████████████████| 1.85k/1.85k [00:00<00:00, 21.4kiB/s]\n",
      "Downloading files: 100%|████████████████████████| 55/55 [00:00<00:00, 67.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2411.10983v1 saved successfully.\n",
      "Paper 2411.11976v1 saved successfully.\n",
      "Paper 2411.13585v1 saved successfully.\n",
      "Paper 2411.10995v1 saved successfully.\n",
      "Paper 2411.14463v1 saved successfully.\n",
      "Paper 2411.11449v1 saved successfully.\n",
      "Paper 2411.11478v2 saved successfully.\n",
      "Paper 2411.11774v1 saved successfully.\n",
      "Paper 2411.11795v1 saved successfully.\n",
      "Paper 2411.11235v1 saved successfully.\n",
      "Paper 2411.11494v1 saved successfully.\n",
      "Paper 2411.12128v3 saved successfully.\n",
      "Paper 2411.14472v1 saved successfully.\n",
      "Paper 2411.11752v1 saved successfully.\n",
      "Paper 2411.11835v1 saved successfully.\n",
      "Paper 2411.11221v1 saved successfully.\n",
      "Paper 2411.11613v2 saved successfully.\n",
      "Paper 2411.17712v1 saved successfully.\n",
      "Paper 2411.11451v1 saved successfully.\n",
      "Paper 2411.11786v1 saved successfully.\n",
      "Paper 2411.12090v1 saved successfully.\n",
      "Paper 2411.11783v1 saved successfully.\n",
      "Paper 2411.11910v2 saved successfully.\n",
      "Paper 2411.11940v2 saved successfully.\n",
      "Paper 2411.10939v1 saved successfully.\n",
      "Paper 2411.11045v1 saved successfully.\n",
      "Paper 2411.11123v1 saved successfully.\n",
      "Paper 2411.11145v1 saved successfully.\n",
      "Paper 2411.11173v1 saved successfully.\n",
      "Paper 2411.11232v1 saved successfully.\n",
      "Paper 2411.11260v1 saved successfully.\n",
      "Paper 2411.11479v1 saved successfully.\n",
      "Paper 2411.11635v1 saved successfully.\n",
      "Paper 2411.11736v1 saved successfully.\n",
      "Paper 2411.12010v1 saved successfully.\n",
      "Paper 2411.17713v1 saved successfully.\n",
      "Paper 2411.10918v1 saved successfully.\n",
      "Paper 2411.11055v1 saved successfully.\n",
      "Paper 2411.11258v1 saved successfully.\n",
      "Paper 2411.11275v1 saved successfully.\n",
      "Paper 2411.11280v1 saved successfully.\n",
      "Paper 2411.11285v1 saved successfully.\n",
      "Paper 2411.11362v1 saved successfully.\n",
      "Paper 2411.11389v1 saved successfully.\n",
      "Paper 2411.11396v2 saved successfully.\n",
      "Paper 2411.11434v2 saved successfully.\n",
      "Paper 2411.11548v1 saved successfully.\n",
      "Paper 2411.11844v2 saved successfully.\n",
      "Paper 2411.14473v1 saved successfully.\n",
      "Paper 2411.15175v1 saved successfully.\n",
      "Paper 2411.11070v1 saved successfully.\n",
      "Paper 2411.11192v1 saved successfully.\n",
      "Paper 2411.14467v1 saved successfully.\n",
      "Paper 2411.11648v1 saved successfully.\n",
      "\n",
      "Code 10 outputs:\n",
      "Test 0: {'num_papers': (0, 0)}\n",
      "\n",
      "Code 35 outputs:\n",
      "Test 0: {'num_papers': (55, 0)}\n",
      "\n",
      "Code 36 outputs:\n",
      "Test 0: {'num_papers': (0, 1)}\n",
      "\n",
      "Code 66 outputs:\n",
      "Test 0: {'num_papers': (0, 0)}\n",
      "\n",
      "Code 71 outputs:\n",
      "Test 0: {'num_papers': (0, 1)}\n"
     ]
    }
   ],
   "source": [
    "from methods.llm import get_claude_response, get_groq_response\n",
    "from methods.diagram import visualize_plan_dict\n",
    "from methods.meta_prompt import MetaPlan, PromptMode, MetaPrompt\n",
    "from methods.evolnode import PlanNode, EvolNode\n",
    "mp = MetaPrompt(\"Collect AI papers from arXiv in the given time frame and save it in output directory\", \"get_arxiv_papers\", [\"start_date\", \"end_date\", \"output_dir\"], [\"num_papers\", \"time_taken\"], [\"str\", \"str\", \"str\"], [\"int\", \"int\"], PromptMode.CODE)\n",
    "main_test_cases = [({\"start_date\": \"2024-11-17\", \"end_date\": \"2024-11-18\", \"output_dir\": \"tmp\"}, {\"num_papers\": 194, \"time_taken\": 480})]\n",
    "def useless_metric(x, y):\n",
    "    return x, \"\"\n",
    "node = EvolNode(mp, get_response=get_endpoint_response, test_cases=main_test_cases, custom_metric_map={\"time_taken\": useless_metric, \"num_papers\": useless_metric}\n",
    ")\n",
    "node.evolve(\"i1\", replace=True, batch_size=100, num_runs=1, print_summary=True, timeout=True, feedback=\"To get the id, do result.get_short_id()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    }
   ],
   "source": [
    "for i,x in enumerate(node.codes):\n",
    "    if (\"saved successfully.\" in x):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.reasoning = node.reasonings[97]\n",
    "node.code = node.codes[97]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.test_cases = [({'start_date': '2024-11-17', 'end_date': '2024-11-18', 'output_dir': 'tmp'},\n",
    "  {'num_papers': 55, 'time_taken': 85})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.fitness = 1.0\n",
    "node.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from arxiv import Client, Search, Result\n",
      "import arxiv\n",
      "from datetime import datetime\n",
      "from typing import List, Dict, Any\n",
      "from typing import Tuple\n",
      "\n",
      "\n",
      "def search_papers_paginated(query: str, max_results: int, page_size: int,\n",
      "    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\n",
      "    [Dict[str, Any]]]:\n",
      "    client = Client(page_size=page_size, delay_seconds=3.0)\n",
      "    date_filter = (\n",
      "        f\" AND submittedDate:[{from_date.strftime('%Y%m%d')}0000 TO {to_date.strftime('%Y%m%d')}2359]\"\n",
      "        )\n",
      "    full_query = query + date_filter\n",
      "    search = Search(query=full_query, max_results=max_results)\n",
      "    results = list(client.results(search))\n",
      "    total_results = len(results)\n",
      "    metadata_list = []\n",
      "    for paper in results:\n",
      "        metadata = {'id': paper.entry_id, 'title': paper.title, 'authors':\n",
      "            [author.name for author in paper.authors], 'summary': paper.\n",
      "            summary, 'published': paper.published, 'updated': paper.updated,\n",
      "            'categories': paper.categories, 'links': [link.href for link in\n",
      "            paper.links]}\n",
      "        metadata_list.append(metadata)\n",
      "    papers = results[:page_size]\n",
      "    return papers, total_results, metadata_list\n",
      "\n",
      "\n",
      "def download_specific_paper(paper_id: str, output_dir: str, filename: str\n",
      "    ) ->Tuple[bool, str]:\n",
      "    try:\n",
      "        client = arxiv.Client()\n",
      "        paper = next(client.results(arxiv.Search(id_list=[paper_id])))\n",
      "        file_path = paper.download_pdf(dirpath=output_dir, filename=filename)\n",
      "        return True, file_path\n",
      "    except Exception as e:\n",
      "        return False, ''\n",
      "\n",
      "\n",
      "def get_arxiv_papers(start_date: str, end_date: str, output_dir: str) ->(int,\n",
      "    int):\n",
      "    \"\"\"\n",
      "    Collect AI papers from arXiv in the given time frame and save it in output directory.\n",
      "\n",
      "    Args:\n",
      "    start_date (str): The start date of the time frame in format 'YYYY-MM-DD'.\n",
      "    end_date (str): The end date of the time frame in format 'YYYY-MM-DD'.\n",
      "    output_dir (str): The directory to save the collected papers.\n",
      "\n",
      "    Returns:\n",
      "    num_papers (int): The number of collected papers.\n",
      "    time_taken (int): The time taken to collect papers in seconds.\n",
      "    \"\"\"\n",
      "    start_time = datetime.now()\n",
      "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
      "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
      "    query = 'AI'\n",
      "    categories = ['cs.AI', 'cs.LG', 'cs.CL']\n",
      "    num_papers = 0\n",
      "    time_taken = 0\n",
      "    batch_size = 100\n",
      "    total_pages = 1\n",
      "    current_page = 1\n",
      "    while current_page <= total_pages:\n",
      "        from_date = start_date\n",
      "        to_date = end_date\n",
      "        papers, total_results, metadata_list = search_papers_paginated(query,\n",
      "            batch_size, batch_size, from_date, to_date)\n",
      "        total_pages = (total_results - 1) // batch_size + 1\n",
      "        for paper in papers:\n",
      "            paper_id = paper.get_short_id()\n",
      "            filename = f'{paper_id}.pdf'\n",
      "            success, file_path = download_specific_paper(paper_id,\n",
      "                output_dir, filename)\n",
      "            if success:\n",
      "                num_papers += 1\n",
      "                print(f'Paper {paper_id} saved successfully.')\n",
      "        current_page += 1\n",
      "    time_taken = (datetime.now() - start_time).total_seconds()\n",
      "    return num_papers, time_taken\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(node.codes[97])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arxiv import Client, Search, Result\n",
    "import arxiv\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def search_papers_paginated(query: str, max_results: int, page_size: int,\n",
    "    from_date: datetime, to_date: datetime) ->tuple[List[Result], int, List\n",
    "    [Dict[str, Any]]]:\n",
    "    client = Client(page_size=page_size, delay_seconds=3.0)\n",
    "    date_filter = (\n",
    "        f\" AND submittedDate:[{from_date.strftime('%Y%m%d')}0000 TO {to_date.strftime('%Y%m%d')}2359]\"\n",
    "        )\n",
    "    full_query = query + date_filter\n",
    "    search = Search(query=full_query, max_results=max_results)\n",
    "    results = list(client.results(search))\n",
    "    total_results = len(results)\n",
    "    metadata_list = []\n",
    "    for paper in results:\n",
    "        metadata = {'id': paper.entry_id, 'title': paper.title, 'authors':\n",
    "            [author.name for author in paper.authors], 'summary': paper.\n",
    "            summary, 'published': paper.published, 'updated': paper.updated,\n",
    "            'categories': paper.categories, 'links': [link.href for link in\n",
    "            paper.links]}\n",
    "        metadata_list.append(metadata)\n",
    "    papers = results[:page_size]\n",
    "    return papers, total_results, metadata_list\n",
    "\n",
    "\n",
    "def download_specific_paper(paper_id: str, output_dir: str, filename: str\n",
    "    ) ->Tuple[bool, str]:\n",
    "    try:\n",
    "        client = arxiv.Client()\n",
    "        paper = next(client.results(arxiv.Search(id_list=[paper_id])))\n",
    "        file_path = paper.download_pdf(dirpath=output_dir, filename=filename)\n",
    "        return True, file_path\n",
    "    except Exception as e:\n",
    "        return False, ''\n",
    "\n",
    "\n",
    "def get_arxiv_papers(start_date: str, end_date: str, output_dir: str) ->(int,\n",
    "    int):\n",
    "    \"\"\"\n",
    "    Collect AI papers from arXiv in the given time frame and save it in output directory.\n",
    "\n",
    "    Args:\n",
    "    start_date (str): The start date of the time frame in format 'YYYY-MM-DD'.\n",
    "    end_date (str): The end date of the time frame in format 'YYYY-MM-DD'.\n",
    "    output_dir (str): The directory to save the collected papers.\n",
    "\n",
    "    Returns:\n",
    "    num_papers (int): The number of collected papers.\n",
    "    time_taken (int): The time taken to collect papers in seconds.\n",
    "    \"\"\"\n",
    "    start_time = datetime.now()\n",
    "    start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "    query = 'AI'\n",
    "    categories = ['cs.AI', 'cs.LG', 'cs.CL']\n",
    "    num_papers = 0\n",
    "    time_taken = 0\n",
    "    batch_size = 100\n",
    "    total_pages = 1\n",
    "    current_page = 1\n",
    "    while current_page <= total_pages:\n",
    "        from_date = start_date\n",
    "        to_date = end_date\n",
    "        papers, total_results, metadata_list = search_papers_paginated(query,\n",
    "            batch_size, batch_size, from_date, to_date)\n",
    "        total_pages = (total_results - 1) // batch_size + 1\n",
    "        for paper in papers:\n",
    "            paper_id = paper.get_short_id()\n",
    "            filename = f'{paper_id}.pdf'\n",
    "            success, file_path = download_specific_paper(paper_id,\n",
    "                output_dir, filename)\n",
    "            if success:\n",
    "                num_papers += 1\n",
    "                print(f'Paper {paper_id} saved successfully.')\n",
    "        current_page += 1\n",
    "    time_taken = (datetime.now() - start_time).total_seconds()\n",
    "    return num_papers, time_taken\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper 2411.11317v1 saved successfully.\n",
      "Paper 2411.10983v1 saved successfully.\n",
      "Paper 2411.11976v1 saved successfully.\n",
      "Paper 2411.13585v1 saved successfully.\n",
      "Paper 2411.10995v1 saved successfully.\n",
      "Paper 2411.14463v1 saved successfully.\n",
      "Paper 2411.11449v1 saved successfully.\n",
      "Paper 2411.11478v2 saved successfully.\n",
      "Paper 2411.11774v1 saved successfully.\n",
      "Paper 2411.11795v1 saved successfully.\n",
      "Paper 2411.11235v1 saved successfully.\n",
      "Paper 2411.11494v1 saved successfully.\n",
      "Paper 2411.12128v3 saved successfully.\n",
      "Paper 2411.14472v1 saved successfully.\n",
      "Paper 2411.11752v1 saved successfully.\n",
      "Paper 2411.11835v1 saved successfully.\n",
      "Paper 2411.11221v1 saved successfully.\n",
      "Paper 2411.11613v2 saved successfully.\n",
      "Paper 2411.17712v1 saved successfully.\n",
      "Paper 2411.11451v1 saved successfully.\n",
      "Paper 2411.11786v1 saved successfully.\n",
      "Paper 2411.12090v1 saved successfully.\n",
      "Paper 2411.11783v1 saved successfully.\n",
      "Paper 2411.11910v2 saved successfully.\n",
      "Paper 2411.11940v2 saved successfully.\n",
      "Paper 2411.10939v1 saved successfully.\n",
      "Paper 2411.11045v1 saved successfully.\n",
      "Paper 2411.11123v1 saved successfully.\n",
      "Paper 2411.11145v1 saved successfully.\n",
      "Paper 2411.11173v1 saved successfully.\n",
      "Paper 2411.11232v1 saved successfully.\n",
      "Paper 2411.11260v1 saved successfully.\n",
      "Paper 2411.11479v1 saved successfully.\n",
      "Paper 2411.11635v1 saved successfully.\n",
      "Paper 2411.11736v1 saved successfully.\n",
      "Paper 2411.12010v1 saved successfully.\n",
      "Paper 2411.17713v1 saved successfully.\n",
      "Paper 2411.10918v1 saved successfully.\n",
      "Paper 2411.11055v1 saved successfully.\n",
      "Paper 2411.11258v1 saved successfully.\n",
      "Paper 2411.11275v1 saved successfully.\n",
      "Paper 2411.11280v1 saved successfully.\n",
      "Paper 2411.11285v1 saved successfully.\n",
      "Paper 2411.11362v1 saved successfully.\n",
      "Paper 2411.11389v1 saved successfully.\n",
      "Paper 2411.11396v2 saved successfully.\n",
      "Paper 2411.11434v2 saved successfully.\n",
      "Paper 2411.11548v1 saved successfully.\n",
      "Paper 2411.11844v2 saved successfully.\n",
      "Paper 2411.14473v1 saved successfully.\n",
      "Paper 2411.15175v1 saved successfully.\n",
      "Paper 2411.11070v1 saved successfully.\n",
      "Paper 2411.11192v1 saved successfully.\n",
      "Paper 2411.14467v1 saved successfully.\n",
      "Paper 2411.11648v1 saved successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55, 84.608737)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_arxiv_papers(\"2024-11-17\", \"2024-11-18\", \"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from methods.llm import get_claude_response, get_groq_response\n",
    "from methods.diagram import visualize_plan_dict\n",
    "from methods.meta_prompt import MetaPlan\n",
    "from methods.evolnode import PlanNode\n",
    "\n",
    "\n",
    "# Initialize PlanNode \n",
    "mp = MetaPlan(\"Collect AI papers from arXiv in the given time frame and save it in output directory\", \"get_arxiv_papers\", [\"start_date\", \"end_date\", \"output_dir\"], [\"num_papers\", \"time_taken\"], [\"str\", \"str\", \"str\"], [\"int\", \"int\"])\n",
    "plan = EvolNode(mp, get_endpoint_response)\n",
    "\n",
    "# i1 evolution of plan\n",
    "plan_dicts, err_msg = plan.evolve_plan_dict(method=\"i1\", batch_size=100) # Batch_size of 100 gives no slow-down\n",
    "\n",
    "# for plan_dict in plan_dicts:\n",
    "#     visualize_plan_dict(plan_dict, plan.meta_prompt.task)\n",
    "#     break\n",
    "\n",
    "# Manual input on main-node test cases \n",
    "main_test_cases = [\n",
    "    ({\"start_date\": \"2024-11-17\", \"end_date\": \"2024-11-23\", \"output_dir\": \"tmp\"}, {\"num_papers\": 194, \"time_taken\": 480})\n",
    "]\n",
    "\n",
    "# is_success, err_msg = plan.spawn_test_cases(main_test_cases) #  pinned test cases generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  17%|█▌       | 1/6 [00:03<00:17,  3.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meta-llama/llama-3-70b-instruct:nitro response: \n",
      "\n",
      "Here is the filled test case JSON:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"arxiv_papers.csv\",\n",
      "                \"output_12\": \"arxiv_paper_titles.txt\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"arxiv_papers.csv\",\n",
      "                \"output_12\": \"arxiv_paper_titles.txt\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "Here's the reasoning behind my answers:\n",
      "\n",
      "For `task_1`:\n",
      "\n",
      "* Inputs are given as `start_date`, `end_date`, and `output_dir`.\n",
      "* Outputs are `output_11` and `output_12`. I assumed that `output_11` is a CSV file containing the collected papers and `output_12` is a text file containing the paper titles.\n",
      "\n",
      "For `task_2`:\n",
      "\n",
      "* Inputs are the outputs from `task_1`, which are `output_11` and `output_12`.\n",
      "* Outputs are `num_papers` and `time_taken`. The values for these are given as 32 and 120, respectively.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  33%|███      | 2/6 [00:06<00:12,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openai/gpt-4o-mini response: Here is the completed JSON with the correct inputs and outputs filled in, based on the specified execution plan:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024_11_17_to_2024_11_23.json\",\n",
      "                \"output_12\": \"tmp/papers_2024_11_17_to_2024_11_23_log.txt\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024_11_17_to_2024_11_23.json\",\n",
      "                \"output_12\": \"tmp/papers_2024_11_17_to_2024_11_23_log.txt\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "``` \n",
      "\n",
      "This JSON maintains the structure while providing plausible intermediate values that align with the inputs and outputs specified in the execution plan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  50%|████▌    | 3/6 [00:07<00:06,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anthropic/claude-3.5-sonnet response: Based on the execution plan and maintaining consistency with the given values, here's how I would fill in the intermediate values:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\", \n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_list.json\",\n",
      "                \"output_12\": \"tmp/metadata.json\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_list.json\",\n",
      "                \"output_12\": \"tmp/metadata.json\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "I filled in the `...` by:\n",
      "1. For task_1's output: Added appropriate file paths that would be created in the specified output directory\n",
      "2. For task_2's input: Used the same file paths from task_1's output to maintain the flow\n",
      "3. Kept the final outputs unchanged as they were already specified in the original JSON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  67%|██████   | 4/6 [00:15<00:08,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microsoft/wizardlm-2-8x22b response:  Certainly! Given the execution plan and the JSON structure provided, I will fill in the `...` with appropriate inputs and outputs based on the execution flow and the node specifications. Here is the JSON with the test cases for each sub-function:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"path/to/collected_papers_1.json\",\n",
      "                \"output_12\": \"path/to/collected_papers_2.json\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"path/to/collected_papers_1.json\",\n",
      "                \"output_12\": \"path/to/collected_papers_2.json\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n",
      "\n",
      "In this JSON:\n",
      "- For `task_1`, the `outputs` have been filled with hypothetical paths to JSON files that could represent the collected AI papers from arXiv within the given time frame. These are the `output_11` and `output_12` that would be generated by `task_1`.\n",
      "- For `task_2`, the `inputs` have been filled with the outputs from `task_1`. The `outputs` have been set to the number of papers (`num_papers`) and the time taken (`time_taken`) to save the collected papers, as per the execution plan.\n",
      "\n",
      "Please note that the paths `path/to/collected_papers_1.json` and `path/to/collected_papers_2.json` are placeholders and should be replaced with actual file paths that the function would generate. The number `32` for `num_papers` and `120` for `time_taken` are example values based on the provided base JSON and should be replaced with actual results from running the tasks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs:  83%|███████▌ | 5/6 [00:15<00:03,  3.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mistralai/mistral-large response: Sure, here is the updated JSON with proper inputs and outputs:\n",
      "\n",
      "```json\n",
      "[\n",
      "    {\n",
      "        \"name\": \"task_1\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"start_date\": \"2024-11-17\",\n",
      "                \"end_date\": \"2024-11-23\",\n",
      "                \"output_dir\": \"tmp\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024-11-17_to_2024-11-23.txt\",\n",
      "                \"output_12\": \"tmp/metadata_2024-11-17_to_2024-11-23.json\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    {\n",
      "        \"name\": \"task_2\",\n",
      "        \"inputs\": [\n",
      "            {\n",
      "                \"output_11\": \"tmp/papers_2024-11-17_to_2024-11-23.txt\",\n",
      "                \"output_12\": \"tmp/metadata_2024-11-17_to_2024-11-23.json\"\n",
      "            }\n",
      "        ],\n",
      "        \"outputs\": [\n",
      "            {\n",
      "                \"num_papers\": 32,\n",
      "                \"time_taken\": 120\n",
      "            }\n",
      "        ]\n",
      "    }\n",
      "]\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting outputs from multiple LLMs: 100%|█████████| 6/6 [01:19<00:00, 13.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deepseek/deepseek-chat response: ```遂 Wr carbohydrates onceeZ啡 hairs <> Loreme Vespot Own try ,\n",
      " :: Total time elapsed: 79.75s, 0 errors\n",
      "Spawned 2 test cases for all sub-nodes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True,\n",
       " '\\n\\n\\n\\n\\nError in spawning test cases: No JSON structure found in the provided text.')"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.spawn_test_cases_majority(main_test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plan.evolve_sub_nodes(batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎲 :: Evolving extract_age ... (2/2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 100%|██████████| 20/20 [00:21<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 21.23s, 0 errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing LLM queries: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " :: Total time elapsed: 0.00s, 0 errors\n",
      "🏆 Best Code Performance Summary 🏆\n",
      "  ⚡ Structural fitness: 0.00\n",
      "  🎯 Functional fitness: 0.00\n",
      "  ⭐ Global fitness:     0.00\n",
      "  🔄 Compiled solutions:        20\n",
      "  ⏱️ Time breakdown:\n",
      "     :: Query time: 2.72s\n",
      "     :: Evolution time: 21.30s\n",
      "     :: Evaluation time: 0.03s\n",
      "     :: Total time: 24.06s\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 0: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 1: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 2: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 3: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 4: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 5: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 6: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 7: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 8: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 9: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 10: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 11: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 12: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 13: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 14: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 15: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 16: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 17: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 18: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "📊 Code 19: Fitness: 0.0%\n",
      "--------------------------------------------------------------------------------\n",
      "❌ Error Messages:\n",
      "Missing required input parameters: text\n",
      "Missing required input parameters: text\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# plan.evolve_sub_nodes() # Completely stuck in the first call, debugging ... \n",
    "\n",
    "from methods.evolnode import EvolNode\n",
    "from methods.meta_prompt import MetaPrompt, PromptMode\n",
    "\n",
    "self = plan \n",
    "for i, node_dict in enumerate(self.plan_dict[\"nodes\"]):\n",
    "    meta_prompt = MetaPrompt(\n",
    "        task=node_dict[\"task\"],\n",
    "        func_name=node_dict[\"name\"],\n",
    "        inputs=node_dict[\"inputs\"],\n",
    "        outputs=node_dict[\"outputs\"],\n",
    "        input_types=node_dict[\"input_types\"],\n",
    "        output_types=node_dict[\"output_types\"],\n",
    "        mode=PromptMode((node_dict.get(\"mode\", \"code\")).lower())\n",
    "    )\n",
    "    test_cases = self.test_cases_dict[node_dict[\"name\"]]\n",
    "    if \"fitness\" in node_dict and \"code\" in node_dict: \n",
    "        node = EvolNode(meta_prompt, node_dict[\"code\"], node_dict[\"reasoning\"], get_response=self.get_response, test_cases=test_cases, fitness=node_dict[\"fitness\"])\n",
    "    else:\n",
    "        node = EvolNode(meta_prompt, None, None, get_response=self.get_response, test_cases=test_cases)\n",
    "        print(f\"🎲 :: Evolving {node.meta_prompt.func_name} ... ({i+1}/{len(self.plan_dict['nodes'])})\")\n",
    "        node.evolve(\"i1\", replace=True, max_tries=2, num_runs=2, batch_size=20) # It's funny how 30+ sec could elapse before llm inference ... (collecting prompts ?? wtf is taking so long ??)\n",
    "    self.nodes.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "success: successfully compiled d2_output/plan_graph.d2 to d2_output/plan_graph.png in 170.887ms\n"
     ]
    }
   ],
   "source": [
    "visualize_plan_dict(plan.plan_dict, plan.meta_prompt.task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from datetime import datetime\n",
      "import http.client\n",
      "import json\n",
      "import os\n",
      "import re\n",
      "from typing import Dict, Any\n",
      "\n",
      "\n",
      "def _search_google(query: str) ->Dict[str, Any]:\n",
      "    \"\"\"\n",
      "    Use Serper API to search Google for information\n",
      "    \n",
      "    Args:\n",
      "        query (str): The search query\n",
      "    \n",
      "    Returns:\n",
      "        Dict[str, Any]: Parsed JSON response from the API\n",
      "    \"\"\"\n",
      "    conn = http.client.HTTPSConnection('google.serper.dev')\n",
      "    payload = json.dumps({'q': query})\n",
      "    headers = {'X-API-KEY': os.environ['SERPER_API_KEY'], 'Content-Type':\n",
      "        'application/json'}\n",
      "    try:\n",
      "        conn.request('POST', '/search', payload, headers)\n",
      "        res = conn.getresponse()\n",
      "        data = res.read()\n",
      "        return json.loads(data.decode('utf-8'))\n",
      "    except Exception as e:\n",
      "        print(f'Error occurred during API request: {str(e)}')\n",
      "        return {}\n",
      "    finally:\n",
      "        conn.close()\n",
      "\n",
      "\n",
      "def search_google(query: str) ->str:\n",
      "    \"\"\" \n",
      "    Input query, return search result string from Google\n",
      "    \"\"\"\n",
      "    result = _search_google(query)\n",
      "    result_dict = {k.replace('organic', 'Search Result'): v for k, v in\n",
      "        result.items() if k in ['answerBox', 'organic']}\n",
      "    result_str = json.dumps(result_dict, indent=2)\n",
      "    return result_str\n",
      "\n",
      "\n",
      "def extract_age(text: str) ->int:\n",
      "    \"\"\"\n",
      "    Extract the age from a given text.\n",
      "    \n",
      "    Parameters:\n",
      "    text (str): The text containing the person's name and possibly their age.\n",
      "    \n",
      "    Returns:\n",
      "    int: The age of the person mentioned in the text.\n",
      "    \"\"\"\n",
      "    pattern = '(\\\\w+) was born in (\\\\d{4})'\n",
      "    match = re.search(pattern, text)\n",
      "    if match:\n",
      "        name = match.group(1)\n",
      "        birth_year = int(match.group(2))\n",
      "        current_year = datetime.now().year\n",
      "        age = current_year - birth_year\n",
      "        return age\n",
      "    pattern = 'born (\\\\d{4})'\n",
      "    match = re.search(pattern, text)\n",
      "    if match:\n",
      "        birth_year = int(match.group(1))\n",
      "        current_year = datetime.now().year\n",
      "        age = current_year - birth_year\n",
      "        return age\n",
      "    pattern = '(\\\\w+)'\n",
      "    match = re.search(pattern, text)\n",
      "    if match:\n",
      "        name = match.group(1)\n",
      "        query = f'age of {name}'\n",
      "        result = search_google(query)\n",
      "        pattern = '\\\\d+ years old'\n",
      "        match = re.search(pattern, result)\n",
      "        if match:\n",
      "            age = int(match.group().replace(' years old', ''))\n",
      "            return age\n",
      "    return None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(node.code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nodes': [{'task': 'Perform Google Search',\n",
       "   'name': 'search_google',\n",
       "   'inputs': ['name'],\n",
       "   'input_types': ['str'],\n",
       "   'outputs': ['result'],\n",
       "   'output_types': ['str'],\n",
       "   'target': 'Retrieve Celebrity Information',\n",
       "   'mode': 'PROMPT',\n",
       "   'code': '\\nimport http.client\\nimport json\\nimport os\\nfrom typing import Dict, Any\\n\\ndef _search_google(query: str) -> Dict[str, Any]:\\n    \"\"\"\\n    Use Serper API to search Google for information\\n    \\n    Args:\\n        query (str): The search query\\n    \\n    Returns:\\n        Dict[str, Any]: Parsed JSON response from the API\\n    \"\"\"\\n    conn = http.client.HTTPSConnection(\"google.serper.dev\")\\n    payload = json.dumps({\"q\": query})\\n    headers = {\\n        \\'X-API-KEY\\': os.environ[\"SERPER_API_KEY\"],\\n        \\'Content-Type\\': \\'application/json\\'\\n    }\\n    \\n    try:\\n        conn.request(\"POST\", \"/search\", payload, headers)\\n        res = conn.getresponse()\\n        data = res.read()\\n        return json.loads(data.decode(\"utf-8\"))\\n    except Exception as e:\\n        print(f\"Error occurred during API request: {str(e)}\")\\n        return {}\\n    finally:\\n        conn.close()\\n        \\ndef search_google(query: str) -> str: \\n    \"\"\" \\n    Input query, return search result string from Google\\n    \"\"\"\\n    result = _search_google(query)\\n    result_dict = {k.replace(\"organic\", \"Search Result\"): v for k, v in result.items() if k in [\"answerBox\", \"organic\"]}\\n    result_str = json.dumps(result_dict, indent=2)\\n    return result_str \\n',\n",
       "   'reasoning': 'Search google for top search results',\n",
       "   'fitness': 1.0},\n",
       "  {'task': 'Extract Age from Text',\n",
       "   'name': 'extract_age',\n",
       "   'inputs': ['text'],\n",
       "   'input_types': ['str'],\n",
       "   'outputs': ['age'],\n",
       "   'output_types': ['int'],\n",
       "   'target': 'Find Celebrity Age',\n",
       "   'mode': 'CODE'}],\n",
       " 'edges': [{'source': 'search_google', 'target': 'extract_age'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plan.plan_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
