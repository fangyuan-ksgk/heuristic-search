{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root endpoint response: {'message': 'API is running'}\n",
      "Health check response: {'status': 'healthy'}\n"
     ]
    }
   ],
   "source": [
    "# Call RunPod served endpoint \n",
    "INTERNAL_PORT = 30000\n",
    "POD_ID = \"312aoreubi3q58\"\n",
    "BASE_URL = f\"https://{POD_ID}-{INTERNAL_PORT}.proxy.runpod.net\"\n",
    "\n",
    "import requests \n",
    "\n",
    "try:\n",
    "    # Call the root endpoint \"/\"\n",
    "    response = requests.get(BASE_URL)\n",
    "    print(\"Root endpoint response:\", response.json())\n",
    "    \n",
    "    # Call the health endpoint \"/health\"\n",
    "    health_response = requests.get(f\"{BASE_URL}/health\")\n",
    "    print(\"Health check response:\", health_response.json())\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependency \n",
    "# pip install flashinfer -i https://flashinfer.ai/whl/cu124/torch2.4\n",
    "# pip install orjson\n",
    "# pip install sglang\n",
    "\n",
    "from sglang.utils import (\n",
    "    execute_shell_command,\n",
    "    wait_for_server,\n",
    "    terminate_process,\n",
    "    print_highlight,\n",
    ")\n",
    "\n",
    "MODEL_PATH = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n",
    "INTERNAL_PORT = 30000\n",
    "CHAT_TEMPLATE = \"llama_3_vision\"\n",
    "\n",
    "\n",
    "embedding_process = execute_shell_command(\n",
    "    f\"\"\"\n",
    "python3 -m sglang.launch_server --model-path {MODEL_PATH} \\\n",
    "    --port={INTERNAL_PORT} --chat-template={CHAT_TEMPLATE}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "wait_for_server(f\"http://localhost:{INTERNAL_PORT}\")\n",
    "\n",
    "\n",
    "# Then we can access endpoint through OpenAI api\n",
    "# Here the complication is we are using RunPod to connect things, BASE_URL is verylikely different from the default one \n",
    "\n",
    "MODEL_PATH = \"unsloth/Llama-3.2-11B-Vision-Instruct\"\n",
    "INTERNAL_PORT = 30000\n",
    "POD_ID = \"312aoreubi3q58\"\n",
    "BASE_URL = f\"https://{POD_ID}-{INTERNAL_PORT}.proxy.runpod.net\"\n",
    "\n",
    "# My assumetion is base_url = f\"{BASE_URL}/v1\" ...\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(base_url=f\"{BASE_URL}v1\", api_key=\"None\")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"unsloth/Llama-3.2-11B-Vision-Instruct\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is in this image?\",\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": \"https://github.com/sgl-project/sglang/blob/main/test/lang/example_image.png?raw=true\"\n",
    "                    },\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=300,\n",
    ")\n",
    "\n",
    "print_highlight(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
